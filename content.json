{"pages":[{"title":"关于","text":"关于我日常搬砖，近 7 年技术搬砖经验，前后端工作经验都有涉及， 4 年 Android/前端 搬砖经验，3 年 Java 服务端。现在主要方向Java 服务端。意在电商或高并发高可用。 崇尚通过不断的学习实践拓宽自己的技术边界，不求改变世界，只为能体现一点点价值。 这里只是记录一些日常学习笔记，不喜勿喷。 联系我（如果有好事情的话^v^）brokge#gmail/qq.com https://github.com/brokge","link":"/about/index.html"}],"posts":[{"title":"Android Bundle 详解","text":"我们都知道，在 Android 应用开发中，需要数据和状态的传递，其中还包括在 跨进程 之间的传递 （比如 IPC/Binder）。关于数据传递有多种方式，其中最常见的就是通过 Bundle 。Bundle 中文意思：捆; 一批，顾名思义就很直观了。 Bundle 相当于传输过程的邮包，里面包裹的是具体的数据。 实现Activity 之间可以通过 创建 intent 并传递参数 的方式来传递， Intent intent = new Intent(this, MyActivity.class); intent.putExtra(\"media_id\", \"a1b2c3\"); ... startActivity(intent); 在当前 Activity 打包数据，在 目标 Activity 解包数据。 通过查看 Api ,我们可以知道 一些常规数据类型，如：int、string、boolean 等经过简单的设置，都没什么问题。但是我们如果想通过一定机制，传递一些复杂的复合对象呢？这就需要 Parcelables 了。传递复杂数据 可以查看相应的文章。 通过 Bundle 进行数据，要注意以下问题 复杂对象 通过 Parcelables 或者 seriable。 对象最大 不能超过 1 mb,否则会出现 TransactionTooLargeException 错误。7.0 (API level 24) 或更高系统上会报出，其他系统会有警告log。 savedInstanceState 保存数据状态，系统运行中这些保存的数据会一直存在，所以尽可能小于 50kb，否则会照成资源的浪费。 以上是bundle 如何使用以及需要注意的事项。 Parcel不是通用的序列化机制（Serializable是通用的序列化机制） 所以不能把 Parcel数据存储在磁盘上或通过网络发送出去。 源码分析通过源码查看，Bundle 继承 BaseBundle 且实现了 Parcelable 接口。BaseBundle 内部 维护一个 ArrayMap&lt;String, Object&gt; mMap 常量来承载我们需要操作的对象。 public final class Bundle extends BaseBundle implements Cloneable, Parcelable BaseBundle 声明常量 mMap： ArrayMap&lt;String, Object> mMap = null; 通过 Bundle bundle=new Bundle(); bundle.putXX(Object object); 实际上内部执行的是对 ArrayMap 的操作 void putXXX(@Nullable String key, Object value) { unparcel(); mMap.put(key, value); } Parcelable假设我们需要将String str = &quot;Hanmeimei&quot;;，从 Activity A 传递到 Activity B 时，我们可以使用 intent.putExtra(“name”,str ); 这样在 Activity B 中就能获取到由 Activity A 传递过来的字符串 str .那么如果我想将一个对象由 Activity A 传递到 Activity B 该怎么办，比如 User 对象。 public class User { private String name; private String password; private String age; public String getName() { return name; } public void setName(String name) { this.name = name; } public String getPassword() { return password; } public void setPassword(String password) { this.password = password; } public String getAge() { return age; } public void setAge(String age) { this.age = age; } public User(String name, String password, String age) { this.name = name; this.password = password; this.age = age; } } Android 并没有提供在两个 activity之间传递任意对象或者引用的方法，但是就是需要传递对象该怎么办呢，这个时候就需要 让该对象实现 Android 提供的 parcelable接口，说到 parcelable 就得说Parcel。 Parcel 是什么？简单说Parcel就是一个存放读取数据的容器， Android系统中的binder进程间通信(IPC)就使用了Parcel类来进行客户端与服务端数据的交互，而且AIDL的数据也是通过Parcel来交互的。在Java空间和C++都实现了Parcel，由于它在C/C++中，直接使用了内存来读取数据，因此，它更有效率。Parcel不是通用的序列化机制（Serialize 是通用的序列化机制） 所以不能把 Parcel数据存储在磁盘上或通过网络传输。 Parcel 的标记接口 是 Parcelable，如果该对象实现了parcelable接口，且实现了对应的方法，就拥有了 Parcel 的特性。 Parcelable 在数据传递的用法在Android 对象传递需求中，那么就可以利用下图红框中的方法进行传递了，putExtra(String name,Parcelable user);可以看出，我们可以传递一个实现 Parcelable接口的对象了 Serializable概念 序列化就是将对象转化为字节流。 反序列化就是将字节流转化为对象。 默认的序列化是深度系列化（即类中嵌套其他对象引用的对象也会被序列化）。 静态成员不会被默认序列化，要让一个类支持序列化只要让这个类实现接口 java.io.Serializable 即可 package java.io; public interface Serializable { } 以上是 Serializable 的接口定义，且 Serializable 只是一个没有定义任何方法的标记接口。 为什么定义标记接口即可实现序列化了呢？声明实现 Serializable 接口后保存读取对象就可以使用 ObjectOutputStream、ObjectInputStream 流了，ObjectOutputStream 是 OutputStream 的子类，但实现了 ObjectOutput 接口，ObjectOutput 是 DataOutput 的子接口，增加了一个 writeObject(Object obj) 方法将对象转化为字节写到流中，ObjectInputStream 是 InputStream 的子类，实现了ObjectInput 接口，ObjectInput 是 DataInput 的子接口，增加了一个 readObject() 方法从流中读取字节转为对象。 序列化和反序列化的实质在于 ObjectOutputStream 的 writeObject 和 ObjectInputStream 的 readObject 方法实现，常见的 String、Date、Double、ArrayList、LinkedList、HashMap、TreeMap 等都默认实现了 Serializable,. 有时候我们对象有些字段的值可能与内存位置（hashcode）、当前时间等有关，所以我们不想序列化他（因为反序列化后的值是没有意义的）。或者有时候如果类中的字段，表示的是类的实现细节，而非逻辑信息则默认序列化，也是不适合的。 由于以上原因：所以我们需要定制序列化，Java 提供的定制主要有 transient 关键字方式 和 实现 writeObject、readObject 方式 及 Externalizable 接口 readExternal、writeExternal 方式。还可以将字段声明为 transient 后通过 writeObject、readObject 方法来自己保存该字段。 @Target({ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) public @interface Transient { boolean value() default true; } Externalizable.java public interface Externalizable extends java.io.Serializable { void writeExternal(ObjectOutput out) throws IOException; void readExternal(ObjectInput in) throws IOException, ClassNotFoundException; } 默认情况下 Java 会根据类中一系列信息自动生成一个版本号，在反序列化时如果类的定义发生了变化版本号就会变化，也就与反序列化流中的版本号不匹配导致会抛出异常，所以我们为了更好的控制和性能问题会自定义 serialVersionUID 版本号来避免类定义发生变化后反序列化版本号不匹配异常问题，如果版本号一样时流中有该字段而类定义中没有则该字段会被忽略，如果类定义中有而流中没有则该字段会被设为默认值，如果对于同名的字段类型变了则会抛出 InvalidClassException。 虚拟机是否允许反序列化不仅取决于类路径和功能代码是否一致，还取决于另一个非常重要的点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID = 1L）。 因为声明实现 Serializable 接口后保存读取对象就可以使用 ObjectOutputStream、ObjectInputStream 流了，ObjectOutputStream 的 writeObject(Object obj) 方法将对象转化为字节写到流中，ObjectInputStream 的 readObject() 方法从流中读取字节转为对象，Serializable 虽然是一个空接口，但是在调用 writeObject 方法时却充当了一种健全的校验作用，如果对象没有实现 Serializable 则在调用 writeObject 时就会抛出异常，所以说 Serializable 算是一种接口标识机制。 如下为 ObjectOutputStream 中 writeObject(Object obj) 的核心标记判断： private void writeObject0 (Object obj, boolean unshared) throws IOException { try { if(obj instanceof Class) { writeClass((Class) obj, unshared); } else if(obj instanceof ObjectStreamClass) { writeClassDesc((ObjectStreamClass) obj, unshared); } else if(obj instanceof String) { writeString((String) obj, unshared); } else if(cl.isArray()) { writeArray(obj, desc, unshared); } else if(obj instanceof Enum) { writeEnum((Enum) obj, desc, unshared); } else if(obj instanceof Serializable) { writeOrdinaryObject(obj, desc, unshared); } else { if(extendedDebugInfo) { throw new NotSerializableException(cl.getName() + \"\\n\" + debugInfoStack.toString()); } else { throw new NotSerializableException(cl.getName()); } } } finally { } } 最后通过以上我们了解 Serializable 和 Parcelable都可以实现复杂数据结构的封装传输，两者的区别是什么？ 两者的设计初衷： Serializable 的作用是为了保存对象的属性到本地文件、数据库、网络流等以方便数据传输，当然这种传输可以是程序内的也可以是两个程序间的。 Parcelable 的设计初衷是因为Serializable 效率过慢，为了在程序内不同组件间以及不同 Android 程序间(AIDL)高效的传输数据而设计，这些数据仅在内存中存在，而且 Parcelable 是通过 IBinder 通信的消息的载体。 两者的区别： 在使用内存的时候，Parcelable 类比 Serializable 性能高。 Serializable在序列化的时候会产生大量的临时变量，从而引起频繁的GC。 Parcelable不能适用在要将数据存储在磁盘上的情况，因为Parcelable 在外界有变化的情况下，不能很好的保证数据的持续性。 适用场景： 只在内存中操作数据时，比如两个 Activity 之间 传输数据。 需要持久化数据时，比如需要将数据保存的本地文件、数据库，所以尽管 Serializable 效率低点， 也不提倡用，但在这种情况下，还是建议你用 Serializable 。","link":"/2019/07/10/Android-Bundle/"},{"title":"Android Dagger2分析","text":"官方地址 参考博客地址 注解依赖注入Dagger2 是一个依赖注入框架，依赖注入的目的就是为了给需求方在合适的时候注入依赖。**@Inject**@Inject 注解就如同一个标签，或者说它是一个记号，它是给 Dagger2 看的。它运用的地方有两处。 @Inject 给一个类的相应的属性做标记时，说明了它是一个依赖需求方，需要一些依赖。 @Inject 给一个类的构造方法进行注解时，表明了它能提供依赖的能力。 就这样，通过 @Inject 注解符号，就很容易标记依赖和它的需求方。但是，单单一个 @Inject 是不能让 Dagger2 正常运行的。还需要另外一个注解配合。这个注解就是 @Component。 @Component 而 @Component 相当于联系纽带，将 @inject 标记的需求方和依赖绑定起来，并建立了联系，而 Dagger2 在编译代码时会依靠这种关系来进行对应的依赖注入 @Provides 和 @ModuleDagger2 为了能够对第三方库中的类进行依赖注入，提供了 @Provides 和 @Module 两个注解。 Provide 本身的字面意思就是提供，显然在 Dagger2 中它的作用就是提供依赖。 Module 是模块的意思，Dagger2 中规定，用 @Provides 注解的依赖必须存在一个用 @Module 注解的类中。 @Inject 和 @Provides 的优先级 Baozi 这个类就符合我上面给的情景，一方面它确实拥有被 @Inject 注解过的构造方法，另一方面在 Module 中它又通过 @Provides 提供了依赖。那么，最终，Dagger2 采取了哪一种呢？ 答案是 Module，其实现象我们在之前的测试时已经可以观察到了，最终屏幕显示的是豆沙包选项。 Dagger2 依赖查找的顺序是先查找 Module 内所有的 @Provides 提供的依赖，如果查找不到再去查找 @Inject 提供的依赖。 Dagger2 中的单例 @Singleton 用 @Singleton 标注在目标单例上，然后用 @Singleton 标注在 Component 对象上。 如果要以 @Provides 方式提供单例的话，需要用 @Singleton 注解依赖提供的方法 @Module public class SecondActivityModule { @Provides @Singleton public TestSingleton provideTestSingleton(){ return new TestSingleton(); } } @Singleton 是一个注解，但是它被一个元注解 @Scope 注解了 为什么要用 @Singleton 同时标注 @Provides 和 @Component ? Component 是联系需求与依赖的纽带，所以用 @Singleton 确定的单例作用域应该也是在 Component 的范围内。也就是说 @Scope 的作用范围其实就是单例能力范围，这个范围在单个的 Component 中. @Qualifiers 和 @Name @Name 只是被 @Qualifier 注解的一个注解。所以，它能够有效完全是因为 @Qualifier @Qualifier @Documented @Retention(RUNTIME) public @interface Named { /** The name. */ String value() default \"\"; } 完全可以自定义不同的注解，避免通过 Named 的方式容易拼错的问题。 @Qualifier @Documented @Target({ElementType.FIELD,ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) public @interface B { } 使用时 @Provides @Named(\"A\") public int provideIntA(){ return 111; } @Provides @B public int provideIntB(){ return 222; } @Inject @Named(\"A\") int testValueA; @Inject @B int testValueB; Dagger2 中的延迟加载 所谓的延迟加载，是当我们使用的时候再去实例化,比如以下方式。 public class TestLazy { String name; public String getName() { if ( name == null ) { name = \"TestLazy\"; } return name; } } Dagger2 提供了延迟加载能力。只需要通过 Lazy 就好了，Lazy 是泛型类，接受任何类型的参数。 public class TestLazy { @Inject @Named(\"TestLazy\") Lazy&lt;String> name; public String getName() { return name.get(); } } 多个component之间的依赖 @Component(modules = {ShangjiaAModule.class, XiaoChiModule.class},dependencies = XiaoChiComponent.class) SubComponentJava 软件开发中，我们经常面临的就是“组合”和“继承”的概念。它们都是为了扩展某个类的功能。前面的 Component 的依赖采用 @Component(dependecies=othercomponent.class) 就相当于组合。那么在 Dagger2 中，运用 @SubComponent 标记一个 Component 的行为相当于继承。 @Subcomponent(modules = FoodModule.class) public interface SubComponent { void inject(ThirdActivity activity); } @Component(modules = XiaoChiModule.class) public interface ParentComponent { SubComponent provideSubComponent(); } DaggerParentComponent.builder().build() .provideSubComponent().inject(this); 使用 Subcomponent 时，还是要先构造 ParentComponent 对象，然后通过它提供的 SubComponent 再去进行依赖注入。 大家可以细细观察下它与 depedency 方法的不同之处。 但是，SubComponent 同时具备了 ParentComponent 和自身的 @Scope 作用域。所以，这经常会造成混乱的地方。大家需要注意。 如果你要我比较，SubComponent 和 dependency 形式哪种更好时，我承认各有优点，但我自己倾向于 dependency，因为它更灵活。 不是说 组合优于继承嘛。 反射基于 javax.inject Dagger 2 是一个设计非常巧妙且粗暴的框架。为什么说巧妙呢？ Dagger 是为Android和Java平台提供的一个完全静态的，在编译时进行依赖注入的框架。 Dagger 通过生成中间代码，解决了基于反射带来的开发和性能上的问题。 Dagger 通过编译的时候进行依赖注入的框架。 Dagger 与其他依赖注入框架不同，它是通过apt插件在编译阶段生成相应的注入代码 Dagger 的目的为了降低程序耦合。 总之：他是在编译的时候完成了一切所需要的工作，提供一种使用方便、耦合度低、避免了通过反射带来的性能问题。 为什么说粗暴呢？使用的时候简单几个注解就可以了，但是 在编译阶段通过 apt 插件生成了全部的注入代码。而且代码量还不小。","link":"/2019/04/16/Android-Dagger2%E5%88%86%E6%9E%90/"},{"title":"Android 多线程开发实践","text":"1. 线程特点： 并行处理任务。 存在 线程数据安全、死锁、内存消耗、对象的生命周期管理、ui 的卡顿 等等。 存在优先级。 2. 线程优先级线程寄宿在进程当中，线程的生命周期直接被进程所影响，而进程的存活又和其优先级直接相关。在处理进程优先级的时候，大部分人靠直觉都能知道前台进程（Foreground Process）优先级要高于后台进程（Background Process）。但这种粗糙的划分无法满足操作系统高精度调度的需求。无论 Android 还是 iOS，系统对于 Foreground，Background 进程有进一步的细化。 2.1 Foreground ProcessForeground一般意味着用户双眼可见，可见却不一定是active。在Android的世界里，一个Activity处于前台之时，如果能采集用户的input事件，就可以判定为active，如果中途弹出一个Dialog，Dialog变成新的active实体，直接面对用户的操作。被部分遮挡的activity尽管依然可见，但状态却变为inactive。不能正确的区分visible和active 2.2 Background Process后台进程同样有更细的划分。所谓的Background可以理解为不可见（invisible）。对于不可见的任务，Android也有重要性的区分。重要的后台任务定义为Service，如果一个进程包含Service（称为Service Process），那么在“重要性”上就会被系统区别对待，其优先级自然会高于不包含Service的进程（称为Background Process），最后还剩一类空进程（Empty Process）。Empty Process初看有些费解，一个Process如果什么都不做，还有什么存在的必要。其实Empty Process并不Empty，还存在不少的内存占用。 在iOS的世界里，Memory被分为Clean Memory和Dirty Memory，Clean Memory是App启动被加载到内存之后原始占用的那一部分内存，一般包括初始的stack, heap, text, data等segment，Dirty Memory是由于用户操作所改变的那部分内存，也就是App的状态值。系统在出现Low Memory Warning的时候会首先清掉Dirty Memory，对于用户来说，操作的进度就全部丢失了，即使再次点击App图标，也是一切从头开始。但由于Clean Memory没有被清除，避免了从磁盘重新读取app数据的io损耗，启动会变快。这也是为什么很多人会感觉手机重启后，app打开的速度都比较慢。 同理Android世界当中的Empty Process还保存有App相关的Clean Memory，这部分Memory对于提升App的启动速度大有帮助。显而易见Empty Process的优先级是最低的。 线程状态 优先级 Active Top Visible High Service High Background Low Empty Low Android将线程分为多个group，其中两类group尤其重要。一类是default group，UI线程属于这一类。另一类是background group，工作线程应该归属到这一类。background group当中所有的线程加起来总共也只能分配到5～10%的time slice，剩下的全部分配给default group，这样设计显然能保证UI线程绘制UI的流畅性。 new Thread(new Runnable() { @Override public void run() { Process.setThreadPriority(Process.THREAD_PRIORITY_BACKGROUND); } }).start(); 所以在我们决定新启一个线程执行任务的时候，首先要问自己这个任务在完成时间上是否重要到要和UI线程争夺CPU资源。如果不是，降低线程优先级将其归于background group，如果是，则需要进一步的profile看这个线程是否造成UI线程的卡顿。 虽说Android系统在任务调度上是以线程为基础单位，设置单个thread的优先级也可以改变其所属的control groups，从而影响CPU time slice的分配。但进程的属性变化也会影响到线程的调度，当一个App进入后台的时候，该App所属的整个进程都将进入background group，以确保处于foreground，用户可见的新进程能获取到尽可能多的CPU资源。用adb可以查看不同进程的当前调度策略。 adb shell ps -P 当你的App重新被用户切换到前台的时候，进程当中所属的线程又会回归的原来的group。在这些用户频繁切换的过程当中，thread的优先级并不会发生变化，但系统在time slice的分配上却在不停的调整。 3. 是否真的需要新线程？开线程并不是提升App性能，解决UI卡顿的万金油。每一个新启的线程会消耗至少64KB的内存，系统在不同的线程之间switch context也会带来额外的开销。如果随意开启新线程，随着业务的膨胀，很容易在App运行的某个时间点发现几十个线程同时在运行。后果是原本想解决UI流畅性，却反而导致了偶现的不可控的卡顿。 移动端App新启线程一般都是为了保证UI的流畅性，增加App用户操作的响应度。但是否需要将任务放入工作线程需要先了解任务的瓶颈在哪，是i/o，gpu还是cpu？UI出现卡顿并不一定是UI线程出现了费时的计算，有可能是其它原因，比如layout层级太深。 尽量重用已有的工作线程（使用线程池）可以避免出现大量同时活跃的线程，比如对HTTP请求设置最大并发数。或者将任务放入某个串行的队列（HandlerThread）按顺序执行，工作线程任务队列适合处理大量耗时较短的任务，避免出现单个任务阻塞整个队列的情况。 用什么姿势开线程？ 常用的方式： new Thread(new Runnable() { @Override public void run() { } }).start(); 这种方式仅仅是起动了一个新的线程，没有任务的概念，不能做状态的管理。start之后，run当中的代码就一定会执行到底，无法中途取消。 ==Runnable作为匿名内部类还持有了外部类的引用，在线程退出之前，该引用会一直存在，阻碍外部类对象被GC回收，在一段时间内造成内存泄漏==。 没有线程切换的接口，要传递处理结果到UI线程的话，需要写额外的线程切换代码。 如果从UI线程启动，则该线程优先级默认为Default，归于default cgroup，会平等的和UI线程争夺CPU资源。这一点尤其需要注意，在对UI性能要求高的场景下要记得 Process.setThreadPriority(Process.THREAD_PRIORITY_BACKGROUND); 虽说处于background group的线程总共只能争取到5～10%的CPU资源，但这对绝大部分的后台任务处理都绰绰有余了，1ms和10ms对用户来说，都是快到无法感知，所以我们一般都偏向于在background group当中执行工作线程任务。 AsyncTaskpublic class MyAsyncTask extends AsyncTask { @Override protected Object doInBackground(Object[] params) { return null; } @Override protected void onPreExecute() { super.onPreExecute(); } @Override protected void onPostExecute(Object o) { super.onPostExecute(o); } } 和使用Thread()不同的是，多了几处API回调来严格规范工作线程与UI线程之间的交互。我们大部分的业务场景几乎都符合这种规范，比如去磁盘读取图片，缩放处理需要在工作线程执行，最后绘制到ImageView控件需要切换到UI线程。 AsyncTask的几处回调都给了我们机会去中断任务，在任务状态的管理上较之Thread()方式更为灵活。值得注意的是AsyncTask的cancel()方法并不会终止任务的执行，开发者需要自己去检查cancel的状态值来决定是否中止任务。 ==AsyncTask也有隐式的持有外部类对象引用的问题，需要特别注意防止出现意外的内存泄漏。== AsyncTask由于在不同的系统版本上串行与并行的执行行为不一致，被不少开发者所诟病，这确实是硬伤，绝大部分的多线程场景都需要明确任务是串行还是并行。 线程优先级为background，对UI线程的执行影响极小。 4. HandlerThread在需要对多任务做更精细控制，线程切换更频繁的场景之下，Thread()和AsyncTask都会显得力不从心。HandlerThread却能胜任这些需求甚至更多。 HandlerThread将Handler，Thread，Looper，MessageQueue几个概念相结合。Handler是线程对外的接口，所有新的message或者runnable都通过handler post到工作线程。Looper在MessageQueue取到新的任务就切换到工作线程去执行。不同的post方法可以让我们对任务做精细的控制，什么时候执行，执行的顺序都可以控制。HandlerThread最大的优势在于引入MessageQueue概念，可以进行多任务队列管理。 HandlerThread背后只有一个线程，所以任务是==串行==执行的。串行相对于并行来说更安全，各任务之间不会存在多线程安全问题。 ==HandlerThread所产生的线程会一直存活==，Looper会在该线程中持续的检查MessageQueue。这一点和Thread()，AsyncTask都不同，thread实例的重用可以避免线程相关的对象的频繁重建和销毁。 HandlerThread较之Thread()，AsyncTask需要写更多的代码，但在实用性，灵活度，安全性上都有更好的表现。 5. ThreadPoolExecutorThread(),AsyncTask适合处理单个任务的场景，HandlerThread适合串行处理多任务的场景。当需要并行的处理多任务之时，ThreadPoolExecutor是更好的选择。 public static Executor THREAD_POOL_EXECUTOR = new ThreadPoolExecutor(CORE_POOL_SIZE, MAXIMUM_POOL_SIZE, KEEP_ALIVE, TimeUnit.SECONDS, sPoolWorkQueue, sThreadFactory); 线程池可以避免线程的频繁创建和销毁，显然性能更好，但线程池并发的特性往往也是疑难杂症的源头，是代码降级和失控的开始。多线程并行导致的bug往往是偶现的，不方便调试，一旦出现就会耗掉大量的开发精力。 ThreadPool较之HandlerThread在处理多任务上有更高的灵活性，但也带来了更大的复杂度和不确定性。 6. IntentService不得不说Android在API设计上粒度很细，同一样工作可以通过各种不同的类来完成。IntentService又是另一种开工作线程的方式，从名字就可以看出这个工作线程会带有service的属性。和AsyncTask不同，没有和UI线程的交互，也不像HandlerThread的工作线程会一直存活。IntentService背后其实也有一个HandlerThread来串行的处理Message Queue，从IntentService的onCreate方法可以看出： @Override public void onCreate() { // TODO: It would be nice to have an option to hold a partial wakelock // during processing, and to have a static startService(Context, Intent) // method that would launch the service &amp; hand off a wakelock. super.onCreate(); HandlerThread thread = new HandlerThread(\"IntentService[\" + mName + \"]\"); thread.start(); mServiceLooper = thread.getLooper(); mServiceHandler = new ServiceHandler(mServiceLooper); } 只不过在所有的Message处理完毕之后，工作线程会自动结束。所以可以把IntentService看做是Service和HandlerThread的结合体，适合需要在工作线程处理UI无关任务的场景。 7. 结束语Android开线程的方式虽然五花八门，但归根到底最后还是映射到linux下的pthread，业务的设计还是脱不了和线程相关的基础概念范畴：线程的执行顺序，调度策略，生命周期，串行还是并行，同步还是异步等等。摸清楚各类API下线程的行为特点，在设计具体业务的线程模型的时候自然轻车熟路了，线程模型的设计要有整个app视角的广度，切忌各业务模块各玩各的。","link":"/2019/05/20/Android-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%BC%80%E5%8F%91%E5%AE%9E%E8%B7%B5/"},{"title":"Android 应用内存分析","text":"Android 开发场景中，分析内存泄漏情况 所需工具1. MTAEclipse Memory Analysis Tools (MAT) 是一个分析 Java堆数据的专业工具，用它可以定位内存泄漏的原因。 工具地址 : https://www.eclipse.org/mat/ 2. AndroidStudio monitor 存在 sdk/toools/ 下的 monitor Dalvik Debug Monitor Server (DDMS) 是 ADT插件的一部分，其中有两项功能可用于内存检查 : heap 查看堆的分配情况 allocation tracker跟踪内存分配情况 DDMS 这两项功能有助于找到内存泄漏的操作行为。 profiler AndroidStudio 内置的 内存检测工具，可以回收内存和下载dump。 使用 通过 AndroidStudio 内置工具 profiler 进行分析并获取到.hprof 文件。 操作步骤 打开 profiler 窗口。 切换到 memory . 针对需要检测的页面，点击 回收箱 按钮，进行强制 GC.然后 点击 下载图标 按钮 获取堆栈信息. 排序方式 Arrange by package ，查看待检测的泄漏对象。 导出 .hprof 文件。 通过 sdk 目录 platform-tools/hprof-conv 工具进行转换 hrpof-conv -z [待转换.hprof] [转换后.hprof] 以上转换后的 .hprof文件，通过 mat 打开，然后点击 histogram 。 进入Histogram可以点击表头进行排序,在表的第一行可以输入正则表达式来匹配结果,找到我们想要找的类，然后右键选择merge shortest paths to Gc roots 然后在选择exclude all phantom/weak/soft etc.references选项. 分析小技巧 Histogram 对比 为查找内存泄漏，通常需要两个 Dump结果作对比，打开 Navigator History面板，将两个表的 Histogram结果都添加到 Compare Basket中去 :添加好后，打开 Compare Basket面板，得到结果。 使用 android:largeHeap=&quot;true&quot;标记 (API Level &gt;= 11) 在 AndroidManifest.xml中的 Application节点中声明即可分配到更大的堆内存, android:largeHeap标记在 Android系统应用中也有广泛的应用 ,比如 Launcher, Browser这些内存大户上均有使用. mat 打开工具栏 QQL图标，可以使用 类sql 语句筛查select * from instanceof android.app.Activity mat 中 Actions 中的概念 # Histogram 列出内存中的对象，对象的个数以及大小。 # Dominator Tree 列出最大的对象以及其依赖存活的Object （大小是以Retained Heap为标准排序的）。 # Top Consumers 通过图形列出最大的object。 # Shallow heap Shallow size就是对象本身占用内存的大小，不包含其引用的对象。针对非数组类型的对象，它的大小就是对象与它所有的成员变量大小的总和。当然这里面还会包括一些java语言特性的数据存储单元。针对数组类型的对象，它的大小是数组元素对象的大小总和。 #Retained Heap 它表示如果一个对象被释放掉，那会因为该对象的释放而减少引用进而被释放的所有的对象（包括被递归释放的）所占用的heap大小。(间接引用的含义：A->B->C, C就是间接引用。不过，释放的时候还要排除被GC Roots直接或间接引用的对象。他们暂时不会被被当做Garbage）","link":"/2019/06/30/Android-%E5%BA%94%E7%94%A8%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90/"},{"title":"Android 性能优化总结","text":"用户手中好的应用指标： 满足需求 合理的交互 高性能 Android 性能优化总结 目标: 流畅 稳定 省电、省流量 安装包小 简称： 快、稳、省、小 建议：1. 流畅卡顿场景：UI 绘制、应用启动、页面跳转、事件响应 卡顿原因： 界面绘制。 主要原因是绘制的层级深、页面复杂、刷新不合理，由于这些原因导致卡顿的场景更多出现在 UI 和启动后的初始界面以及跳转到页面的绘制上。 数据处理。 导致这种卡顿场景的原因是数据处理量太大，一般分为三种情况，一是数据在处理 UI 线程，二是数据处理占用 CPU 高，导致主线程拿不到时间片，三是内存增加导致 GC 频繁，从而引起卡顿。 UI 绘制: 原理： Android 应用程序把经过Measure、Layout、Draw后的 surface 缓存数据，通过 SurfaceFlinger 服务把数据渲染到显示屏幕上， 通过 Android 的刷新机制来刷新数据。也就是说应用层负责绘制，系统层负责渲染，通过进程间通信把应用层需要绘制的数据传递到系统层服务，系统层服务通过刷新机制把数据更新到屏幕上。 卡顿根本原因： 绘制任务太重，绘制一帧内容耗时太长。 主线程太忙，根据系统传递过来的 VSYNC 信号来时还没准备好数据导致丢帧。 常用工具：通过性能分析工具 Profile GPU Rendering、TraceView、Systrace UI 性能分析找出出现问题的点，然后解决。 优化建议： 布局优化 避免过度绘制 启动优化 合理的刷新机制 在实现动画效果时，需要根据不同场景选择合适的动画框架来实现。有些情况下，可以用硬件加速方式来提供流畅度。 2. 稳定 Android 应用的稳定性定义很宽泛，影响稳定性的原因很多，比如内存使用不合理、代码异常场景考虑不周全、代码逻辑不合理等，都会对应用的稳定性造成影响。其中最常见的两个场景是：Crash 和 ANR，这两个错误将会使得程序无法使用，比较常用的解决方式如下： 2.1.内存优化虚拟机有两种运行模式：Dalvik 和 ART。 Android 内存控制权 Generational Heap Memory 。 对象的生命周期： 创建阶段-&gt;应用阶段-&gt;不可见阶段-&gt;不可达阶段-&gt;收集阶段-&gt;终结阶段-&gt;对象空间重新分配阶段 Zygote 进程是所有的应用程序进程之父。 每个应用 都有 Dalvik Heap Size 最大阈值。有 RAM 大小不同会有所差异。 内存回收young Generation(年轻代)、Old Generation(年老代)、Permanent Generation(持久代) 根据对象的生命周期、所处的代区域、不同的内存数据类型，执行不同的GC 操作。 分配的对象会存放在 Young Generation 区域。对象在某个时机触发 GC 回收垃圾，而没有回收的就根据不同规则，有可能被移动到 Old Generation，最后累积一定时间在移动到 Permanent Generation 区域。系统会根据内存中不同的内存数据类型分别执行不同的 GC 操作。GC 通过确定对象是否被活动对象引用来确定是否收集对象，进而动态回收无任何引用的对象占据的内存空间。但需要注意的是频繁的 GC 会增加应用的卡顿情况，影响应用的流畅性，因此需要尽量减少系统 GC 行为，以便提高应用的流畅度，减小卡顿发生的概率。 分析工具： Memory Monitor Heap Viewer Allocation Tracker Memory Analyzer Tool(MAT) LeakCanary 第三方库. 常见内存泄漏场景: 资源性对象未关闭。比如Cursor、File文件等，往往都用了一些缓冲，在不使用时，应该及时关闭它们。 注册对象未注销。比如事件注册后未注销，会导致观察者列表中维持着对象的引用。 类的静态变量持有大数据对象。 非静态内部类的静态实例. Handler临时性内存泄漏。如果Handler是非静态的，容易导致 Activity 或 Service 不会被回收。 容器中的对象没清理造成的内存泄漏。 WebView。WebView 存在着内存泄漏的问题，在应用中只要使用一次 WebView，内存就不会被释放掉。 优化内存空间： 对象引用。强引用、软引用、弱引用、虚引用四种引用类型，根据业务需求合理使用不同，选择不同的引用类型。 减少不必要的内存开销。注意自动装箱，增加内存复用，比如有效利用系统自带的资源、视图复用、对象池、Bitmap对象的复用。 使用最优的数据类型。比如针对数据类容器结构，可以使用ArrayMap数据结构，避免使用枚举类型，使用缓存Lrucache等等。 图片内存优化。可以设置位图规格，根据采样因子做压缩，用一些图片缓存方式对图片进行管理等等。 2.2. 提高代码质量 提高代码质量。比如开发期间的代码审核，看些代码设计逻辑，业务合理性等。代码静态扫描工具。常见工具有Android Lint、Findbugs、Checkstyle、PMD等等。Crash监控。把一些崩溃的信息，异常信息及时地记录下来，以便后续分析解决。Crash上传机制。在Crash后，尽量先保存日志到本地，然后等下一次网络正常时再上传日志信息。 3. 电量优化 优化耗时的计算。 4. 安装包大小 代码混淆。 资源优化。比如使用 Android Lint 删除冗余资源。 图片优化。比如利用 AAPT 工具对 PNG 格式的图片做压缩处理，降低图片色彩位数、使用 WebP图片格式等。 避免重复功能的库。 插件化。 参考 Android 应用内存分析","link":"/2019/07/16/Android-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"title":"Android 数据结构之 LinkedHashMap","text":"原理分析： LinkedHashMap 是 HashMap 的子类，其在 HashMap 的基础上只添加了一个双向链表和一个顺序模式属性，其每次 put 元素都会往这个双向链表上添加节点，其构造方法比 HashMap 多了一个 boolean 类型的 accessOrder 参数，当该参数为 true 时则按照元素最后访问时间在双向链表中排序，为 false 则按照插入顺序排序，默认为 false。 具体源码如下： public class LinkedHashMap&lt;K,V> extends HashMap&lt;K,V> implements Map&lt;K,V> { /** * The head of the doubly linked list. */ private transient LinkedHashMapEntry&lt;K,V> header; private final boolean accessOrder; public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) { super(initialCapacity, loadFactor); this.accessOrder = accessOrder; } } LinkedHashMap 继承自 HashMap，所以其 put 和 get 操作重写了父类的实现 /** * This override differs from addEntry in that it doesn't resize the * table or remove the eldest entry. */ void createEntry(int hash, K key, V value, int bucketIndex) { HashMapEntry&lt;K,V> old = table[bucketIndex]; LinkedHashMapEntry&lt;K,V> e = new LinkedHashMapEntry&lt;>(hash, key, value, old); table[bucketIndex] = e; e.addBefore(header); size++; } public V get(Object key) { LinkedHashMapEntry&lt;K,V> e = (LinkedHashMapEntry&lt;K,V>)getEntry(key); if (e == null) return null; e.recordAccess(this); return e.value; } private static class LinkedHashMapEntry&lt;K,V> extends HashMapEntry&lt;K,V> { LinkedHashMapEntry&lt;K,V> before, after; LinkedHashMapEntry(int hash, K key, V value, HashMapEntry&lt;K,V> next) { super(hash, key, value, next); } /** * Removes this entry from the linked list. */ private void remove() { before.after = after; after.before = before; } /** * Inserts this entry before the specified existing entry in the list. */ private void addBefore(LinkedHashMapEntry&lt;K,V> existingEntry) { after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; } /** * This method is invoked by the superclass whenever the value * of a pre-existing entry is read by Map.get or modified by Map.set. * If the enclosing Map is access-ordered, it moves the entry * to the end of the list; otherwise, it does nothing. */ void recordAccess(HashMap&lt;K,V> m) { LinkedHashMap&lt;K,V> lm = (LinkedHashMap&lt;K,V>)m; if (lm.accessOrder) { lm.modCount++; remove(); addBefore(lm.header); } } void recordRemoval(HashMap&lt;K,V> m) { remove(); } } 可以看到 LinkedHashMapMap 中所有 put 进来的 Entry 最终除过按照 HashMap 的 put 操作进行哈希表存储后又额外添加进了一个以 head 为头结点的双向循环链表尾部。==所以说 KinkedHashMap 完全具备 HashMap 的所有特性，也允许 key 和 value 为 null 值，此外自己比 HashMap 厉害的地方在于保证了访问的有序性==。 接着在进行迭代器访问时与 HashMap 的区别在于 LinkedHashMap 是直接迭代遍历操作其自己维护的双向有序链表，以此来保证顺序性，如下： private abstract class LinkedHashIterator&lt;T> implements Iterator&lt;T> { LinkedHashMapEntry&lt;K,V> nextEntry = header.after; LinkedHashMapEntry&lt;K,V> lastReturned = null; /** * The modCount value that the iterator believes that the backing * List should have. If this expectation is violated, the iterator * has detected concurrent modification. */ int expectedModCount = modCount; public boolean hasNext() { return nextEntry != header; } public void remove() { if (lastReturned == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); LinkedHashMap.this.remove(lastReturned.key); lastReturned = null; expectedModCount = modCount; } Entry&lt;K,V> nextEntry() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); if (nextEntry == header) throw new NoSuchElementException(); LinkedHashMapEntry&lt;K,V> e = lastReturned = nextEntry; nextEntry = e.after; return e; } } 所以 LinkedHashMap 继承自 HashMap，除过具备 HashMap 的一切优缺点外自身具备插入有序性或者访问有序性，其实现依赖 HashMap 自身的存储结构不变外在 put、get 操作处添加了一个对自己内部维护的双向有序链表的操作。 场景使用 LRU 的结合： LRU 是一种流行的替换算法，它的全称是 Least Recently Used，最近最少使用，常常在缓存设计的场景中充当一种策略，它的核心思路是最近刚被使用的很快再次被用的可能性最高，而最久没被访问的很快再次被用的可能性最低，所以被优先清理。 LRU 容器的实现 在添加元素到 LinkedHashMap 后会调用 removeEldesEntry方法，传递的参数是最久没被访问的键值对，如果这个方法返回true 则这个最久的键值对就会被删除，LinkedHashMap的实现总是返回false，所有容量没有限制。 LinkedHashMap 的实现protected boolean removeEldestEntry(Map.Entry&lt;K,V> eldest) { return false; } 重写后的实现 public class LRUCache&lt;k,v> extents LinkedHashMap&lt;k,v>{ private int maxEntries; public LRUCache(int maxEntries){ super(16,0.75,true); this.maxEntries=maxEntries; } @override protected boolean removeEldesEntry(Entry&lt;k,v> eldest){ return size()>maxentries; } }","link":"/2019/07/24/Android-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-LinkedHashMap/"},{"title":"JVM-Java 虚拟机入门","text":"1. JVM 整体架构 1.1 概述**JVM(虚拟机)**：指以软件的方式模拟具有完整硬件系统功能、运行在一个完全隔离环境中的完整计算机系统 ，是物理机的软件实现。常用的虚拟机有VMWare，Virtual Box，Java Virtual Machine Java虚拟机阵营：Sun HotSpot VM、BEA JRockit VM、IBM J9 VM、Azul VM、Apache Harmony、Google Dalvik VM、Microsoft JVM 1.2 JVM 构成由三个主要的子系统 类加载器子系统 运行时数据区（内存结构） 执行引擎 1.3 Java 编译器Java Compiler 运行时编译源码(.java)成字节码(.class)，由jre（Java Run Environment）运行。jre由java虚拟机（jvm）实现。Jvm 分析字节码，后解释并执行。 编译流程如下： 2. JVM类加载器2.1 类加载过程类加载过程：类加载器将class文件加载到虚拟机的内存的过程。 步骤如下： 加载：在硬盘上查找并通过IO读入字节码文件 连接：执行校验、准备、解析（可选）步骤 校验：校验字节码文件的正确性 准备：给类的静态变量分配内存，并赋予默认值 解析：类装载器装入类所引用的其他所有类 初始化：对类的静态变量初始化为指定的值，执行静态代码块 2.2 加载器种类 启动类加载器：负责加载JRE的核心类库，如jre目标下的rt.jar,charsets.jar等 扩展类加载器：负责加载JRE扩展目录ext中JAR类包 系统类加载器：负责加载ClassPath路径下的类包 用户自定义加载器：负责加载用户自定义路径下的类包 示例代码可以输出加载器名称： 2.3 类加载机制类加载过程种，有两种机制来保证加载的安全与避免重复加载： 全盘负责委托机制：当一个ClassLoader加载一个类时，除非显示的使用另一个ClassLoader，该类所依赖和引用的类也由这个ClassLoader载入。 双亲委派机制：指先委托父类加载器寻找目标类，而不是自己先找，在父类找不到的情况下，才不得已在自己的路径中查找并载入目标类。 上面两条机制可以简单总结就是：一人得道鸡犬升天 和 败家子模型。 双亲委派模式优势： 沙箱安全机制：自己写的String.class类不会被加载，这样便可以防止核心API库被随意篡改避免类的重复加载：当父亲已经加载了该类时，就没有必要子ClassLoader再 加载一次。 2.4 JVM加载jar包是否会将包里的所有类全部加载进内存？先说答案： JVM 对 class 文件是按需加载(运行期间动态加载)，非一次性加载。 下面通过一个例子来测试，注意：(启动需要加上参数：-verbose:class) public class TestDynamicLoad { static { System.out.println(&quot;*************static code************&quot;); } public static void main(String[] args){ new A(); System.out.println(&quot;*************load test************&quot;); new B(); } } class A{ public A(){ System.out.println(&quot;*************initial A************&quot;); } } class B{ public B(){ System.out.println(&quot;*************initial B************&quot;); } } [Loaded java.lang.Void from /Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/jre/lib/rt.jar] *************static code************ [Loaded com.study.jvm.A from file:/U/jvmtest/out/production/jvmtest/] *************initial A************ *************load test************ [Loaded com.study.jvm.B from file:/Us/jvmtest/out/production/jvmtest/] *************initial B************ 从上面输入日志可以发现，A 和 B 都是在加载之后开始输出构造函数内的日志。 2.5 jvm 如何知道我们的类在何方？我们的 class 信息存放在不同的地方，如不同文件夹里面的jar，项目bin目录，target目录等，jvm 怎么知道都需要加载哪些路径呢？ 通过查看 sun.misc.Launcher.AppClassLoader 源码可以发现，是通过读取 java.class.path 配置，通过这个指定去特定配置目录去加载。 通过 jps 和 jcmd 两个命令来验证 jps jcmd pid help jcmd pid VM.system_properties 3. JVM 内存结构3.1 内存结构结构图 本地方法栈(线程私有)：登记native方法，在Execution Engine执行时加载本地方法库 程序计数器（线程私有）：就是一个指针，指向方法区中的方法字节码（用来存储指向下一条指令的地址,也即将要执行的指令代码），由执行引擎读取下一条指令，是一个非常小的内存空间，几乎可以忽略不记。 Java栈（线程私有）：Java线程执行方法的内存模型，一个线程对应一个栈，每个方法在执行的同时都会创建一个栈帧（用于存储局部变量表，操作数栈，动态链接，方法出口等信息）==不存在垃圾回收问题==，只要线程一结束该栈就释放，生命周期和线程一致。 方法区(线程共享)：类的所有字段和方法字节码，以及一些特殊方法如构造函数，接口代码也在此定义。简单说，所有定义的方法的信息都保存在该区域，静态变量+常量+类信息(构造方法/接口定义)+运行时常量池都存在方法区中，虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做 Non-Heap（非堆），目的应该是与 Java 堆区分开来。 堆(线程共享)：虚拟机启动时创建，用于存放对象实例，几乎所有的对象（包含常量池）都在堆上分配内存，当对象无法再该空间申请到内存时将抛出OutOfMemoryError异常。同时也是垃圾收集器管理的主要区域。可通过 -Xmx –Xms 参数来分别指定最大堆和最小堆 JVM 对该区域规范了两种异常： 针对与栈：线程请求的栈深度大于虚拟机栈所允许的深度，将抛出 StackOverFlowError 异常 针对堆：若虚拟机栈可动态扩展，当无法申请到足够内存空间时将抛出OutOfMemoryError，通过jvm参数–Xss指定栈空间，空间大小决定函数调用的深度3.2 Java 栈 每个线程都有自己私有的栈，栈内部是由n个栈帧（函数/方法）组成。栈帧（函数/方法）内部又包含：局部变量表、操作数栈、动态链接、返回地址。 3.3 栈+堆+方法区的交互关系HotSpot 是使用==指针==的方式来访问对象Java堆中会存放访问==类元数据==的地址reference存储的就直接是对象的地址 3.4 堆 3.4.1 Young Generation 新生区类诞生、成长、消亡的区域，一个类在这里产生，应用，最后被垃圾回收器收集，结束生命。新生区分为两部分： 伊甸区（Eden space）和幸存者区（Survivor pace） ，所有的类都是在伊甸区被new出来的。 幸存区有两个： 0(From)区（Survivor 0 space）和1(To)区（Survivor 1 space）。 当伊甸园的空间用完时，程序又需要创建对象，JVM的垃圾回收器将对伊甸园区进行垃圾回收(Minor GC)，将伊甸园区中的不再被其他对象所引用的对象进行销毁。然后将伊甸园中的剩余对象移动到幸存 0区。若幸存 0区也满了，再对该区进行垃圾回收，然后移动到1区。那如果1区也满了呢？则 移动到 老年代。 3.4.2 old Generation 老年区新生区经过多次GC仍然存活的对象移动到老年区。若老年区也满了，那么这个时候将产生MajorGC（FullGC），进行老年区的内存清理。若老年区执行了Full GC 之后发现依然无法进行对象的保存，就会产生 OOM 异常“OutOfMemoryError” 3.4.3 MetaDataSpace 元数据区元数据区取代了永久代(jdk1.8以前)，本质和永久代类似，都是对JVM规范中方法区的实现，区别在于元数据区并不在虚拟机中，而是使用本地物理内存，永久代在虚拟机中，永久代逻辑结构上属于堆，但是物理上不属于堆，堆大小=新生代+老年代。元数据区也有可能发生OutOfMemory异常。 JAVA 版本 是否有永久代 Jdk1.6及之前 有永久代, 常量池在方法区 Jdk1.7 有永久代，但已经逐步“去永久代”，常量池在堆 Jdk1.8及之后 无永久代，常量池在元空间. 元数据区的动态扩展，默认 –XX:MetaspaceSize值为21MB的高水位线。一旦触及则Full GC将被触发并卸载没有用的类（类对应的类加载器不再存活），然后高水位线将会重置。新的高水位线的值取决于GC后释放的元空间。如果释放的空间少，这个高水位线则上升。如果释放空间过多，则高水位线下降。 为什么jdk1.8用元数据区取代了永久代？ 官方解释：移除永久代是为融合HotSpot JVM与 JRockit VM而做出的努力，因为JRockit没有永久代，不需要配置永久代。 JVM执行引擎执行引擎：读取运行时数据区的Java字节码并逐个执行","link":"/2020/07/01/JVM-Java-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%85%A5%E9%97%A8/"},{"title":"Dubbo详解","text":"一、前言 分布式架构的 有 3 种解决方案 基于反向代理的中心化架构 嵌入应用内部的去中心化架构 基于独立代理进程的Service Mesh架构 基于反向代理的集中式分布式架构这是最简单和传统做法，在服务消费者和生产者之间，代理作为独立一层集中部署，由独立团队(一般是运维或框架)负责治理和运维。常用的集中式代理有硬件负载均衡器(如F5)，或者软件负载均衡器(如Nginx)，这种软硬结合两层代理也是业内常见做法，兼顾配置的灵活性(Nginx比F5易于配置)。 Http+Nginx 方案总结 优点： 简单快速、几乎没有学习成本 适用场景： 轻量级分布式系统、局部分布式架构。 瓶颈： Nginx中心负载、Http传输、JSON序列化、开发效率、运维效率。 嵌入应用内部的去中心化架构这是很多互联网公司比较流行的一种做法，代理(包括服务发现和负载均衡逻辑)以客户库的形式嵌入在应用程序中。这种模式一般需要独立的服务注册中心组件配合，服务启动时自动注册到注册中心并定期报心跳，客户端代理则发现服务并做负载均衡。我们所熟悉的 duboo 和spring cloud Eureka +Ribbon/‘rɪbən/ 都是这种方式实现。 相比第一代架构它有以下特点几点： 去中心化，客户端直连服务端 动态注册和发现服务 高效稳定的网络传输 高效可容错的序列化 基于独立代理进程的架构(Service Mesh)这种做法是上面两种模式的一个折中，代理既不是独立集中部署，也不嵌入在客户应用程序中，而是作为独立进程部署在每一个主机上，一个主机上的多个消费者应用可以共用这个代理，实现服务发现和负载均衡，如下图所示。这个模式一般也需要独立的服务注册中心组件配合，作用同第二代架构。 三种架构的比较 模式 优点 缺点 适应场景 案例 集中式负载架构 简单 集中式治理 与语言无关 配置维护成本高 多了一层IO 单点问题 大部分公司都适用，对运维有要求 亿贝、携程、早期互联网公司 客户端嵌入式架构 无单点 性能更好 客户端复杂 语言栈要求 中大规模公司、语言栈统一 Dubbo 、 Twitter finagle、 Spring Cloud Ribbon 独立进程代理架构 无单点 性能更好 与语言无关 运维部署复杂 开发联调复杂 中大规模公司 对运维有要求 Smart Stack Service Mesh 二、什么是 Dubbodubbo 阿里开源的一个 SOA（面向服务的架构） 服务治理框架，从目前来看把它称作是一个RPC远程调用框架更为贴切。单从RPC框架来说，功能较完善，支持多种传输和序列化方案。所以想必大家已经知道他的核心功能了：就是远程调用。 流程说明： Provider(提供者)绑定指定端口并启动服务 指供者连接注册中心，并发本机IP、端口、应用信息和提供服务信息发送至注册中心存储 Consumer(消费者），连接注册中心 ，并发送应用信息、所求服务信息至注册中心 注册中心根据 消费 者所求服务信息匹配对应的提供者列表发送至Consumer 应用缓存。 Consumer 在发起远程调用时基于缓存的消费者列表择其一发起调用。 Provider 状态变更会实时通知注册中心、在由注册中心实时推送至Consumer 这么设计的意义： Consumer 与Provider 解偶，双方都可以横向增减节点数。 注册中心对本身可做对等集群，可动态增减节点，并且任意一台宕掉后，将自动切换到另一台 去中心化，双方不直接依懒注册中心，即使注册中心全部宕机短时间内也不会影响服务的调用 服务提供者无状态，任意一台宕掉后，不影响使用 Dubbo 设计层级 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成动态代理 扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封装 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool Dubbo SPI 机制Java SPI 的具体约定为:当服务的提供者，提供了服务接口的一种实现之后，在jar包的META-INF/services/目录里同时创建一个以服务接口命名的文件。该文件里就是实现该服务接口的具体实现类。而当外部程序装配这个模块的时候，就能通过该jar包META-INF/services/里的配置文件找到具体的实现类名，并装载实例化，完成模块的注入。 基于这样一个约定就能很好的找到服务接口的实现类，而不需要再代码里制定。jdk提供服务实现查找的一个工具类java.util.ServiceLoader META-INF/services/xxx.dubbo.server.UserService 中的值： xxx.dubbo.server.impl.UserServiceImpl2 装载获取SPI实现类： public static void main(String[] args) { Iterator&lt;UserService&gt; services = ServiceLoader.load(UserService.class).iterator(); UserService service = null; while (services.hasNext()) { service = services.next(); } System.out.println(service.getUser(111)); } Dubbo SPI 在JAVA自带的SPI基础上加入了扩展点的功能，即每个实现类都会对应至一个扩展点名称，其目的是 应用可基于此名称进行相应的装配。 dubbo spi 文件内容： luban=xxx.dubbo.server.LubanFilter 装配自定义Filter 想了解更详细信息：官方文档 三、Dubbo注册中心详解注册中心的作用为了到达服务集群动态扩容的目的，注册中心存储了服务的地址信息与可用状态信息，并实时推送给订阅了相关服务的客户端。 一个完整的注册中心需要实现以下功能： 接收服务端的注册与客户端的引用，即将引用与消费建立关联，并支持多对多。 当服务非正常关闭时能即时清除其状态 当注册中心重启时，能自动恢复注册数据，以及订阅请求 注册中心本身的集群 。 Dubbo所支持的注册中心 Multicast 注册中心 基于组网广播技术，只能用在局域网内，一般用于简单的测试服务 Zookeeper 注册中心(**推荐**) Zookeeper 是 Apacahe Hadoop 的子项目，是一个树型的目录服务，支持变更推送，适合作为 Dubbo 服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 Redis 注册中心 基于Redis的注册中心 Simple 注册中心 基于本身的Dubbo服务实现（SimpleRegistryService），不支持集群可作为自定义注册中心的参考，但不适合直接用于生产环境。 Redis 注册中心关于Redis注册中心我们需要了解两点， 1. 如何存储服务的注册与订阅关系redis 注册中心配置 &lt;dubbo:registry protocol=&quot;redis&quot; address=&quot;192.168.0.147:6379&quot;/&gt; 当我们启动两个服务端后发现，Reids中增加了一个Hash 类型的记录，其key为/dubbo/xxx.dubbo.server.UserService/providers。Value中分别存储了两个服务提供者的URL和有效期。 同样消费者也是类似其整体结构如下： //服务提供者注册信息 /dubbbo/com.xxx.teach.service.DemoService/providers dubbo://192.168.246.1:20880/XXX.DemoService=1542619052964 dubbo://192.168.246.2:20880/XXX.DemoService=1542619052964 //服务消费订阅信息 /dubbbo/com.xxx.teach.service.DemoService/consumers dubbo://192.168.246.1:20880/XXX.DemoService=1542619788641 主 Key 为服务名和类型 Map 中的 Key 为 URL 地址 Map 中的 Value 为过期时间，用于判断脏数据，脏数据由监控中心删除 2. 是当服务状态改变时如何即时更新？第二个问题 当提供者突然 宕机状态能即里变更吗？这里Dubbo采用的是定时心跳的机制 来维护服务URL的有效期，默认每30秒更新一次有效期。即URL对应的毫秒值。具体代码参见：com.alibaba.dubbo.registry.redis.RedisRegistry#expireExecutor com.alibaba.dubbo.registry.redis.RedisRegistry#deferExpiredcom.alibaba.dubbo.registry.integration.RegistryDirectorycom.alibaba.dubbo.registry.support.ProviderConsumerRegTable Zookeeper 注册中心关于Zookeeper 注册中心同样需要了解其存储结构和更新机制。Zookeper是一个树型的目录服务，本身支持变更推送相比 redis 的实现 Publish/Subscribe 功能更稳定。 Provider和Consumer向Zookeeper注册临时节点，当连接断开时删除相应的注册节点。 Consumer订阅 Providers节点的子节点，通过 watch 事件，当 Zookeeper 删除临时节点时，实时感知 Provider 的变化情况，实时同步自身的Invoker对象，保证 RPC的可用性。 结构： 源码查看 @Override protected void doRegister(URL url) { try { zkClient.create(toUrlPath(url), url.getParameter(Constants.DYNAMIC_KEY, true)); } catch (Throwable e) { throw new RpcException(\"Failed to register \" + url + \" to zookeeper \" + getUrl() + \", cause: \" + e.getMessage(), e); } } @Override public void create(String path, boolean ephemeral) { if (!ephemeral) { if (checkExists(path)) { return; } } int i = path.lastIndexOf('/'); // 把前面的先创建好 dubbo/com.xx.xx/providers if (i > 0) { // 创建持久节点 create(path.substring(0, i), false); } if (ephemeral) { //创建一个临时节点,node 节点（保存具体地址）。 createEphemeral(path); } else { // 创建持久节点 createPersistent(path); } } 失败重连：com.alibaba.dubbo.registry.support.FailbackRegistry 提供者突然断开：存储基于Zookeeper 临时节点机制实现，在客户端会话超时后（默认为40秒） Zookeeper 会自动删除所有临时节点。 // 创建临时节点 com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createEphemeral 在 zookeeper 断开的40秒内 如果 有客户端加入 会调用 已失效的提供者连接吗？ 答：不会，提供者宕机后 ，其与客户端的链接也随即断开，客户端在调用前会检测长连接是否可用状态的方法。 // 检测连接是否有效 com.alibaba.dubbo.rpc.protocol.dubbo.DubboInvoker#isAvailable 创建 configurators与 routers 会通过 持久节点来创建, 比如：“dubbo/com.xx.xx/providers” com.alibaba.dubbo.remoting.zookeeper.curator.CuratorZookeeperClient#createPersistent 服务订阅机制实现： // 注册目录 com.alibaba.dubbo.registry.integration.RegistryDirectory 三、 Dubbo 调用模块dubbo调用模块核心功能是发起一个远程方法的调用并顺利拿到返回结果，其体系组成如下： 透明代理： 通过动态代理技术，屏蔽远程调用细节以提高编程友好性。 负载均衡： 当有多个提供者是，如何选择哪个进行调用的负载算法。 容错机制： 当服务调用失败时采取的策略 调用方式： 支持同步调用、异步调用 透明代理：针对代理 dubbo 提供 三种方式： 默认方式是： public static final String DEFAULT_PROXY = &quot;javassist&quot;; 调用链如下： > com.alibaba.dubbo.config.ReferenceConfig#createProxy --> com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory ----> com.alibaba.dubbo.common.bytecode.Proxy#getProxy(java.lang.ClassLoader, java.lang.Class&lt;?>...) ------> com.alibaba.dubbo.common.bytecode.ClassGenerator#newInstance(java.lang.ClassLoader) 负载均衡Dubbo 目前官方支持以下负载均衡策略： 随机(random)：按权重设置随机概率。此为默认算法. 轮循(roundrobin):按公约后的权重设置轮循比率。 最少活跃调用数(leastactive):相同活跃数的随机，活跃数指调用前后计数差。 一致性Hash(consistenthash ):相同的参数总是发到同一台机器 设置方式支持如下四种方式设置，优先级由低至高 &lt;!-- 服务端级别--> &lt;dubbo:service interface=\"...\" loadbalance=\"roundrobin\" /> &lt;!-- 客户端级别--> &lt;dubbo:reference interface=\"...\" loadbalance=\"roundrobin\" /> &lt;!-- 服务端方法级别--> &lt;dubbo:service interface=\"...\"> &lt;dubbo:method name=\"...\" loadbalance=\"roundrobin\"/> &lt;/dubbo:service> &lt;!-- 客户端方法级别--> &lt;dubbo:reference interface=\"...\"> &lt;dubbo:method name=\"...\" loadbalance=\"roundrobin\"/> &lt;/dubbo:reference> 容错Dubbo 官方目前支持以下容错策略： 失败自动切换： 调用失败后基于retries=“2” 属性重试其它服务器 快速失败： 快速失败，只发起一次调用，失败立即报错。 勿略失败： 失败后勿略，不抛出异常给客户端。 失败重试： 失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作 并行调用: 只要一个成功即返回，并行调用指定数量机器，可通过 forks=”2” 来设置最大并行数。 广播调用： 广播调用所有提供者，逐个调用，任意一台报错则报错 设置方式支持如下两种方式设置，优先级由低至高 &lt;!-- Failover 失败自动切换 retries=\"1\" 切换次数 Failfast 快速失败 Failsafe 勿略失败 Failback 失败重试，5秒后仅重试一次 Forking 并行调用 forks=\"2\" 最大并行数 Broadcast 广播调用 --> &lt;dubbo:service interface=\"...\" cluster=\"broadcast\" /> &lt;dubbo:reference interface=\"...\" cluster=\"broadcast\"/ > 注：容错机制 在基于 API设置时无效 如 referenceConfig.setCluster(“failback”); 经测试不启作用 异步调用异步调用是指发起远程调用之后获取结果的方式。 同步等待结果返回（默认） 异步等待结果返回 不需要返回结果 异步调用配置: &lt;dubbo:reference id=&quot;asyncDemoService&quot; interface=&quot;com.xxx.teach.service.async.AsyncDemoService&quot;&gt; &lt;!-- 异步调async：true 异步调用 false 同步调用--&gt; &lt;dubbo:method name=&quot;sayHello1&quot; async=&quot;false&quot;/&gt; &lt;dubbo:method name=&quot;sayHello2&quot; async=&quot;false&quot;/&gt; &lt;dubbo:method name=&quot;notReturn&quot; return=&quot;false&quot;/&gt; &lt;/dubbo:reference&gt; 注：在进行异步调用时 容错机制不能为 cluster=”forking” 或 cluster=”broadcast” 异步调用结果获取Demo demoService.sayHello1(\"han\"); Future&lt;Object> future1 = RpcContext.getContext().getFuture(); demoService.sayHello2(\"han2\"); Future&lt;Object> future2 = RpcContext.getContext().getFuture(); Object r1 = null, r2 = null; // wait 直到拿到结果 或超时 r1 = future1.get(); // wait 直到拿到结果 或超时 r2 = future2.get(); 过滤器** 类似于 WEB 中的Filter ，Dubbo本身提供了Filter 功能用于拦截远程方法的调用。其支持自定义过滤器与官方的过滤器使用：**#TODO 演示添加日志访问过滤: &lt;dubbo:provider filter=&quot;accesslog&quot; accesslog=&quot;logs/dubbo.log&quot;/&gt; 以上配置 就是 为 服务提供者 添加 日志记录过滤器， 所有访问日志将会集中打印至 accesslog 当中。 调用通信 内部实现原理 IO模型： BIO 同步阻塞 NIO 同步非阻塞 AIO 异步非阻塞 连接模型： 长连接 短连接 线程分类： IO线程 服务端业务线程 客户端调度线程 客户端结果exchange线程。 保活心跳线程 重连线程 线程池模型： 固定数量线程池 缓存线程池 有限线程池 Dubbo 长连接实现与配置初始连接： 引用服务增加提供者 —&gt; 获取连接 –&gt; 是否获取共享连接 –&gt;创建连接客户端–&gt;开启心跳检测状态检查定时任务–&gt; 开启连接状态检测. com.alibaba.dubbo.rpc.protocol.dubbo.DubboProtocol#getClients 心跳发送：在创建一个连接客户端同时也会创建一个心跳客户端，客户端默认基于60秒发送一次心跳来保持连接的存活，可通过 heartbeat 设置。 com.alibaba.dubbo.remoting.exchange.support.header.HeaderExchangeClient#startHeatbeatTimer 断线重连：每创建一个客户端连接都会启动一个定时任务每两秒中检测一次当前连接状态，如果断线则自动重连。 com.alibaba.dubbo.remoting.transport.AbstractClient#initConnectStatusCheckCommand 连接销毁:基于注册中心通知，服务端断开后销毁 com.alibaba.dubbo.remoting.transport.AbstractClient#close() Dubbo 传输协作线程 客户端调度线程：用于发起远程方法调用的线程。 客户端结果Exchange线程： 当远程方法返回response后由该线程填充至指定ResponseFuture，并叫醒等待的调度线程。 客户端IO线程 由传输框架实现，用于request 消息流发送、response 消息流读取与解码等操作。 服务端IO线程：由传输框架实现，用于request消息流读取与解码 与Response发送。 业务执行线程： 服务端具体执行业务方法的线程 客户端线程协作： 调度线程 调用远程方法 对request 进行协议编码 发送request 消息至IO线程 等待结果的获取 IO线程 读取response流 response 解码 提交Exchange 任务 Exchange线程 填写response值 至 ResponseFuture 唤醒调度线程，通知其获取结果 服务端线程协作： IO线程： request 流读取 request 解码 提交业务处理任务 业务线程： 业务方法执行 response 编码 回写结果至channel 线程池 fixed： 固定线程池,此线程池启动时即创建固定大小的线程数，不做任何伸缩， cached： 缓存线程池,此线程池可伸缩，线程空闲一分钟后回收，新请求重新创建线程 Limited： 有限线程池,此线程池一直增长，直到上限，增长后不收缩。 四、Dubbo 协议RPC 协议名词解释在一个典型RPC的使用场景中，包含了服务发现、负载、容错、网络传输、序列化等组件，其中RPC协议就指明了程序如何进行网络传输和序列化 。也就是说一个RPC协议的实现就等于一个非透明的远程调用实现，如何做到的的呢？ 地址：服务提供者地址 端口：协议指定开放的端口 报文编码：协议报文编码 ，分为请求头和请求体两部分。 序列化方式：将请求体序列化成对象 运行服务: 网络传输实现 dubbo 支持的RPC协议 名称 实现描述 连接描述 适用场景 dubbo 传输服务: mina, netty(默认), grizzy序列化: hessian2(默认), java, fastjson自定义报文 单个长连接NIO异步传输 1、常规RPC调用2、传输数据量小3、提供者少于消费者 rmi 传输：java rmi 服务序列化：java原生二进制序列化 多个短连接BIO同步传输 1、常规RPC调用2、与原RMI客户端集成3、可传少量文件4、不支持防火墙穿透 hessian 传输服务：servlet容器序列化：hessian二进制序列化 基于Http 协议传输，依懒servlet容器配置 1、提供者多于消费者2、可传大字段和文件3、跨语言调用 http 传输服务：servlet容器序列化：java原生二进制序列化 依懒servlet容器配置 1、数据包大小混合 thrift 与thrift RPC 实现集成，并在其基础上修改了报文头 长连接、NIO异步传输 关于RMI不支持防火墙穿透的补充说明： 原因在于RMI 底层实现中会有两个端口，一个是固定的用于服务发现的注册端口，另外会生成一个**随机**端口用于网络传输。因为这个随机端口就不能在防火墙中提前设置开放开。所以存在防火墙穿透问题 协议的使用与配置:Dubbo框架配置协议非常方便，用户只需要在 provider 应用中 配置*&lt;**dubbo:protocol&gt;*元素即可。 &lt;!-- name: 协议名称 dubbo|rmi|hessian|http| host:本机IP可不填，则系统自动获取 port：端口、填-1表示系统自动选择 server：运行服务 mina|netty|grizzy|servlet|jetty serialization：序列化方式 hessian2|java|compactedjava|fastjson 详细配置参见dubbo 官网 dubbo.io --> &lt;dubbo:protocol name=\"dubbo\" host=\"192.168.0.11\" port=\"20880\" server=\"netty\" serialization=“hessian2” charset=“UTF-8” /> dubbo 支持的序列化： 特点 fastjson 文本型：体积较大，性能慢、跨语言、可读性高 fst 二进制型：体积小、兼容 JDK 原生的序列化。要求 JDK 1.7 支持。 hessian2 二进制型：跨语言、容错性高、体积小 java 二进制型：在JAVA原生的基础上 可以写入Null compactedjava 二进制型：与java 类似，内容做了压缩 nativejava 二进制型：原生的JAVA 序列化 kryo 二进制型：体积比hessian2 还要小，但容错性 没有hessian2 好 Hessian 序列化 参数及返回值需实现 Serializable 接口 参数及返回值不能自定义实现 List,Map,Number,Date,Calendar等接口，只能用 JDK 自带的实现，因为 hessian 会做特殊处理，自定义实现类中的属性值都会丢失。 Hessian 序列化，只传成员属性值和值的类型，不传方法或静态变量，兼容情况 [1][2]： 数据通讯 情况 结果 A-&gt;B 类A多一种 属性（或者说类B少一种 属性） 不抛异常，A多的那 个属性的值，B没有， 其他正常 A-&gt;B 枚举A多一种 枚举（或者说B少一种 枚举），A使用多 出来的枚举进行传输 抛异常 A-&gt;B 枚举A多一种 枚举（或者说B少一种 枚举），A不使用 多出来的枚举进行传输 不抛异常，B正常接 收数据 A-&gt;B A和B的属性 名相同，但类型不相同 抛异常 A-&gt;B serialId 不相同 正常传输 接口增加方法，对客户端无影响，如果该方法不是客户端需要的，客户端不需要重新部署。输入参数和结果集中增加属性，对客户端无影响，如果客户端并不需要新属性，不用重新部署。输入参数和结果集属性名变化，对客户端序列化无影响，但是如果客户端不重新部署，不管输入还是输出，属性名变化的属性值是获取不到的。 总结：服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。 五、RPC协议报文编码与实现详解RPC 传输实现：RPC的协议的传输是基于 TCP/IP 做为基础使用Socket 或Netty、mina等网络编程组件实现。但有个问题是TCP是面向字节流的无边边界协议，其只管负责数据传输并不会区分每次请求所对应的消息，这样就会出现TCP协义传输当中的拆包与粘包问题 拆包与粘包产生的原因：我们知道tcp是以流动的方式传输数据，传输的最小单位为一个报文段（segment）。tcp Header中有个Options标识位，常见的标识为mss(Maximum Segment Size)指的是，连接层每次传输的数据有个最大限制MTU(Maximum Transmission Unit)，一般是1500比特，超过这个量要分成多个报文段，mss则是这个最大限制减去TCP的header，光是要传输的数据的大小，一般为1460比特。换算成字节，也就是180多字节。 tcp为提高性能，发送端会将需要发送的数据发送到缓冲区，等待缓冲区满了之后，再将缓冲中的数据发送到接收方。同理，接收方也有缓冲区这样的机制，来接收数据。这时就会出现以下情况： 应用程序写入的数据大于MSS大小，这将会发生拆包。 应用程序写入数据小于MSS大小，这将会发生粘包。 接收方法不及时读取套接字缓冲区数据，这将发生粘包。 拆包与粘包解决办法： 设置定长消息，服务端每次读取既定长度的内容作为一条完整消息。 {“type”:”message”,”content”:”hello”}\\n 使用带消息头的协议、消息头存储消息开始标识及消息长度信息，服务端获取消息头的时候解析出消息长度，然后向后读取该长度的内容。 比如： Http协议 heade 中的 Content-Length 就表示消息体的大小。 Dubbo 协议报文编码：dubbo 协议报文编码： magic：类似java字节码文件里的魔数，用来判断是不是dubbo协议的数据包。魔数是常量0xdabb,用于判断报文的开始。 flag：标志位, 一共8个地址位。低四位用来表示消息体数据用的序列化工具的类型（默认hessian），高四位中，第一位为1表示是request请求，第二位为1表示双向传输（即有返回response），第三位为1表示是心跳ping事件。 status：状态位, 设置请求响应状态，dubbo定义了一些响应的类型。具体类型见 com.alibaba.dubbo.remoting.exchange.Response invoke id： 消息id, long 类型。每一个请求的唯一识别id（由于采用异步通讯的方式，用来把请求request和返回的response对应上） body length： 消息体 body 长度, int 类型，即记录Body Content有多少个字节。 com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec#encodeRequestData() @Override protected void encodeRequestData(Channel channel, ObjectOutput out, Object data, String version) throws IOException { RpcInvocation inv = (RpcInvocation) data; //版本号 out.writeUTF(version); //接口路径 out.writeUTF(inv.getAttachment(Constants.PATH_KEY)); //接口版本 out.writeUTF(inv.getAttachment(Constants.VERSION_KEY)); //方法名称 out.writeUTF(inv.getMethodName()); //参数类型 out.writeUTF(ReflectUtils.getDesc(inv.getParameterTypes())); //参数值 Object[] args = inv.getArguments(); if (args != null) for (int i = 0; i &lt; args.length; i++) { out.writeObject(encodeInvocationArgument(channel, inv, i)); } out.writeObject(inv.getAttachments()); } com.alibaba.dubbo.rpc.protocol.dubbo.DubboCodec#encodeResponseData() @Override protected void encodeResponseData(Channel channel, ObjectOutput out, Object data, String version) throws IOException { Result result = (Result) data; // currently, the version value in Response records the version of Request boolean attach = Version.isSupportResponseAttatchment(version); Throwable th = result.getException(); if (th == null) { Object ret = result.getValue(); if (ret == null) { out.writeByte(attach ? RESPONSE_NULL_VALUE_WITH_ATTACHMENTS : RESPONSE_NULL_VALUE); } else { out.writeByte(attach ? RESPONSE_VALUE_WITH_ATTACHMENTS : RESPONSE_VALUE); out.writeObject(ret); } } else { out.writeByte(attach ? RESPONSE_WITH_EXCEPTION_WITH_ATTACHMENTS : RESPONSE_WITH_EXCEPTION); out.writeObject(th); } if (attach) { // returns current version of Response to consumer side. result.getAttachments().put(Constants.DUBBO_VERSION_KEY, Version.getProtocolVersion()); out.writeObject(result.getAttachments()); } } 解码 @Override protected Object decodeBody(Channel channel, InputStream is, byte[] header) throws IOException { byte flag = header[2], proto = (byte) (flag &amp; SERIALIZATION_MASK); // 获取 request id long id = Bytes.bytes2long(header, 4); //判断是request 解码还是 response 解码 if ((flag &amp; FLAG_REQUEST) == 0) { //客户端： 解码 response. Response res = new Response(id); if ((flag &amp; FLAG_EVENT) != 0) { res.setEvent(Response.HEARTBEAT_EVENT); } // get status. byte status = header[3]; res.setStatus(status); try { if (status == Response.OK) { Object data; if (res.isHeartbeat()) { data = decodeHeartbeatData(channel, CodecSupport.deserialize(channel.getUrl(), is, proto)); } else if (res.isEvent()) { data = decodeEventData(channel, CodecSupport.deserialize(channel.getUrl(), is, proto)); } else { DecodeableRpcResult result; if (channel.getUrl().getParameter( Constants.DECODE_IN_IO_THREAD_KEY, Constants.DEFAULT_DECODE_IN_IO_THREAD)) { result = new DecodeableRpcResult(channel, res, is, (Invocation) getRequestData(id), proto); result.decode(); } else { result = new DecodeableRpcResult(channel, res, new UnsafeByteArrayInputStream(readMessageData(is)), (Invocation) getRequestData(id), proto); } data = result; } res.setResult(data); } else { res.setErrorMessage(CodecSupport.deserialize(channel.getUrl(), is, proto).readUTF()); } } catch (Throwable t) { if (log.isWarnEnabled()) { log.warn(\"Decode response failed: \" + t.getMessage(), t); } res.setStatus(Response.CLIENT_ERROR); res.setErrorMessage(StringUtils.toString(t)); } return res; } else { // 服务端 ：解码 request. Request req = new Request(id); req.setVersion(Version.getProtocolVersion()); req.setTwoWay((flag &amp; FLAG_TWOWAY) != 0); if ((flag &amp; FLAG_EVENT) != 0) { req.setEvent(Request.HEARTBEAT_EVENT); } try { Object data; if (req.isHeartbeat()) { data = decodeHeartbeatData(channel, CodecSupport.deserialize(channel.getUrl(), is, proto)); } else if (req.isEvent()) { data = decodeEventData(channel, CodecSupport.deserialize(channel.getUrl(), is, proto)); } else { DecodeableRpcInvocation inv; if (channel.getUrl().getParameter( Constants.DECODE_IN_IO_THREAD_KEY, Constants.DEFAULT_DECODE_IN_IO_THREAD)) { inv = new DecodeableRpcInvocation(channel, req, is, proto); inv.decode(); } else { inv = new DecodeableRpcInvocation(channel, req, new UnsafeByteArrayInputStream(readMessageData(is)), proto); } data = inv; } req.setData(data); } catch (Throwable t) { if (log.isWarnEnabled()) { log.warn(\"Decode request failed: \" + t.getMessage(), t); } // bad request req.setBroken(true); req.setData(t); } return req; } } 六、最后 负载均衡：多个机器调用哪一台? 答：多种负载均衡策略。 服务发现：怎样发现新的服务地址呢？ 答：服务注册与发现。 健康检测：服务关宕机或恢复后怎么办？ 答：两种情况，消费者还是用老的请求，则 容错：如果调用其中一台调用出错了怎么办？ 答：容错策略。 Dubbo官方文档","link":"/2020/06/02/Dubbo%E8%AF%A6%E8%A7%A3/"},{"title":"JVM 类加载和初始化","text":"Java 类加载和初始化我们知道 当要实例化一个类时，JVM会首先加载该类，并且在加载过程中检查这个类是否有静态属性以及静态代码块，如果有，就按顺序分配内存并初始化他们，并且只在类加载的过程中初始化一次。 对于构造代码块，以及普通属性，是在类实例化时进行的，并且每次实例化都会调用，并且普通属性先于构造代码块，构造代码块先于构造方法执行。 public class Main { //1 private static String staticValue = \"第一步 main 静态属性\" + System.currentTimeMillis(); //3 private String value = \"第三步 main 普通属性\" + System.currentTimeMillis(); //2 static { System.out.println(System.currentTimeMillis() + \"\"); try { Thread.sleep(1000); } catch (InterruptedException e) { } System.out.println(staticValue); System.out.println(System.currentTimeMillis() + \"\"); System.out.println(\"第二步 main 静态代码块\"); } //4 { System.out.println(System.currentTimeMillis() + \"\"); try { Thread.sleep(1000); } catch (InterruptedException e) { } System.out.println(\"第四步 main 构造代码块 \"+System.currentTimeMillis()); //通过查看时间戳，常规属性是在 构造代码块之前就初始化。 System.out.println(value); } //5 private Main() { System.out.println(\"第五步 main 构造方法\"); } public static String getString() { System.out.println(\"getString()\"); return \"\"; } public static void main(String[] args) { Main main = new Main(); } } 控制台输出： 1565683686844 第一步 main 静态属性1565683686844 1565683687848 第二步 main 静态代码块 1565683687849 第四步 main 构造代码块 1565683688852 第三步 main 普通属性1565683687849 第五步 main 构造方法 可以发现第三步和第四步顺序是掉换的，但是根据时间戳知道，第三步是发生在第四部之前，也就是说第三步的普通属性的初始化在构造代码块之前执行。 类什么时候加载？加载时机： 其他类引用时候（被使用的时候）。 类初始化（自己初始化）。 加载工具： Classloader(类加载器) 类加载器：Bootstrap Loader(用于加载核心 jar)、Extension class Loader（扩展 jar）、Application class Loader（应用 jar）. 同一个类加载器对于类名一样的类，只会加载一次。单纯的加载 ，类的一切都不会执行。当类满足以下两个条件会被卸载 该类的所有实例都被 GC 加载该类的类加载器被 GC 类初始化的方式 实例通过使用new()关键字创建或者使用class.forName()反射，但它有可能导致ClassNotFoundException。 类的静态方法被调用。 类的静态域（静态变量）被赋值。 静态域被访问，而且不是常量（final）。 顶层类中执行 assert 语句。 类初始化规则： 类从顶至底部顺序初始化，所以声明在顶部的字段的早于底部的字段初始化 超类早于子类和衍生类的初始化 如果类的初始化是由于访问静态域而触发，那么只有声明静态域的类才被初始化，而不会触发超类的初始化或者子类的初始化即使静态域被子类或子接口或者它的实现类所引用。 接口初始化不会导致父接口的初始化。 静态域的初始化是在类的静态初始化期间，非静态域的初始化时在类的实例创建期间。这意味这静态域初始化在非静态域之前。 非静态域通过构造器初始化，子类在做任何初始化之前构造器会隐含地调用父类的构造器，他保证了非静态或实例变量（父类）初始化早于子类。 名词说明类的初始化和类的实例化是不同的:类的初始化发生在类实例化之前。类的初始化是指类加载过程中的初始化阶段对类变量按照程序猿的意图进行赋值的过程；而类的实例化是指在类完全加载到内存中后创建对象的过程。 静态域=静态变量 private static String staticValue =\"静态域\"; 静态域：如果将类中的域定义为static，则这个域属于这个类，而不属于这个类的某个对象，每个类中只有 一个这样的域，而每一个类对象对于所有的实例域(即没有定义为static的域)都有自己的一份拷贝。例如： class Employee { private int id; private static String staticValue =\"静态域\"; } 如果有1000个Employee对象，则有1000个实例域 id，但是只有一个静态域 staticValue；即使没有一个Employee对象，静态域nextId也存在，它属于类，不属于任何对象。 实例域=非静态变量 private String staticValue =\"实例域\"; 静态常量=final private final static String staticValue =\"静态常量\"; 静态方法 public static String getString() { System.out.println(\"getString()\"); return \"\"; } 静态代码块 static { } 构造代码块 { System.out.println(\"构造代码块\"); } 构造方法 class main{ private main(){ System.out.println(\"构造方法\"); } } 参考：类加载器","link":"/2019/07/30/JVM-%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%88%9D%E5%A7%8B%E5%8C%96/"},{"title":"JVM- 虚拟机调优工具","text":"1. JVM 调优监控命令行工具介绍 1.1 jps查看运行进程 以及进程 id。 1.2 Jinfo查看正在运行的Java应用程序的扩展参数 查看jvm的参数 jinfo -flags 30880 查看java系统参数 jinfo -sysprops 30880 1.3 jstatjstat 命令可以查看堆内存各部分的使用量，以及加载类的数量。命令的格式如下：jstat [-命令选项] [vmid] [间隔时间/毫秒] [查询次数]注意：使用的jdk版本是jdk8. 类加载统计 jstat -class 5989 标题 含义 Loaded 加载class的数量 Bytes 所占用空间大小 Unloaded 未加载数量 Bytes 未加载占用空间 Time 时间 垃圾回收统计 jstat -gc 5989 标题 含义 S0C 第一个幸存区的大小 S1C 第二个幸存区的大小 S0U 第一个幸存区的使用大小 S1U 第二个幸存区的使用大小 EC 伊甸园区的大小 EU 伊甸园区的使用大小 OC 老年代大小 OU 老年代使用大小 MC 方法区大小(元空间) MU 方法区使用大小 CCSC 压缩类空间大小 CCSU 压缩类空间使用大小 YGC 年轻代垃圾回收次数 YGCT 年轻代垃圾回收消耗时间 FGC 老年代垃圾回收次数 FGCT 老年代垃圾回收消耗时间 GCT 垃圾回收消耗总时间 堆内存统计 jstat -gccapacity 5989 标题 含义 NGCMN 新生代最小容量 NGCMX 新生代最大容量 NGC 当前新生代容量 S0C 第一个幸存区大小 S1C 第二个幸存区的大小 EC 伊甸园区的大小 OGCMN 老年代最小容量 OGCMX 老年代最大容量 OGC 当前老年代大小 OC 当前老年代大小 MCMN 最小元数据容量 MCMX 最大元数据容量 MC 当前元数据空间大小 CCSMN 最小压缩类空间大小 CCSMX 最大压缩类空间大小 CCSC 当前压缩类空间大小 YGC 年轻代gc次数 FGC 老年代GC次数 新生代垃圾回收统计 jstat -gcnew 13988 新生代内存统计 jstat -gcnewcapacity 13988 老年代垃圾回收统计 jstat -gcold 13988 老年代内存统计jstat -gcoldcapacity 13988 元数据空间统计jstat -gcmetacapacity 13988 1.4 jmap此命令可以用来查看内存信息,查询内存溢出（OOM）可以通过这个命令。 实例个数以及占用内存大小 jmap -histo 8899 &gt;./log.txt 标题 含义 num 序号 instances 实例数量 bytes 占用空间大小 class name 类名称 堆信息 jmap -heap 8899 堆内存dump jmap -dump:format=b,file = log.hprof 8899 也可以设置内存溢出自动导出dump文件(内存很大的时候，可能会导不出来) -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=./ （路径） 可以用 jvisualvm 命令工具导入该 dump文件分析 1.5 Jstack查看堆栈信息,栈溢出或死锁可以通过这个工具。 jstack 88999 用 jstack 查找死锁，见如下示例，也可以用jvisualvm 查看死锁 public class DeadLockTest { private static Object lock1 = new Object(); private static Object lock2 = new Object(); public static void main(String[] args) { new Thread(new Runnable() { public void run() { synchronized (lock1) { try { System.out.println(\"thread1 begin\"); Thread.sleep(5000); } catch (InterruptedException e) { } synchronized (lock2) { System.out.println(\"thread1 end\"); } } } }).start(); new Thread(new Runnable() { public void run() { synchronized (lock2) { try { System.out.println(\"thread2 begin\"); Thread.sleep(5000); } catch (InterruptedException e) { } synchronized (lock1) { System.out.println(\"thread2 end\"); } } } }).start(); System.out.println(\"main thread end\"); } } 2. 可视化工具 jvisualvmjvisualvm 2.1 启动jvisualvm的远程链接启动普通的jar程序JMX端口配置： java -Dcom.sun.management.jmxremote.port=12345 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -jar foo.jar tomcat的JMX配置 JAVA_OPTS=-Dcom.sun.management.jmxremote.port=8999 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false jvisualvm 远程连接服务需要在远程服务器上配置host(连接ip 主机名)，并且要关闭防火墙。 3. jstack找出占用cpu最高的堆栈信息 使用命令top -p &lt;pid&gt; ，显示你的java进程的内存情况，pid是你的java进程号，比如4977 按H，获取每个线程的内存情况 找到内存和cpu占用最高的线程tid，比如4977 转为十六进制得到 0x1371 ,此为线程id的十六进制表示 执行 jstack 4977|grep -A 10 1371，得到线程堆栈信息中1371这个线程所在行的后面10行 查看对应的堆栈信息找出可能存在问题的代码","link":"/2020/07/10/JVM-%E8%99%9A%E6%8B%9F%E6%9C%BA%E8%B0%83%E4%BC%98%E5%B7%A5%E5%85%B7/"},{"title":"JVM- 内存回收机制","text":"JVM 内存结构 2. JVM内存分配与回收2.1 Minor Gc 和 Full GC 有什么不同呢？垃圾回收方式有两种 MinorGC 和 Full GC 新生代GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC非常频繁，回收速度一般也比较快。通常 Minor GC 也叫做 Young GC. 老年代GC（Major GC/Full GC）:指发生在老年代的 GC，出现了Major GC经常会伴随至少一次的 Minor GC（并非绝对），Major GC的速度一般会比Minor GC的慢10倍以上。老年代 GC 也可称为 Old GC. 2.2 对象优先在Eden区分配大多数情况下，对象在新生代中 Eden 区分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次Minor GC 实验测试public static void main(String[] args) throws InterruptedException { byte[] allocation1, allocation2; allocation1 = new byte[20231 * 1024]; //allocation2 = new byte[10000*1024]; try { System.in.read(); } catch (IOException e) { e.printStackTrace(); } } Heap PSYoungGen total 38400K, used 26468K [0x0000000795580000, 0x0000000798000000, 0x00000007c0000000) eden space 33280K, 79% used [0x0000000795580000,0x0000000796f59348,0x0000000797600000) from space 5120K, 0% used [0x0000000797b00000,0x0000000797b00000,0x0000000798000000) to space 5120K, 0% used [0x0000000797600000,0x0000000797600000,0x0000000797b00000) ParOldGen total 87552K, used 0K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000) object space 87552K, 0% used [0x0000000740000000,0x0000000740000000,0x0000000745580000) Metaspace used 3600K, capacity 4536K, committed 4864K, reserved 1056768K class space used 401K, capacity 428K, committed 512K, reserved 1048576K 从上图我们可以看出eden区内存已经被分配79%（即使程序什么也不做，新生代也会使用至少2000多k内存）。假如我们再为allocation2分配内存会出现什么情况呢？ Heap PSYoungGen total 38400K, used 3190K [0x0000000795580000, 0x000000079a080000, 0x00000007c0000000) eden space 33280K, 8% used [0x0000000795580000,0x0000000795819a30,0x0000000797600000) from space 5120K, 10% used [0x0000000797600000,0x0000000797684010,0x0000000797b00000) to space 5120K, 0% used [0x0000000799b80000,0x0000000799b80000,0x000000079a080000) ParOldGen total 87552K, used 30239K [0x0000000740000000, 0x0000000745580000, 0x0000000795580000) object space 87552K, 34% used [0x0000000740000000,0x0000000741d87c20,0x0000000745580000) Metaspace used 3600K, capacity 4536K, committed 4864K, reserved 1056768K class space used 401K, capacity 428K, committed 512K, reserved 1048576K 简单解释一下为什么会出现这种情况： 因为给 allocation2 分配内存的时候eden区内存几乎已经被分配完了，我们刚刚讲了当Eden区没有足够空间进行分配时，虚拟机将发起一次 Minor GC.GC 期间虚拟机又发现 allocation1 无法存入 Survior 空间，所以只好通过 分配担保机制 把新生代的对象提前转移到老年代中去，老年代上的空间足够存放 allocation1，所以不会出现Full GC。执行 Minor GC后，后面分配的对象如果能够存在eden区的话，还是会在eden区分配内存。 发生 Minor GC的次数 也可以通过下面的统计信息看到，YGC = 1。 2.3 对象进入老年的方式2.3.1 大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。为什么要这样呢？为了避免为大对象分配内存时由于分配担保机制带来的复制而降低效率。 2.3.2 长期存活的对象将进入老年代既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别那些对象应放在新生代，那些对象应放在老年代中。为了做到这一点，虚拟机给每个对象一个对象年龄（Age）计数器。如果对象在 Eden 出生并经过第一次 Minor GC 后仍然能够存活，并且能被 Survivor 容纳的话，将被移动到 Survivor 空间中，并将对象年龄设为1.对象在 Survivor 中每熬过一次 MinorGC,年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。 2.4 如何判断对象可以被回收堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断那些对象已经死亡（即不能再被任何途径使用的对象）。 2.4.1 引用计数法给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。这个方法实现简单，效率高，==但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。 所谓对象之间的相互引用问题，== 如下面代码所示：除了对象objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为0，于是引用计数算法无法通知 GC 回收器回收他们。 public class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 2.4.2 可达性分析算法这个算法的基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的。 GC Roots根节点：类加载器、Thread、虚拟机栈的本地变量表、static成员、常量引用、本地方法栈的变量等等 2.4.3 finalize()方法最终判定对象是否存活即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑”阶段，要真正宣告一个对象死亡，还可以再抢救以下，至少要经历再次标记过程。标记的前提是对象在进行可达性分析后发现没有与 GCRoots 相连接的引用链。 两次标记 。 第一次标记并进行一次筛选。筛选的条件是此对象是否有必要执行finalize()方法。当对象没有覆盖finalize方法，或者finzlize方法已经被虚拟机调用过，虚拟机将这两种情况都视为“没有必要执行”，对象被回收。 第二次标记如果这个对象被判定为有必要执行finalize（）方法，那么这个对象将会被放置在一个名为：F-Queue的队列之中，并在稍后由一条虚拟机自动建立的、低优先级的Finalizer线程去执行。这里所谓的“执行”是指虚拟机会触发这个方法，但并不承诺会等待它运行结束。这样做的原因是，如果一个对象finalize（）方法中执行缓慢，或者发生死循环（更极端的情况），将很可能会导致F-Queue队列中的其他对象永久处于等待状态，甚至导致整个内存回收系统崩溃。 finalize（）方法是对象脱逃死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模标记，如果对象要在finalize（）中成功拯救自己(只要重新与引用链上的任何的一个对象建立关联即可)，譬如把自己赋值给某个类变量或对象的成员变量，那在第二次标记时它将移除出“即将回收”的集合。 如果对象这时候还没逃脱，那基本上它就真的被回收了。 测试代码： public class OOMTest { // JVM设置 // -Xms10M -Xmx10M -XX:+PrintGCDetails -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=D:\\jvm.dump public static void main(String[] args) { List list = new ArrayList(); int i = 0; int j = 0; while (true) { list.add(new User(i++, UUID.randomUUID().toString())); new User(j--, UUID.randomUUID().toString()); } } } 2.4.4 如何判断一个常量是废弃常量运行时常量池主要回收的是废弃的常量。那么，我们如何判断一个常量是废弃常量呢？假如在常量池中存在字符串 “abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量 “abc” 就是废弃常量，如果这时发生内存回收的话而且有必要的话，”abc” 就会被系统清理出常量池。 2.4.5 如何判断一个类是无用的类方法区主要回收的是无用的类，那么如何判断一个类是无用的类的呢？判定一个常量是否是“废弃常量”比较简单，而要判定一个类是否是“无用的类”的条件则相对苛刻许多。类需要同时满足下面3个条件才能算是 “无用的类” ： 该类所有的实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。 加载该类的 ClassLoader 已经被回收。 该类对应的 java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 虚拟机可以对满足上述3个条件的无用类进行回收，这里说的仅仅是“可以”，而并不是和对象一样不使用了就会必然被回收。 3. 垃圾收集算法 3.1 标记-清除算法算法分为“标记”和“清除”阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象。它是最基础的收集算法，效率也很高，但是会带来两个明显的问题： 效率问题 空间问题（标记清除后会产生大量不连续的碎片） 3.2 复制算法为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 3.3 标记-整理算法根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一段移动，然后直接清理掉端边界以外的内存。 3.4 分代收集算法当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。比如在新生代中，每次收集都会有大量对象死去，所以可以选择复制算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 4. 垃圾收集器 如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。 先对 并行和并发概念进行了解： 并行（Parallel） ：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态。适合科学计算、后台处理等弱交互场景。 并发（Concurrent）：指用户线程与垃圾收集线程同时执行（但不一定是并行，可能会交替执行），用户程序在继续运行，而垃圾收集器运行在另一个CPU上。适合Web应用。 虽然我们对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的HotSpot虚拟机就不会实现那么多不同的垃圾收集器了。 4.1 Serial收集器Serial（串行）收集器收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 ++新生代采用复制算法，老年代采用标记-整理算法。++ 虚拟机的设计者们当然知道 Stop The World 带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率。 4.2 ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样。==新生代采用复制算法，老年代采用标记-整理算法。== 它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 4.3 Parallel Scavenge收集器Parallel Scavenge 收集器类似于 ParNew 收集器，是 Server 模式（内存大于 2G，2 个 cpu）下的默认收集器，那么它有什么特别之处呢？ Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。==新生代采用复制算法，老年代采用标记-整理算法。== 4.4 Serial Old收集器Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。 4.5 Parallel Old收集器Parallel Scavenge收集器的老年代版本。使用多线程和“标记-整理”算法。在注重吞吐量以及CPU资源的场合，都可以优先考虑 Parallel Scavenge收集器和Parallel Old收集器。 4.6 CMS收集器(-XX:+UseConcMarkSweepGC(主要是old区使用))CMS（Concurrent MarkSweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它而非常符合在注重用户体验的应用上使用，它是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。 整个过程分为四个步骤： 初始标记：暂停所有的其他线程(STW)，并记录下直接与root相连的对象，速度很快 ； 并发标记：同时开启GC和用户线程，用一个闭包结构去记录可达对象。但在这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。因为用户线程可能会不断的更新引用域，所以GC线程无法保证可达性分析的实时性。所以这个算法里会跟踪记录这些发生引用更新的地方。 重新标记：重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短 并发清除：开启用户线程，同时GC线程开始对未标记的区域做清扫。 从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。 但是它有下面三个明显的缺点： 对CPU资源敏感（会和服务抢资源）； 无法处理浮动垃圾(在java业务程序线程与垃圾收集线程并发执行过程中又产生的垃圾，这种浮动垃圾只能等到下一次gc再清理了)； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 CMS的相关参数 -XX:+UseConcMarkSweepGC 启用cms -XX:ConcGCThreads:并发的GC线程数（并非STW时间，而是和服务一起执行的线程数） -XX:+UseCMSCompactAtFullCollection:FullGC之后做压缩（减少碎片） -XX:CMSFullGCsBeforeCompaction:多少次FullGC之后压缩一次（因压缩非常的消耗时间，所以不能每次FullGC都做） -XX:CMSInitiatingOccupancyFraction:触发FulGC条件（默认是92） -XX:+UseCMSInitiatingOccupancyOnly:是否动态调节 -XX:+CMSScavengeBeforeRemark:FullGC之前先做YGC（一般这个参数是打开的） -XX:+CMSClassUnloadingEnabled:启用回收Perm区（jdk1.7及以前） 4.7 G1收集器(-XX:+UseG1GC)G1 (Garbage-First)是一款面向服务器的垃圾收集器,主要针对配备多颗处理器及大容量内存的机器. 以极高概率满足GC停顿时间要求的同时,还具备高吞吐量性能特征. G1将Java堆划分为多个大小相等的独立区域（Region），虽保留新生代和老年代的概念，但不再是物理隔阂了，它们都是（可以不连续）Region的集合。 分配大对象（直接进Humongous区，专门存放短期巨型对象，不用直接进老年代，避免Full GC的大量开销）不会因为无法找到连续空间而提前触发下一次GC。 被视为JDK1.7中HotSpot虚拟机的一个重要进化特征。它具备以下特点：并行与并发：G1能充分利用CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿时间。部分其他收集器原本需要停顿Java线程来执行GC动作，G1收集器仍然可以通过并发的方式让java程序继续执行。 分代收集：虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但是还是保留了分代的概念。 空间整合：与CMS的“标记–清理”算法不同，G1从整体来看是基于“标记整理”算法实现的收集器；从局部上来看是基于“复制”算法实现的。 可预测的停顿：这是G1相对于CMS的另一个大优势，降低停顿时间是G1 和 CMS 共同的关注点，但G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内完成垃圾收集。 G1收集器的运作大致分为以下几个步骤： 初始标记（initial mark，STW）：在此阶段，G1 GC对根进行标记。该阶段与常规的 (STW) 年轻代垃圾回收密切相关。 并发标记（Concurrent Marking）：G1 GC在整个堆中查找可访问的（存活的）对象。 最终标记（Remark，STW）：该阶段是 STW 回收，帮助完成标记周期。 筛选回收（Cleanup，STW）：筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。 G1收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的Region(这也就是它的名字Garbage-First的由来)。这种使用Region划分内存空间以及有优先级的区域回收方式，保证了GF收集器在有限时间内可以尽可能高的收集效率。 5. 如何选择垃圾收集器 优先调整堆的大小让服务器自己来选择 如果内存小于100M，使用串行收集器 如果是单核，并且没有停顿时间的要求，串行或JVM自己选择 如果允许停顿时间超过1秒，选择并行或者JVM自己选 如果响应时间最重要，并且不能超过1秒，使用并发收集器 下图有连线的可以搭配使用，官方推荐使用G1，因为性能高","link":"/2020/07/08/JVM-%E5%86%85%E5%AD%98%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6/"},{"title":"Java-分布式锁这样实现","text":"为了防止分布式系统中的多个进程之间相互干扰，我们需要一种分布式协调技术来对这些进程进行调度。而这个分布式协调技术的核心就是来实现这个分布式锁。 分布式锁实现，目前有很多种方式比如以下方式。 1. 常用实现方式 基于数据库实现分布式锁 性能较差，容易出现单点故障。 锁没有失效时间，容易死锁。 基于缓存实现分布式锁 比如通过Redis 实现，但相对复杂。 存在死锁（或短时间死锁）的可能。 基于 Zookeeper 实现分布式锁 实现相对简单 可靠性高 性能好 从上面来看，Zookeeper 是一个理想状态下的分布式锁实现，那zk为什么理想呢？这要从 zk 的特性来说。 首先zk 是一个分布式的，开放源码的分布式应用程序协调服务。 基于节点的数据结构，和unix文件路径相似的节点，可以往这个节点存储或获取数据。 通过客户端可对 znode 进行增删改查的操作，还可以注册watcher监控znode变化。 节点类型分为持久节点（包含：顺序和非顺序）、临时节点（顺序和非顺序） 同父的子节点不可重复。 3. Zookeeper 实现分布式锁一利用zk 同父节点下子节点不可重名的特性。 实现流程如下： 针对以上流程代码实现如下，包含可重入锁的特性。 public class ZKDistributeLock implements Lock{ private String lockPath; private ZkClient client; // 锁重入计数 private ThreadLocal&lt;Integer> reentrantCount = new ThreadLocal&lt;>(); public ZKDistributeLock(String lockPath) { super(); this.lockPath = lockPath; client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); } @Override public boolean tryLock() { // 不会阻塞 if (this.reentrantCount.get() != null) { int count = this.reentrantCount.get(); if (count > 0) { this.reentrantCount.set(++count); return true; } } // 创建节点 try { client.createEphemeral(lockPath); this.reentrantCount.set(1); } catch (ZkNodeExistsException e) { return false; } return true; } @Override public void unlock() { // 重入的释放锁处理 if (this.reentrantCount.get() != null) { int count = this.reentrantCount.get(); if (count > 1) { this.reentrantCount.set(--count); return; } else { this.reentrantCount.set(null); } } client.delete(lockPath); } @Override public void lock() { // 如果获取不到锁，阻塞等待 if (!tryLock()) { // 没获得锁，阻塞自己 waitForLock(); // 再次尝试 lock(); } } private void waitForLock() { CountDownLatch cdl = new CountDownLatch(1); IZkDataListener listener = new IZkDataListener() { @Override public void handleDataDeleted(String dataPath) throws Exception { System.out.println(\"----收到节点被删除了-------------\"); cdl.countDown(); } @Override public void handleDataChange(String dataPath, Object data) throws Exception { } }; client.subscribeDataChanges(lockPath, listener); // 阻塞自己 if (this.client.exists(lockPath)) { try { cdl.await(); } catch (InterruptedException e) { e.printStackTrace(); } } // 取消注册 client.unsubscribeDataChanges(lockPath, listener); } @Override public void lockInterruptibly() throws InterruptedException { // TODO Auto-generated method stub } @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException { // TODO Auto-generated method stub return false; } @Override public Condition newCondition() { // TODO Auto-generated method stub return null; } public static void main(String[] args) { // 并发数 int currency = 50; // 循环屏障 CyclicBarrier cb = new CyclicBarrier(currency); // 多线程模拟高并发 for (int i = 0; i &lt; currency; i++) { new Thread(new Runnable() { public void run() { System.out.println(Thread.currentThread().getName() + \"---------我准备好---------------\"); // 等待一起出发 try { cb.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } ZKDistributeLock lock = new ZKDistributeLock(\"/distLock11\"); try { lock.lock(); System.out.println(Thread.currentThread().getName() + \" 获得锁！\"); } finally { lock.unlock(); } } }).start(); } } } 是否可以利用持久节点进行创建？ 利用持久节点，存在死锁的风险。因为如果持有锁的进程挂掉了，没有来的及删除节点，则出现死锁。所以需要用临时节点，因为临时节点有自删除的特性。 但是采用临时节点就没问题了吗？ 有问题，临时节点会出现惊群效应，在高并发场景下，如果有多个线程 注册了 watcher 机制，那么在这种实现方式下，就会有多个线程同时被唤醒，进行锁争抢，会造成巨大的服务器性能损耗，甚至可能出现宕机的风险，这就是所谓的惊群效应。 针对惊群效应，下面给出个改进版。 4. zk实现分布式锁方式二利用zk 临时顺序节点的特性。 利用临时顺序节点来实现分布式锁，包含可重入特性的完整代码。 获取锁：取排队号（创建自己的临时顺序节点），然后判断自己是否是最小号，如是，则获得锁；不是，则注册前一节点的watcher,阻塞等待 释放锁：删除自己创建的临时顺序节点 通知：watcher机制通知阻塞等待的线程。 public class ZKDistributeImproveLock implements Lock { private String lockPath; private ZkClient client; private ThreadLocal&lt;String> currentPath = new ThreadLocal&lt;>(); private ThreadLocal&lt;String> beforePath = new ThreadLocal&lt;>(); // 锁重入计数 private ThreadLocal&lt;Integer> reentrantCount = new ThreadLocal&lt;>(); public ZKDistributeImproveLock(String lockPath) { super(); this.lockPath = lockPath; client = new ZkClient(\"localhost:2181\"); client.setZkSerializer(new MyZkSerializer()); if (!this.client.exists(lockPath)) { try { this.client.createPersistent(lockPath); } catch (ZkNodeExistsException e) { } } } @Override public boolean tryLock() { if (this.reentrantCount.get() != null) { int count = this.reentrantCount.get(); if (count > 0) { this.reentrantCount.set(++count); return true; } } if (this.currentPath.get() == null) { currentPath.set(this.client.createEphemeralSequential(lockPath + \"/\", \"aaa\")); } // 获得所有的子 List&lt;String> children = this.client.getChildren(lockPath); // 排序list Collections.sort(children); // 判断当前节点是否是最小的 if (currentPath.get().equals(lockPath + \"/\" + children.get(0))) { this.reentrantCount.set(1); return true; } else { // 取到前一个 // 得到字节的索引号 int curIndex = children.indexOf(currentPath.get().substring(lockPath.length() + 1)); beforePath.set(lockPath + \"/\" + children.get(curIndex - 1)); } return false; } @Override public void lock() { if (!tryLock()) { // 阻塞等待 waitForLock(); // 再次尝试加锁 lock(); } } private void waitForLock() { CountDownLatch cdl = new CountDownLatch(1); // 注册watcher IZkDataListener listener = new IZkDataListener() { @Override public void handleDataDeleted(String dataPath) throws Exception { System.out.println(\"-----监听到节点被删除\"); cdl.countDown(); } @Override public void handleDataChange(String dataPath, Object data) throws Exception { } }; client.subscribeDataChanges(this.beforePath.get(), listener); // 怎么让自己阻塞 if (this.client.exists(this.beforePath.get())) { try { cdl.await(); } catch (InterruptedException e) { e.printStackTrace(); } } // 醒来后，取消watcher client.unsubscribeDataChanges(this.beforePath.get(), listener); } @Override public void unlock() { // 重入的释放锁处理 if (this.reentrantCount.get() != null) { int count = this.reentrantCount.get(); if (count > 1) { this.reentrantCount.set(--count); return; } else { this.reentrantCount.set(null); } } // 删除节点 this.client.delete(this.currentPath.get()); } @Override public void lockInterruptibly() throws InterruptedException { // TODO Auto-generated method stub } @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException { // TODO Auto-generated method stub return false; } @Override public Condition newCondition() { // TODO Auto-generated method stub return null; } public static void main(String[] args) { // 并发数 int currency = 50; // 循环屏障 CyclicBarrier cb = new CyclicBarrier(currency); // 多线程模拟高并发 for (int i = 0; i &lt; currency; i++) { new Thread(new Runnable() { public void run() { System.out.println(Thread.currentThread().getName() + \"---------我准备好---------------\"); // 等待一起出发 try { cb.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } ZKDistributeImproveLock lock = new ZKDistributeImproveLock(\"/distLock\"); try { lock.lock(); System.out.println(Thread.currentThread().getName() + \" 获得锁！\"); } finally { lock.unlock(); } } }).start(); } } }","link":"/2020/08/13/Java-%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E8%BF%99%E6%A0%B7%E5%AE%9E%E7%8E%B0/"},{"title":"Java-枚举六问","text":"一问：Java 枚举是如何保证线程安全的？ 答：因为 Java 类加载与初始化是 JVM 保证线程安全，而 Java enum 枚举在编译器编译后的字节码实质是一个 final 类，每个枚举类型是这个 final 类中的一个静态常量属性，其属性初始化是在该 final 类的 static 块中进行，而 static 的常量属性和代码块都是在类加载时初始化完成的，所以自然就是 JVM 保证了并发安全。（不清楚 enum 编译后为啥是静态常量的可以查看历史推送了解更多） 二问：不使用 synchronized 和 lock如何创建一个线程安全的单例？ 答：这是一个很 open 的题目，我们平时提到单例并发都是用锁机制，实际抛开锁机制也有几种实现方式可以保证创建单例的并发安全，而且各具特色。 // 通过枚举实现单例模式 public enum Singleton { INSTANCE; public void func() { } } // 通过饿汉模式实现单例 public class Singleton { private static Singleton instance = new Singleto(); private Singleton() { } public static Singleton getInstance() { return instance; } } // 通过静态内部类模式实现单例 public class Singleton { private static class SingletonHolder { private static final Singleton INSTANCE = new Singleton(); } private Singleton() { } public static final Singleton getInstance() { return SingletonHolder.INSTANCE; } } // 通过 CAS（AtomicReference）实现单例模式 public class Singleton { private static final AtomicReference&lt;Singleton> INSTANCE = new AtomicReference&lt;Singleton>(); private Singleton() { } public static Singleton getInstance() { for (;;) { Singleton singleton = INSTANCE.get(); if (null != singleton) { return singleton; } singleton = new Singleton(); if (INSTANCE.compareAndSet(null, singleton)) { return singleton; } } } } 可以看到，上面四种方式都可以不使用 synchronized 或者 lock 来保证了单例创建的并发安全。前面三种都是借助了 JVM 的 ClassLoader 类加载初始化保证并发安全的机制（至于 JVM 底层其实也是使用了 synchronized 或者 lock 的机制），而对于最后一种通过 CAS 机制保证了并发安全（至于什么是 CAS 我们后面并发相关每日一题会再详细推送讨论的，这里先记住 CAS 就是一种非阻塞乐观锁机制，是一种基于忙等待的算法，依赖底层硬件实现，相对于锁其没有线程切换和阻塞的额外消耗，但是如果忙等待一直执行不成功的死循环会对 CPU 造成较大的开销），最后一种才是真正的无锁实现。 四问：为什么有人说在一些场景下通过枚举实现的单例是最好的方式，原因是什么？ 答：其实这个题目算是一箭双雕，既考察了 Java 枚举的实质特性又考察了单例模式的一些弊端问题。除过枚举实现的单例模式以外的其他实现方式都有一个比较大的问题是一旦实现了 Serializable 接口后就不再是单例了，因为每次调用 readObject() 方法返回的都是一个新创建出来的对象（当然可以通过使用 readResolve() 方法来避免，但是终归麻烦），而 Java 规范中保证了每一个枚举类型及其定义的枚举变量在 JVM 中都是唯一的，在枚举类型的序列化和反序列化上 Java 做了特殊处理，序列化时 Java 仅仅是将枚举对象的 name 属性输出到结果中，反序列化时则是通过 java.lang.Enum 的 valueOf 方法来根据名字查找枚举对象，同时禁用了 writeObject、readObject、readObjectNoData、writeReplace 和 readResolve 等方法。这个问题也暴露出另一个新问题，Java 枚举序列化有哪些坑？ 五问：Java 枚举序列化有哪些坑？ 答：如果我们枚举被序列化本地持久化了，那我们就不能删除原来枚举类型中定义的任何枚举对象，否则程序在运行过程中反序列化时 JVM 就会找不到与某个名字对应的枚举对象了，所以我们要尽量避免多枚举对象序列化的使用（当然了，枚举实现的单例枚举对象一般都不会增删改，所以不存在问题）。 六问：Java 迭代器和枚举器的区别是什么？ 答：主要区别如下。Enumeration 枚举器接口是 JDK 1.0 提供的，适用于传统类，而 Iterator 迭代器接口是 JDK 1.2 提供的，适用于 Collections。Enumeration 只有两个方法接口，我们只能读取集合的数据而不能对数据进行修改，而 Iterator 有三个方法接口，除了能读取集合的数据外也能对数据进行删除操作。Enumeration 不支持 fail-fast 机制，而 Iterator 支持 fail-fast 机制（一种错误检测机制，当多线程对集合进行结构上的改变的操作时就有可能会产生 fail-fast 机制，譬如 ConcurrentModificationException 异常）。总归现在尽量使用 Iterator 迭代器而不是 Enumeration 枚举器。","link":"/2019/01/06/Java%E6%9E%9A%E4%B8%BE%E5%85%AD%E9%97%AE/"},{"title":"Java 反射进阶","text":"参考博客 一、获取不存在的对象？1. 获取不到 class 对象 通过一个对象的 getClass() 方法。 通过 .class 关键字。 通过 Class.forName()。 抛出 ClassNotFoundException 异常。 2. 获取不到 Field获取不到 Field 的情况分3种： 确实不存在这个 Field 抛出 NoSuchFieldException 由于修饰符导致的权限问题。 抛出 SecurityException Field 存在，但获取不到 抛出 NoSuchFieldException 针对 Field 存在，但获取不到情况：由于 getField 和 getDeclaredField 在父类获取的限制。可以先获取 当前类的 父类，即 superClass ，然后 通过 getField或者getDeclaredField的方式获取 Class superClass = clzBase.getSuperclass(); 3. 获取不到 Method 获取本身就不存在的 Method 抛出 NoSuchMethodException 参数类型不匹配而找不到 抛出NoSuchMethodException，传递参数的方式： Method methodtest = class1.getDeclaredMethod(\"test\",int.class,float.class); Method methodtest = class1.getDeclaredMethod(\"test\",new Class[]{int.class,float.class}); 4. 获取不到 Constructor与 method 的类似 5. getInterfaces() 的作用大家可能都会觉得 getInterfaces() 的作用是获取一个类中定义的接口，但是其实不是的，getInterfaces() 获取的是一个类所有实现的接口。 二、反射中的权限问题1. 操纵非 public 修饰的 Field抛出 IllegalAccessException 错误 field.setAccessible(true); 我这里以 private 为例，其实 protected 和 default 也是一样的，但是它们不同于 private 的地方在于，它们在本 package 范围内是可见的，有兴趣的同学可以测试一下，测试代码在一个 package，而测试的类在另外一个 package。 2. 操纵一个 final 类型的 Field抛出 IllegalAccessException 错误 setAccessible(true) 虽然被 public 修饰，但是它同样被 final 修饰，这在正常的开发流程说明这个属性不能够再被改变。 如果要解决这个问题，同样可以使用 setAccessible(true) 方法 3. 操纵非 public 修饰的 Methodmethod.setAccessible(true); 通过 setAccessible(true) 同样可以解决这个问题。 4. 操纵非 public 修饰的 Constructor同前面两种，同样是通过 setAccessible(true) 来搞定。 所以，在反射中如果要操作被 private 修饰的对象，那么就必须调用它的 setAccessible(true)。 三、setAccessible() 的秘密我们已经知道 Field、Method 和 Constructor 都有 setAccessible() 这个方法，至于是什么呢？这是因为它们有共同的祖先 AccessObject。 public class AccessibleObject implements AnnotatedElement { public void setAccessible(boolean flag) throws SecurityException { SecurityManager sm = System.getSecurityManager(); if (sm != null) sm.checkPermission(ACCESS_PERMISSION); setAccessible0(this, flag); } /* Check that you aren't exposing java.lang.Class.&lt;init> or sensitive fields in java.lang.Class. */ private static void setAccessible0(AccessibleObject obj, boolean flag) throws SecurityException { if (obj instanceof Constructor &amp;&amp; flag == true) { Constructor&lt;?> c = (Constructor&lt;?>)obj; if (c.getDeclaringClass() == Class.class) { throw new SecurityException(\"Cannot make a java.lang.Class\" + \" constructor accessible\"); } } obj.override = flag; } /** * Get the value of the {@code accessible} flag for this object. * * @return the value of the object's {@code accessible} flag */ public boolean isAccessible() { return override; } } 可以看到，主要是设置内部一个 override 变量。 那么，我们以 Method 的 invoke 方法为例。 public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException { if (!override) { if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) { Class&lt;?> caller = Reflection.getCallerClass(); checkAccess(caller, clazz, obj, modifiers); } } MethodAccessor ma = methodAccessor; // read volatile if (ma == null) { ma = acquireMethodAccessor(); } return ma.invoke(obj, args); } 如果一个 Method 的 overide 为 false 的话，它就会根据 Modifiers 判断是否具有访问权限。 public static boolean quickCheckMemberAccess(Class&lt;?> memberClass,int modifiers) { return Modifier.isPublic(getClassAccessFlags(memberClass) &amp; modifiers); } 这个方法主要是简单地判断 modifiers 是不是 public，如果不是的话就返回 false。所以 protected、private、default 修饰符都会返回 false,只有 public 都会返回 true。 而不是 public 修饰的话会执行下面的代码 void checkAccess(Class&lt;?> caller, Class&lt;?> clazz, Object obj, int modifiers)throws IllegalAccessException { if (caller == clazz) { // quick check return; // ACCESS IS OK } Object cache = securityCheckCache; // read volatile Class&lt;?> targetClass = clazz; if (obj != null &amp;&amp; Modifier.isProtected(modifiers) &amp;&amp; ((targetClass = obj.getClass()) != clazz)) { // Must match a 2-list of { caller, targetClass }. if (cache instanceof Class[]) { Class&lt;?>[] cache2 = (Class&lt;?>[]) cache; if (cache2[1] == targetClass &amp;&amp; cache2[0] == caller) { return; // ACCESS IS OK } // (Test cache[1] first since range check for [1] // subsumes range check for [0].) } } else if (cache == caller) { // Non-protected case (or obj.class == this.clazz). return; // ACCESS IS OK } // If no return, fall through to the slow path. slowCheckMemberAccess(caller, clazz, obj, modifiers, targetClass); } // Keep all this slow stuff out of line: void slowCheckMemberAccess(Class&lt;?> caller, Class&lt;?> clazz, Object obj, int modifiers, Class&lt;?> targetClass) throws IllegalAccessException { Reflection.ensureMemberAccess(caller, clazz, obj, modifiers); // Success: Update the cache. Object cache = ((targetClass == clazz) ? caller : new Class&lt;?>[] { caller, targetClass }); // Note: The two cache elements are not volatile, // but they are effectively final. The Java memory model // guarantees that the initializing stores for the cache // elements will occur before the volatile write. securityCheckCache = cache; // write volatile } 最终通过 Reflection 这个类的静态方法 ensureMemberAccess() 确认。 public static void ensureMemberAccess(Class&lt;?> currentClass, Class&lt;?> memberClass, Object target,int modifiers)throws IllegalAccessException { if (currentClass == null || memberClass == null) { throw new InternalError(); } if (!verifyMemberAccess(currentClass, memberClass, target, modifiers)) { throw new IllegalAccessException(\"Class \" + currentClass.getName() + \" can not access a member of class \" + memberClass.getName() + \" with modifiers \\\"\" + Modifier.toString(modifiers) + \"\\\"\"); } } 如果没有访问权限，程序将会在此抛出一个 IllegalAccessException 的异常。 所以，如果通过反射方式去操作一个 Field、Method 或者是 Constructor，最好先调用它的 setAccessible(true) 以防止程序运行异常。 四、Class.newInstance() 和 Constructor.newInstance() 的区别Class.newInstance() 的使用有严格的限制，那就是一个 Class 对象中，必须存在一个无参数的 Constructor，并且这个 Constructor 必须要有访问的权限。 通过 Constructor.newInstance() 却没有这种限制。Constructor.newInstance() 适应任何类型的 Constructor,无论它们有参数还是无参数，只要通过 setAccessible() 控制好访问权限就可以了。 五、谨慎使用 Method.invoke() 方法public Object invoke(Object obj, Object... args) throws IllegalAccessException, IllegalArgumentException, InvocationTargetException 第一个 Object 参数代表的是对应的 Class 对象实例，这在上面一节已经见识到了。而后面的参数就是可变形参了，它接受多个参数。我们考虑一种特殊情况。 public class TestT&lt;T>{ public void test(T t){} } 这是一个泛型类，T 表示接受任意类型的参数。 Method tMethod = clzT.getDeclaredMethod(\"test\",Integer.class); tMethod.setAccessible(true); tMethod.invoke(new TestT&lt;Integer>(),1); 报错 ：NoSuchMethodException，提示找不到这个方法。原因是类型擦除。当一个方法有泛型参数时，编译器会自动向上转型，T 向上转型是 Object。所以实际上是 void test(Object t); 上面的代码试图去找 test(Integer t) 这个方法，自然是找不到。 Method tMethod = clzT.getDeclaredMethod(\"test\",Object.class); tMethod.setAccessible(true); tMethod.invoke(new TestT&lt;Integer>(),1); 在 Java 反射中，一个 Method 执行时遭遇的异常会被包装在一个特定的异常中，这个异常就是 InvocationTargetException。 六、总结 异常名称 原因 ClassNotFoundException 1. class.forName() 传入的包名有误 2.Class本身不存在 NoSuchFieldException 1. Field 名称不正确。2. getDeclaredField 和 getField()方法使用不当。 NoSuchMethodException 1. 方法本身不存在。2. 传入的参数的类型不匹配。3. 传入的参数 个数不匹配。 IllegalAccessException 1. 访问非 public修饰的对象如Field、Method、Consturctor。2. 操作 final 修饰的 Field. IllegalArgumentException 1. Method.invoke 中参数匹配。2. Field 操作时设置的值不匹配。3. Constructor.newInstance() 传入的参数不匹配。 InvocationTargetException 1. Method 运行时产生异常.2.Constructor.newInstance() 作用时产生异常 InstantiationException 1. Class.newInstance() 或者Constructor.newInstance()异常。","link":"/2019/02/15/Java-%E5%8F%8D%E5%B0%84%E8%BF%9B%E9%98%B6/"},{"title":"Java  注解","text":"注解注解是代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。并执行相应的处理。 一 、注解分类 java 内置注解 @Override、 @Deprecated、@SuppressWarnings、@SafeVarargs(jdk 7 新增) 标注注解的元注解 它是用来修饰注解的注解，从而创建新的注解。 @Target, 说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目的。 @Retention, 定义了该Annotation被保留的时间长短，SOURCE、CLASS、RUNTIME（可通过反射获取内部属性）。 @Documented, 标记注解，可以工具文档化。 @Inherited 当前注解是否可以继承。 @Repeatable: JDK 8 新增，允许一个注解在同一声明类型（类、属性或方法）上多次使用。 @Targe 注解取值是一个 ElementType 类型的数组。 ElementType 说明 ElementType.TYPE|修饰类、接口或枚举类型。 ElementType.FIELD|修饰成员变量。 ElementType.METHOD|修饰方法。 ElementType.PARAMETER|修饰参数。 ElementType.CONSTRUCTOR|修饰构造方法。 ElementType.LOCAL_VARIABLE|修饰局部变量。 ElementType.ANNOTATION_TYPE|修饰注解。 ElementType.PACKAGE|修饰包。 ElementType.TYPE_PARAMETER|修饰参数声明。 ElementType.TYOPE_USR|使用类型。 @Retention 3种类型，分别表示不同的保留周期 RetentionPolicy.SOURCE:源码级注解。 注解信息只会保留在.java 源码中，源码在编译后，注解信息会被丢弃，不会保留到.class 中。 RetentionPolicy.CLASS:编译时注解。 注解信息会保留在.java 源码以及.class 中。当运行java 程序时，JVM 会丢弃该注解信息，不会保留到 JVM 中。 RetentionPolicy.RUNTIME:运行时注解。 当运行 java 程序时，JVM 也会保留该注解信息，可以通过反射获取该注解信息。 定义注解1. 基本定义 定义新的注解类型使用 @interface 关键字。 public @interface Cup{ } 程序中使用该注解： @Cup public class AnnotationTest{ } 2. 定义成员变量(注解的属性) 注解 只有成员变量，没有方法。注解的成员变量在注解定义中以“无形参的方法”形式来声明，其“方法名”定义了该成员变量的名字，其返回值定义了该成员变量的类型； public @interface Cup{ String name(); int price(); } 上面两个成员变量以方法的形式来定义。使用该注解时候就应该为该成员变量赋值 pubilc class AnnotationTest(){ @Cup(name=\"马克杯\",price=100) public void drink(){ } } 也可以在定义注解的时候通过 default 来指定默认值： public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 3. 定义运行时注解 使用@Retention 来设定注解的保留策略 ，这三哥策略的生命周期长度为：SOURCE&lt;CLASS&lt;RUNTIME. @Retention(RetentionPolicy.RUNTIME) public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 4. 定义编译时注解 @Retention(RetentionPolicy.CLASS) public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 5. Repeatable 定义可重复注解 Repeatable 自然是可重复的意思。@Repeatable 是 Java 1.8 才加进来的，所以算是一个新的特性。 什么样的注解会多次应用呢？通常是注解的值可以同时取多个。 举个例子，一个人他既是程序员又是产品经理,同时他还是个画家。 @interface Persons { Person[] value(); } @Repeatable(Persons.class) @interface Person{ String role default \"\"; } @Person(role=\"artist\") @Person(role=\"coder\") @Person(role=\"PM\") public class SuperMan{ } 注意上面的代码，@Repeatable 注解了 Person。而 @Repeatable 后面括号中的类相当于一个容器注解。 什么是容器注解呢？就是用来存放其它注解的地方。它本身也是一个注解。 以下代码中就是相关容器注解。 @interface Persons { Person[] value(); } 二、注解的提取注解通过反射获取。首先可以通过 Class 对象的 isAnnotationPresent() 方法判断它是否应用了某个注解 public boolean isAnnotationPresent(Class&lt;? extends Annotation> annotationClass) {} 然后通过 getAnnotation() 方法来获取 Annotation 对象。 public &lt;A extends Annotation> A getAnnotation(Class&lt;A> annotationClass) {} 或者是 getAnnotations() 方法。 public Annotation[] getAnnotations() {} 前一种方法返回指定类型的注解，后一种方法返回注解到这个元素上的所有注解。 在处理的注解的过程中可能会用到以下方法 Field getDeclaredField(String attrbuteName); Method getDeclaredMethod(String methodName); 三、其他注解： 使用场景 分为 三类：编译前、编译时生成代码、运行时。 运行时注解：例如：ButterKnife ，黑科技、低性能 编译时注解：Dagger2：生成中间代码，所以性能高 注解处理器：编译时处理器、运行时处理器。","link":"/2019/08/30/Java-%E6%B3%A8%E8%A7%A3/"},{"title":"Java 反射","text":"参考博客网址 反射入口Object.getClass() Car car = new Car(); Class clazz = car.getClass(); .class Class clazz = Car.class; Class cls1 = int.class; Class cls2 = String.class; Class.forName() 方法 try { Class clz = Class.forName(\"com.aa.test.Car\"); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); } Class 的名字 Class.getName(); Class.getSimpleName(); Class.getCanonicalName(); Class clz = new Outter.Inner[][][]{}.getClass(); System.out.println(\" Inner Class name:\"+clz.getName()); System.out.println(\" Inner Class simple name:\"+clz.getSimpleName()); System.out.println(\" Inner Class canonical name:\"+clz.getCanonicalName()); //run 是匿名类 Runnable run = new Runnable() { @Override public void run() { // TODO Auto-generated method stub } }; System.out.println(\" anonymous Class name:\"+run.getClass().getName()); System.out.println(\" anonymous Class simple name:\"+run.getClass().getSimpleName()); System.out.println(\" anonymous Class canonical name:\"+run.getClass().getCanonicalName()); // local 是局部类 class local{}; System.out.println(\"Local a name:\"+local.class.getName()); System.out.println(\"Local a simplename:\"+local.class.getSimpleName()); System.out.println(\"Local a canonicalname:\"+local.class.getCanonicalName()); 输出 //[[[ 代表三维数组 Inner Class name:[[[Lcom.frank.test.Outter$Inner; Inner Class simple name:Inner[][][] Inner Class canonical name:com.frank.test.Outter.Inner[][][] //匿名类: anonymous Class name:com.frank.test.Test$1 anonymous Class simple name: anonymous Class canonical name:null //局部类 Local a name:com.frank.test.Test$1local Local a simplename:local Local a canonicalname:null Canonical 是官方、标准的意思，那么 getCanonicalName() 自然就是返回一个 Class 对象的官方名字，这个官方名字 canonicalName 是 Java 语言规范制定的，如果 Class 对象没有 canonicalName 的话就返回 null。 getCanonicalName() 是 getName() 和 getSimpleName() 的结合。 getCanonicalName() 返回的也是全限定类名，但是对于内部类，不用 $ 开头，而用 .。 getCanonicalName() 对于数组类型的 Class，同 simplename 一样直接在后面添加 [] 。 getCanonicalName() 不同于 simplename 的地方是，不存在 canonicalName 的时候返回 null 而不是空字符串。 局部类和匿名内部类不存在 canonicalName。 Class 获取修饰符 System.out.println(\"modifiers value:\"+TestModifier.class.getModifiers()); System.out.println(\"modifiers :\"+Modifier.toString(TestModifier.class.getModifiers())); 输出 modifiers value:1025 modifiers :public abstract 大家肯定会有疑问，为什么会返回一个整型数值呢？ 这是因为一个类定义的时候可能会被多个修饰符修饰，为了一并获取，所以 Java 工程师考虑到了位运算，用一个 int 数值来记录所有的修饰符，然后不同的位对应不同的修饰符，这些修饰符对应的位都定义在 Modifier 这个类当中。举例： 待比较 2进制 public（0x00000001） 00000000001 1025 10000000001 进行&amp;运算结果 00000000001 public static boolean isPublic(int mod) { return (mod &amp; PUBLIC) != 0; } 得出结果 00000000001 不等于0 返回true ，类修饰符为 public 获取Class 的成员一个类的成员包括属性（有人翻译为字段或者域）、方法。对应到 Class 中就是 Field、Method、Constructor 获取Field //获取的是 Class 中的属性，不能获取其父类的属性 public Field getDeclaredField(String name) throws NoSuchFieldException, SecurityException; //获取的是 public 属性，并且 getField() 在当前 Class 获取不到时会向祖先类获取 public Field getField(String name) throws NoSuchFieldException, SecurityException //获取所有的属性，但不包括从父类继承下来的属性 public Field[] getDeclaredFields() throws SecurityException {} //获取自身的所有的 public 属性，包括从父类继承下来的。 public Field[] getFields() throws SecurityException { 1.两者的区别就是 getDeclaredField() 获取的是 Class 中的属性,不能获取其父类的属性。 getField() 方法获取的是public属性，并且 getField() 在当前 Class 获取不到时会向祖先类获取。2. getDeclaredFileds() 方法可以获取 private、protected、public 和 default 属性，但是它获取不到从父类继承下来的属性。getFields() 自身的所有的 public 属性，包括从父类继承下来的。 方法 本 Class SupperClass getField public public getDeclaredField public、protected、default、private x getFields public public getDeclaredFields public、protected、default、private x 获取 Method 方法 本 Class SupperClass getMethod public public getDeclaredMethod public、protected、default、private x getMethods public public getDeclaredMethods public、protected、default、private x 类或者接口中的方法对应到 Class 就是 Method。 相应的 API 如下，parameterTypes 是方法对应的参数，获取范围和Field类似。： public Method getDeclaredMethod(String name, Class&lt;?>... parameterTypes) public Method getMethod(String name, Class&lt;?>... parameterTypes) public Method[] getDeclaredMethods() throws SecurityException public Method getMethod(String name, Class&lt;?>... parameterTypes) 获取ContructorJava 反射把构造器从方法中单独拎出来了，用 Constructor 表示。 public Constructor&lt;T> getDeclaredConstructor(Class&lt;?>... parameterTypes) public Constructor&lt;T> getConstructor(Class&lt;?>... parameterTypes) public Constructor&lt;?>[] getDeclaredConstructors() throws SecurityException public Constructor&lt;?>[] getConstructors() throws SecurityException 因为，Constructor 不能从父类继承，所以就没有办法通过 getConstructor() 获取到父类的 Constructor。 Field 的操作类中 定义的属性， 它们的类型要么是 8 种基础类型 int、long、float、double、boolean、char、byte 和 short。要么是引用，所有的引用都是 Object 的后代。Field 类型的获取 public Type getGenericType() {} public Class&lt;?> getType() {} 注意，两者返回的类型不一样，getGenericType() 方法能够获取到泛型类型，比如 hashMap&lt;String,String&gt;,比getType 更详细。Field 修饰符的获取 public int getModifiers() {} 这个与前面 Class 获取修饰符一致。Field 内容的读取与赋值Field 这个类定义了一系列的 get 方法来获取不同类型的值。 public Object get(Object obj); public int getInt(Object obj); public long getLong(Object obj) throws IllegalArgumentException, IllegalAccessException; public float getFloat(Object obj) throws IllegalArgumentException, IllegalAccessException; public short getShort(Object obj) throws IllegalArgumentException, IllegalAccessException; public double getDouble(Object obj) throws IllegalArgumentException, IllegalAccessException; public char getChar(Object obj) throws IllegalArgumentException, IllegalAccessException; public byte getByte(Object obj) throws IllegalArgumentException, IllegalAccessException; public boolean getBoolean(Object obj) throws IllegalArgumentException, IllegalAccessException Field 又定义了一系列的 set 方法用来对其自身进行赋值。 public void set(Object obj, Object value); public void setInt(Object obj,int value); public void setLong(Object obj,long value) throws IllegalArgumentException, IllegalAccessException; public void setFloat(Object obj,float value) throws IllegalArgumentException, IllegalAccessException; public void setShort(Object obj,short value) throws IllegalArgumentException, IllegalAccessException; public void setDouble(Object obj,double value) throws IllegalArgumentException, IllegalAccessException; public void setChar(Object obj,char value) throws IllegalArgumentException, IllegalAccessException; public void setByte(Object obj,byte b) throws IllegalArgumentException, IllegalAccessException; public void setBoolean(Object obj,boolean b) throws IllegalArgumentException, IllegalAccessException 可能有同学会对方法中出现的 Object 参数有疑问，它其实是类的实例引用，这里涉及一个细节。 Class 本身不对成员进行储存，它只提供检索，所以需要用 Field、Method、Constructor 对象来承载这些成员，所以，针对成员的操作时，一般需要为成员指定类的实例引用。如果难于理解的话，可以这样理解，班级这个概念是一个类，一个班级有几十名学生，现在有A、B、C 3 个班级，将所有班级的学生抽出来集合到一个场地来考试，但是学生在试卷上写上自己名字的时候，还要指定自己的班级，这里涉及到的 Object 其实就是类似的作用，表示这个成员是具体属于哪个 Object。这个是为了精确定位。 在执行 set 属性的时，如果操作 private 修饰的成员，需要加上 field.setAccessible(true); Method 操作Method 对应普通类的方法。我们看看一般普通类的方法的构成。 public int add(int a,int b); 方法由下面几个要素构成： 方法名 方法参数 方法返回值 方法的修饰符 方法可能会抛出的异常 Method 获取方法名 class.getDeleclaredMethods(); Method 获取方法参数 //返回的是一个 Parameter 数组 public Parameter[] getParameters() {} // 获取所有的参数类型 public Class&lt;?>[] getParameterTypes() {} // 获取所有的参数类型，包括泛型 public Type[] getGenericParameterTypes() {} 返回的是一个 Parameter 数组，在反射中 Parameter 对象就是用来映射方法中的参数。经常使用的Parameter.java 类中的方法 // 获取参数名字 public String getName() {} // 获取参数类型 public Class&lt;?> getType() {} // 获取参数的修饰符 public int getModifiers() {} Method 获取返回值类型 // 获取返回值类型 public Class&lt;?> getReturnType() {} // 获取返回值类型包括泛型 public Type getGenericReturnType() {} Method 获取修饰符 public int getModifiers() {} Method 获取异常类型 public Class&lt;?>[] getExceptionTypes() {} public Type[] getGenericExceptionTypes() {} Method 方法的执行 这个应该是整个反射机制的核心内容了，很多时候运用反射目的其实就是为了以常规手段执行 Method。 public Object invoke(Object obj, Object... args) {} Method 调用 invoke() 的时候，存在许多细节： invoke() 方法中第一个参数 Object 实质上是 Method 所依附的 Class 对应的类的实例，如果这个方法是一个静态方法，那么 ojb 为 null，后面的可变参数 Object 对应的自然就是参数。 invoke() 返回的对象是 Object，所以实际上执行的时候要进行强制转换。 在对 Method 调用 invoke() 的时候，如果方法本身会抛出异常，那么这个异常就会经过包装，由 Method 统一抛出 InvocationTargetException。而通过 InvocationTargetException.getCause() 可以获取真正的异常。 Constructor 的操作Constructor 同 Method 差不多，但是它特别的地方在于，它能够创建一个对象。 在 Java 反射机制中有两种方法可以用来创建类的对象实例：Class.newInstance() 和 Constructor.newInstance()。官方文档建议开发者使用后面这种方法，下面是原因。 Class.newInstance() 只能调用无参的构造方法，而 Constructor.newInstance() 则可以调用任意的构造方法。 Class.newInstance() 通过构造方法直接抛出异常，而 Constructor.newInstance() 会把抛出来的异常包装到 InvocationTargetException 里面去，这个和 Method 行为一致。 Class.newInstance() 要求构造方法能够被访问，而 Constructor.newInstance() 却能够访问 private 修饰的构造器。 反射中的数组数组本质上是一个 Class，而在 Class 中存在一个方法用来识别它是否为一个数组。在 Class.java 中 方法 isArray() 判断是否是数组由于 数组本质上 还是 Class 所以可以通过 getName(); //获取数组的里面的元素的类型，比如 int[] 数组的 componentType 自然就是 int getComponentType(); *动态创建数组**反射创建数组通过 Array.newInstance() 这个方法。 public static Object newInstance(Class&lt;?> componentType, int... dimensions) throws IllegalArgumentException, NegativeArraySizeException {} 第一个参数 为数组内原始类型，后面的是可变参数，表示的是相应维度的数组长度限制。比如创建一个 二维数组 Array.newInstance(int.class,2,3); Array 的读取与赋值 对Array 整体的赋值和读取public void set(Object obj, Object value) throws IllegalArgumentException, IllegalAccessException; public Object get(Object obj) throws IllegalArgumentException, IllegalAccessException; 2. 对 Array 指定位置进行赋值和读取,经典的几种方式分别为： ```java public static void set(Object array, int index, Object value) throws IllegalArgumentException, ArrayIndexOutOfBoundsException; public static void setBoolean(Object array, int index, boolean z) throws IllegalArgumentException, ArrayIndexOutOfBoundsException; public static Object get(Object array, int index) throws IllegalArgumentException, ArrayIndexOutOfBoundsException; public static short getShort(Object array, int index) throws IllegalArgumentException, ArrayIndexOutOfBoundsException; 反射中的枚举 enum同数组一样本质上还是一个 Class 而已。 枚举的表现形式： public enum State { IDLE, DRIVING, STOPPING, test(); int test1() { return 0; } } 在 java 反射 中，可以把枚举看成一般的Class,但是反射机制提供了3个特别的 API 用于操作枚举。 //判断是否是枚举类型 Class.isEnum() //获取枚举所有的常量 Class.getEnumConstants() //判断一个 Field 是不是枚举常量 java.lang.reflect.Field.isEnumConstant() 枚举的获取与设置因为等同于 Class， 所以 枚举的获取与设置，可以 通过 Field 中的get() 和 set() 方法。 需要注意的是，如果要获取枚举里面的Field、Method、Constructor 可以调用 Class 的通用 API. 总结 Java 中的反射是非常规编码方式。 Java 反射机制的操作入口是获取 Class 文件。 有 Class.forName()、 .class 和 Object.getClass() 3 种。 获取 Class 对象后还不够，需要获取它的 Members，包含 Field、Method、Constructor。 Field 操作主要涉及到类别的获取，及数值的读取与赋值。 Method 算是反射机制最核心的内容，通常的反射都是为了调用某个 Method 的 invoke() 方法。 通过 Class.newInstance() 和 Constructor.newInstance() 都可以创建类的对象实例，但推荐后者。因为它适应于任何构造方法，而前者只会调用可见的无参数的构造方法。 数组和枚举可以被看成普通的 Class 对待。","link":"/2019/02/10/Java-%E5%8F%8D%E5%B0%84/"},{"title":"Redis核心原理详解","text":"一、Redis 基础数据结构Redis 有 5 种基础数据结构，分别为：string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)，下面就详细说说这几种数据结构。 列表（list）我们先来看列表。列表这种数据类型支持存储一组数据。这种数据类型对应两种实现方法，一种是压缩列表（ziplist），另一种是双向循环链表。当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件： 列表中保存的单个数据（有可能是字符串类型的）小于 64 字节； 列表中数据个数少于 512 个。 所谓压缩列表，它并不是基础数据结构，而是 Redis为了压缩内存和支持存储不同类型的数据， 自己设计的一种数据存储结构。它有点儿类似数组，通过一片连续的内存空间，来存储数据。不过，它跟数组不同的一点是，它允许存储的数据大小不同,由于分配的空间不同，所以不能通过地址索引随机访问。如果不能同时满足刚刚讲的两个条件的时候，列表就要通过双向循环链表来实现了。 字典（hash）典类型用来存储一组数据对。每个数据对又包含键值两部分。字典类型也有两种实现方式。一种是我们刚刚讲到的压缩列表，另一种是散列表。同样，只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件： 字典中保存的键和值的大小都要小于 64 字节； 字典中键值对的个数要小于 512 个。 当不能同时满足上面两个条件的时候 Redis 就使用散列表来实现字典类型。Redis 使用MurmurHash2这种运行速度快、随机性好的哈希算法作为哈希函数。对于哈希冲突问题，Redis 使用链表法来解决。除此之外，Redis 还支持散列表的动态扩容、缩容。 集合（set）集合这种数据类型用来存储一组不重复的数据。这种数据类型也有两种实现方法，一种是基于有序数组，另一种是基于散列表。当要存储的数据，同时满足下面这样两个条件的时候，Redis 就采用有序数组，来实现集合这种数据类型。 存储的数据都是整数； 存储的数据元素个数不超过 512个。 当不能同时满足这两个条件的时候，Redis 就使用散列表来存储集合中的数据。 有序集合（sortedset）有序集合这种数据类型，它用来存储一组数据，并且每个数据会附带一个得分。通过得分的大小，进行数据结构组织。当数据量比较小的时候，Redis 会用压缩列表来实现有序集合。 所有数据的大小都要小于 64 字节； 元素个数要小于 128 个。 当两者都不满足时，会采用跳表这样的数据结构来存储。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。 一些高级命令 keys：全量遍历键，用来列出所有满足特定正则字符串规则的key，当redis数据量比较大时，性能比较差，要避免使用 scan：渐进式遍历键，scan 参数提供了三个参数，第一个是 cursor 整数值，第二个是 key 的正则模式，第三个是遍历的 limit hint。第一次遍历时，cursor 值为 0，然后将返回结果中第一个整数值作为下一次遍历的 cursor。一直遍历到返回的 cursor 值为 0 时结束。 Info：查看redis服务运行信息，分为 9 大块，每个块都有非常多的参数，这 9 个块分别是: Server 服务器运行的环境参数 Clients 客户端相关信息 Memory 服务器运行内存统计数据 Persistence 持久化信息 Stats 通用统计数据 Replication 主从复制相关信息 CPU CPU 使用情况 Cluster 集群信息 二、线程模型核心原理Redis的单线程和高性能 Redis 单线程为什么还能这么快？ 因为它所有的数据都在内存中，所有的运算都是内存级别的运算，而且单线程避免了多线程的切换性能损耗问题。正因为 Redis 是单线程，所以要小心使用 Redis 指令，对于那些耗时的指令(比如keys)，一定要谨慎使用，一不小心就可能会导致 Redis 卡顿。 Redis 单线程如何处理那么多的并发客户端连接？ Redis的IO多路复用：redis利用epoll来实现IO多路复用，将连接信息和事件放到队列中，依次放到文件事件分派器，事件分派器将事件分发给事件处理器。 Nginx也是采用IO多路复用原理解决C10K问题 三、持久化RDB快照（snapshot）在默认情况下， Redis 将内存数据库快照保存在名字为 dump.rdb 的二进制文件中。你可以对 Redis 进行设置， 让它在“N秒内数据集至少有 M 个改动”这一条件被满足时， 自动保存一次数据集。比如说， 以下设置会让 Redis 在满足“ 60 秒内有至少有 1000 个键被改动”这一条件时， 自动保存一次数据集： save 60 1000 AOF（append-only file）快照功能并不是非常耐久（durable）： 如果 Redis 因为某些原因而造成故障停机， 那么服务器将丢失最近写入、且仍未保存到快照中的那些数据。从 1.1 版本开始， Redis 增加了一种完全耐久的持久化方式： AOF 持久化，将修改的每一条指令记录进文件你可以通过修改配置文件来打开 AOF 功能： appendonly yes 从现在开始， 每当 Redis 执行一个改变数据集的命令时（比如 SET）， 这个命令就会被追加到 AOF 文件的末尾。这样的话， 当 Redis 重新启时， 程序就可以通过重新执行 AOF 文件中的命令来达到重建数据集的目的。你可以配置 Redis 多久才将数据 fsync 到磁盘一次。 有三个选项： 每次有新命令追加到 AOF 文件时就执行一次 fsync：非常慢，也非常安全。 每秒 fsync 一次：足够快（和使用 RDB 持久化差不多），并且在故障时只会丢失 1 秒钟的数据。 从不 fsync ：将数据交给操作系统来处理。更快，也更不安全的选择。推荐（并且也是默认）的措施为每秒 fsync一次， 这种 fsync 策略可以兼顾速度和安全性。 RDB 和 AOF ，我应该用哪一个？如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失， 那么你可以只使用 RDB 持久化。有很多用户都只使用 AOF 持久化， 但我们并不推荐这种方式： 因为定时生成 RDB 快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比 AOF 恢复的速度要快。 Redis 4.0 混合持久化重启 Redis 时，我们很少使用 rdb 来恢复内存状态，因为会丢失大量数据。我们通常使用 AOF 日志重放，但是重放 AOF 日志性能相对 rdb 来说要慢很多，这样在 Redis 实例很大的情况下，启动需要花费很长的时间。 Redis 4.0 为了解决这个问题，带来了一个新的持久化选项——混合持久化。AOF在重写(aof文件里可能有太多没用指令，所以aof会定期根据内存的最新数据生成aof文件)时将重写这一刻之前的内存rdb快照文件的内容和增量的 AOF修改内存数据的命令日志文件存在一起，都写入新的aof文件，新的文件一开始不叫appendonly.aof，等到重写完新的AOF文件才会进行改名，原子的覆盖原有的AOF文件，完成新旧两个AOF文件的替换；AOF根据配置规则在后台自动重写，也可以人为执行命令bgrewriteaof重写AOF。 于是在 Redis 重启的时候，可以先加载 rdb 的内容，然后再重放增量 AOF 日志就可以完全替代之前的 AOF 全量文件重放，重启效率因此大幅得到提升。 开启混合持久化： aof-use-rdb-preamble yes 混合持久化aof文件结构 四、缓存淘汰策略当 Redis 内存超出物理内存限制时，内存的数据会开始和磁盘产生频繁的交换 (swap)。交换会让 Redis 的性能急剧下降，对于访问量比较频繁的 Redis 来说，这样龟速的存取效率基本上等于不可用。在生产环境中我们是不允许 Redis 出现交换行为的，为了限制最大使用内存，Redis 提供了配置参数 maxmemory 来限制内存超出期望大小。当实际内存超出 maxmemory 时，Redis 提供了几种可选策略 (maxmemory-policy) 来让用户自己决定该如何腾出新的空间以继续提供读写服务。 noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。 五、Redis集群原理分析redis cluster 是 redis 分布式集群解决方案，从 3.0 之后推出解决 redis 分布式方面需求，实现数据分片、故障转移、扩容所容机制等。 Redis Cluster 将所有数据划分为 16384 的 slots(槽位)，每个节点负责其中一部分槽 位。槽位的信息存储于每个节点中。 当 Redis Cluster 的客户端来连接集群时，它也会得到一份集群的槽位配置信息并将 其缓存在客户端本地。这样当客户端要查找某个 key 时，可以直接定位到目标节点。 同时因为槽位的信息可能会存在客户端与服务器不一致的情况，还需要纠正机制来实 现槽位信息的校验调整。 redis集群大小的选择,可以装多少数据? 理论是可以做到16384个集群,每个槽对应一个实例,但是redis官方建议是最大1000个实例。存储足够大了。另外：节点之间会有频繁通信，传递的包括槽位信息，集群太大会有带宽消耗。 槽位定位算法槽位定位算法 也就是常说的 一致性Hash算法Cluster 默认会对 key 值使用 crc16 算法进行 hash 得到一个整数值，然后用这个整 数值对 16384 进行取模来得到具体槽位。 HASH_SLOT = CRC16(key) mod 16384 那么增加了slot槽的计算,是不是比单机性能差? 共16384个槽, slots槽计算方式公开的,为了避免每次都需要服务器计算重定向,优秀的java客户端都实现了本地计算,并且缓存服务器slots分配,有变动时再更新本地内容,从而避免了多次重定向带来的性能损耗 访问倾斜和数据存储倾斜问题怎么处理？ 倾斜导致某些节点量大，压力大 可以从两方面着手 事前预测数据哪些会是热点数据，设计的时候规避。 事后 通过 slot 调整，压力分摊（slot 调整：rebalance 和 reshared） 跳转重定位当客户端向一个错误的节点发出了指令，该节点会发现指令的 key 所在的槽位并不归 自己管理，这时它会向客户端发送一个特殊的跳转指令携带目标操作的节点地址，告 诉客户端去连这个节点去获取数据。客户端收到指令后除了跳转到正确的节点上去操 作，还会同步更新纠正本地的槽位映射表缓存，后续所有 key 将使用新的槽位映射 表。 ask和moved重定向的区别 重定向包括两种情况,若确定slot不属于当前节点, redis会返回moved,若当煎redis节点正在处理slot迁移,则代表此处请求对应的key暂时不在此节定向。 网络抖动真实世界的机房网络往往并不是风平浪静的，它们经常会发生各种各样的小问题。比 如网络抖动就是非常常见的一种现象，突然之间部分连接变得不可访问，然后很快又 恢复正常。 为解决这种问题，Redis Cluster 提供了一种选项cluster node timeout，表示当某 个节点持续 timeout 的时间失联时，才可以认定该节点出现故障，需要进行主从切 换。如果没有这个选项，网络抖动会导致主从频繁切换 (数据的重新复制)。 主从切换 选举原理Redis 集群使用一个类似于木筏算法（Raft algorithm）概念。如果没有了解过可以看看这个 Raft, 在 Redis 集群中这个术语叫做 阶段（epoch）。 Redis 集群中的每个节点，包括主节点和从节点，都在创建的时候设置了 currentEpoch 为0。每次主从选举之后 Epoch 会相应的加 1。如果出现脑裂的情况，其他主节点只认 epoch 最大的那个。 Redis集群选举原理分析当 slave 发现自己的 master 变为FAIL状态时，便尝试进行 Failover，以期成为新的 master。由于挂掉的master 可能会有多个 slave，从而存在多个 slave 竞争成为 master节点的过程， 其过程如下： slave 发现自己的 master变为FAIL 将自己记录的集群currentEpoch（年代，纪元）加 1，并广播FAILOVER_AUTH_REQUEST 信息 其他节点收到该信息，只有master响应，判断请求者的合法性，并发送FAILOVER_AUTH_ACK，对每一个epoch只发送一次ack 尝试failover的slave收集FAILOVER_AUTH_ACK 超过半数后变成新Master 广播 Pong 通知其他集群节点。 从节点并不是在主节点一进入 FAIL 状态就马上尝试发起选举，而是有一定延迟，一定的延迟确保我们等待FAIL状态在集群中传播，slave如果立即尝试选举，其它masters或许尚未意识到FAIL状态，可能会拒绝投票 延迟计算公式： DELAY = 500ms + random(0 ~ 500ms) + SLAVE_RANK * 1000ms SLAVE_RANK 表示此slave已经从master复制数据的总量的rank。Rank越小代表已复制的数据越新。这种方式下，持有最新数据的slave将会首先发起选举（理论上）。 发布订阅机制发布/订阅（Publish/Subscribe）在一个 Redis 集群中，客户端能订阅任何一个节点，也能发布消息给任何一个节点。集群会确保发布的消息都会按需进行转发。 目前的实现方式是单纯地向所有节点广播所有的发布消息，在将来的实现中会用 bloom filters 或其他算法来优化。 参考 Redis 集群教程 数据结构与算法","link":"/2020/02/11/Redis%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"title":"Rocket MQ 扫盲","text":"文章来源 FrancisQ 老哥 消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道似啥吧？ 消息队列扫盲所以问题并不是消息队列是什么，而是 消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？ 消息队列为什么会出现？消息队列算是作为后端程序员的一个必备技能吧，因为分布式应用必定涉及到各个系统之间的通信问题，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。 消息队列能用来干什么？异步你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？ 很好👍，你又提出了一个概念，同步通信。就比如现在业界使用比较多的 Dubbo 就是一个适用于各个系统之间同步通信的 RPC 框架。 我来举个🌰吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。 我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。 当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短息系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 头重脚轻 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？ 这样整个系统的调用链又变长了，整个时间就变成了550ms。 当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。 我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦😋😋😋” 咦~ 为了多吃点，真恶心。 然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。 最终我们从大妈手中接过饭菜然后去寻找座位了… 回想一下，我们在给大妈发送需要的信息之后我们是 同步等待大妈给我配好饭菜 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。 那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 (传达一个消息) ，然后我们就可以在饭桌上安心的玩手机了 (干自己其他事情) ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以看其他事情了，这是一个 异步 的概念。 所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。 这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms = 160ms。 但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。 解耦回到最初同步调用的过程，我们写个伪代码简单概括一下。 那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？ 如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？ 这样改来改去是不是很麻烦，那么 此时我们就用一个消息队列在中间进行解耦 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 result ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 “广播消息” 来实现。 我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 订阅 特定的主题。比如我们这里的主题就可以叫做 订票 ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 生产消息到指定主题中 ，而 消费者只需要关注从指定主题中拉取消息 就行了。 如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。 削峰我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？ 如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 直接崩溃 了？ 短信业务又不是我们的主业务，我们能不能 折中处理 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 尽自己所能地去消息队列中取消息和消费消息 ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。 留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？ 消息队列能带来什么好处？其实上面我已经说了。异步、解耦、削峰。 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。 消息队列会带来副作用吗？没有哪一门技术是“银弹”，消息队列也有它的副作用。 比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 降低了系统的可用性 ？ 那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 整个系统的复杂度是不是上升了 ？ 抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。 或者我消费端处理失败了，请求重发，这样也会产生重复的消息。 对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？ 那么，又 如何解决重复消费消息的问题 呢？ 如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？ 那么，又 如何解决消息的顺序消费问题 呢？ 就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 Spring 的话我们在上面伪代码中加入 @Transactional 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。 那么，又如何 解决分布式事务问题 呢？ 我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？ 那么，又如何 解决消息堆积的问题 呢？ 可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？ 别急，办法总是有的。 RocketMQ是什么？ 哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 RocketMQ ，还让不让人活了？！🤬 别急别急，话说你现在清楚 MQ 的构造吗，我还没讲呢，我们先搞明白 MQ 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。 RocketMQ 是一个 队列模型 的消息中间件，具有高性能、高可靠、高实时、分布式 的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 Apache，成为了 Apache 的一个顶级项目。 在阿里内部，RocketMQ 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 RocketMQ 流转。 废话不多说，想要了解 RocketMQ 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 RocketMQ 很快、很牛、而且经历过双十一的实践就行了！ 队列模型和主题模型在谈 RocketMQ 的技术架构之前，我们先来了解一下两个名词概念——队列模型 和 主题模型 。 首先我问一个问题，消息队列为什么要叫消息队列？ 你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？ 的确，早期的消息中间件是通过 队列 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件成为消息队列。 但是，如今例如 RocketMQ 、Kafka 这些优秀的消息中间件不仅仅是通过一个 队列 来实现消息存储的。 队列模型就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。 在一开始我跟你提到了一个 “广播” 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。 当然你可以让 Producer 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 解耦 这一原则。 主题模型那么有没有好的方法去解决这一个问题呢？有，那就是 主题模型 或者可以称为 发布订阅模型 。 感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。 在主题模型中，消息的生产者称为 发布者(Publisher) ，消息的消费者称为 订阅者(Subscriber) ，存放消息的容器称为 主题(Topic) 。 其中，发布者将消息发送到指定主题中，订阅者需要 提前订阅主题 才能接受特定主题的消息。 RocketMQ中的消息模型RockerMQ 中的消息模型就是按照 主题模型 所实现的。你可能会好奇这个 主题 到底是怎么实现的呢？你上面也没有讲到呀！ 其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的 分区 ，RocketMQ 中的 队列 ，RabbitMQ 中的 Exchange 。我们可以理解为 主题模型/发布订阅模型 就是一个标准，那些中间件只不过照着这个标准去实现而已。 所以，RocketMQ 中的 主题模型 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。 我们可以看到在整个图中有 Producer Group 、Topic 、Consumer Group 三个角色，我来分别介绍一下他们。 Producer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。 Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。 Topic 主题： 代表一类消息，比如订单消息，物流消息等等。 你可以看到图中生产者组中的生产者会向主题发送消息，而 主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。 每个主题中都有多个队列(这里还不涉及到 Broker)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consuer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。 当然也可以消费者个数小于队列个数，只不过不太建议。如下图。 每个消费组在每个队列上维护一个消费位置 ，为什么呢？ 因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 消费位移(offset) ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。 可能你还有一个问题，为什么一个主题中需要维护多个队列 ？ 答案是 提高并发能力 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 发布订阅模式 。如下图。 但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。 所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置 实现了 主题模式/发布订阅模式 。 RocketMQ的架构图讲完了消息模型，我们理解起 RocketMQ 的技术架构起来就容易多了。 RocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。我来向大家分别解释一下这四个角色是干啥的。 Broker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。 这里，我还得普及一下关于 Broker 、Topic 和 队列的关系。上面我讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？ 一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。 如果某个 Topic 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。 Topic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。 所以说我们需要配置多个Broker。 NameServer： 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。 Producer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。 Consumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。 听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？ 嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer 、Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？ 但是，我们上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以我们需要使用多个 Broker 来保证 负载均衡 。 如果说，我们的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。 如果还不是很理解的话，可以去看我介绍 Spring Cloud 的那篇文章，其中介绍了 Eureka 注册中心。 当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。 其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来🤨。 第一、我们的 Broker 做了集群并且还进行了主从部署 ，由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构， salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面我还会提到哦)。 第二、为了保证 HA ，我们的 NameServer 也做了集群部署，但是请注意它是 去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过 单个Broker和所有NameServer保持长连接 ，并且在每隔30秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info 。 第三、在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过 轮询 的方法去向每个队列中生产数据以达到 负载均衡 的效果。 第四、消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据。Consumer 可以以两种模式启动—— 广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给 同一个消费组中的所有消费者 ，集群模式下消息只会发送给一个消费者。 如何解决 顺序消费、重复消费其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 RocketMQ ，而是应该每个消息中间件都需要去解决的。 在上面我介绍 RocketMQ 的技术架构的时候我已经向你展示了 它是如何保证高可用的 ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 RocketMQ 集群。 其实 Kafka 的架构基本和 RocketMQ 类似，只是它注册中心使用了 Zookeeper 、它的 分区 就相当于 RocketMQ 中的 队列 。还有一些小细节不同会在后面提到。 顺序消费在上面的技术架构介绍中，我们已经知道了 RocketMQ 在主题上是无序的、它只有在队列层面才是保证有序 的。 这又扯到两个概念——普通顺序 和 严格顺序 。 所谓普通顺序是指 消费者通过 同一个消费队列收到的消息是有顺序的 ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 。 所谓严格顺序是指 消费者收到的 所有消息 均是有顺序的。严格顺序消息 即使在异常情况下也会保证消息的顺序性 。 但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 binlog 同步。 一般而言，我们的 MQ 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。 那么，我们现在使用了 普通顺序模式 ，我们从上面学习知道了在 Producer 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 三个消息会被发送到不同队列 ，因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。 那么，怎么解决呢？ 其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。 重复消费emmm，就两个字—— 幂等 。在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。 那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？ 所以我们需要给我们的消费者实现 幂等 ，也就是对同一个消息的处理结果，执行多少次都不变。 那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的。当然还有使用 数据库插入法 ，基于数据库的唯一键来保证重复数据不会被插入多条。 不过最主要的还是需要 根据特定场景使用特定的解决方案 ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。 而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题 。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题 ，也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的 重复调用问题 。 分布式事务如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。 那么，如何去解决这个问题呢？ 如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。 在 RocketMQ 中使用的是 事务消息加上事务反查机制 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。 在第一步发送的 half 消息 ，它的意思是 在事务提交之前，对于消费者来说，这个消息是不可见的 。 那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 改变主题 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。 你可以试想一下，如果没有从第5步开始的 事务反查机制 ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 RocketMQ 中就是使用的上述的事务反查来解决的，而在 Kafka 中通常是直接抛出一个异常让用户来自行解决。 你还需要注意的是，在 MQ Server 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。 消息堆积问题在上面我们提到了消息队列一个很重要的功能——削峰 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？ 其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。 我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 限流降级 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 是否是消费者出现了大量的消费错误 ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。 当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 同时你还需要增加每个主题的队列数量 。 别忘了在 RocketMQ 中，一个队列只会被一个消费者消费 ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。 回溯消费回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ 中， Broker 在向Consumer 投递成功消息后，消息仍然需要保留 。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。 这是官方文档的解释，我直接照搬过来就当科普了😁😁😁。 RocketMQ 的刷盘机制上面我讲了那么多的 RocketMQ 的架构和设计原理，你有没有好奇 在 Topic 中的 队列是以什么样的形式存在的？ 队列中的消息又是如何进行存储持久化的呢？ 我在上文中提到的 同步刷盘 和 异步刷盘 又是什么呢？它们会给持久化带来什么样的影响呢？ 下面我将给你们一一解释。 同步刷盘和异步刷盘 如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK ，同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是 性能上会有较大影响 ，一般地适用于金融等特定业务场景。 而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， 降低了读写延迟 ，提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。 一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。 同步复制和异步复制上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。 同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到主从结点上时才返回写入成功 。 异步复制： 消息写入主节点之后就直接返回写入成功 。 然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。 那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？ 答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 可用性 。为什么呢？其主要原因是 RocketMQ 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。 比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。 在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个 Topic 是分布在不同 Broker 中的。 但是这种复制方式同样也会带来一个问题，那就是无法保证 严格顺序 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 Topic 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。 而在 RocketMQ 中采用了 Dledger 解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。 也不是说 Dledger 是个完美的方案，至少在 Dledger 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制板书以上节点的效率和直接异步复制还是有一定的差距的。 存储机制还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。 但是，在 Topic 中的 队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？ 还未解决，其实这里涉及到了 RocketMQ 是如何设计它的存储结构了。我首先想大家介绍 RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile 。 CommitLog： 消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。 ConsumeQueue： 消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset **，消息大小 size 和消息 Tag 的 HashCode 值。consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件**，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M； IndexFile： IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。 总结来说，整个消息存储的结构，最主要的就是 CommitLoq 和 ConsumeQueue 。而 ConsumeQueue 你可以大概理解为 Topic 中的队列。 RocketMQ 采用的是 混合型的存储结构 ，即为 Broker 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 Topic 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。 而 RocketMQ 为什么要这么做呢？原因是 提高数据的写入效率 ，不分 Topic 意味着我们有更大的几率获取 成批 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。 所以，在 RocketMQ 中又使用了 ConsumeQueue 作为每个队列的索引文件来 提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。 讲到这里，你可能对 RockeMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。 emmm，是不是有一点复杂🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。 如果上面没看懂的读者一定要认真看下面的流程分析！ 首先，在最上面的那一块就是我刚刚讲的你现在可以直接 **把 ConsumerQueue 理解为 Queue**。 在图中最左边说明了 红色方块 代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic 、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接 **全部顺序存储到了 CommitLog **。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。 上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。 因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考🤔🤔一下吧。 为什么 CommitLog 文件要设计成固定大小的长度呢？提醒：内存映射机制。 总结总算把这篇博客写完了。我讲的你们还记得吗😅？ 这篇文章中我主要想大家介绍了 消息队列出现的原因 消息队列的作用(异步，解耦，削峰) 消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等) 消息队列的两种消息模型——队列和主题模式 分析了 RocketMQ 的技术架构(NameServer 、Broker 、Producer 、Comsumer) 结合 RocketMQ 回答了消息队列副作用的解决方案 介绍了 RocketMQ 的存储机制和刷盘策略。 等等。。。","link":"/2020/08/23/Rocket-MQ-%E6%89%AB%E7%9B%B2/"},{"title":"Python 疫苗爬虫脚本","text":"关于疫苗的python爬虫脚本，主要爬新华网。 源码地址查看 Github brokge / vaccine-scrapy 1. 需要环境 python 3.5.2 scrapy 1.5.2 conda 4.6.11 或者 pip3 pycryptodome pymysql 如果碰到 兼容性问题最好的方式，是通过 conda 安装所需要的环境变量，记得安装之后重启下系统 conda install -c conda-forge scrapy 当然也可以通过 pip3 来安装 pip3 --default-timeout=100 install scrapy pip3 install pycryptodome pip3 install pymysql 2. 运行scrapy crawl spiderNews 3. 文件目录说明通过 tree 命令查看目录结构 . ── README.md ├── scrapy.cfg └── vaccine ├── README.md ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── items.cpython-37.pyc │ ├── pipelines.cpython-37.pyc │ └── settings.cpython-37.pyc ├── dbsql.sql ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py ├── spiders │ ├── VaccineNewsSpider.py │ ├── __init__.py │ └── __pycache__ │ ├── VaccineNewsSpider.cpython-37.pyc │ └── __init__.cpython-37.pyc └── vaccine_name.txt dbsql.sql —数据表结构脚本 vaccine_name.txt —需要查询的关键字配置文件 VaccineNewsSpider.py — 爬虫执行文件 pipelines.py — 配置 通道的位置，根据自己情况调整 # 链接数据的配置 self.connect = pymysql.connect( # 数据库地址 host=\"127.0.0.1\", port=3306, db=\"dbcontent\", user='yusuzi', passwd='yusuzi', charset='utf8', use_unicode=True ) 4. 执行效果截图 5. 核心逻辑在 VaccineNewsSpider.py 文件中，主要逻辑流程如下 构造 url请求地址， 填充 关键字 请求 url 解析 返回数据 根据请求url 生成 md5 去重。 写入数据哭。 5.1 构造请求并解析返回数据class VaccineNewsSpider(scrapy.Spider): name = \"spiderNews\" url_format = \"http://so.news.cn/getNews?keyword=%s&curPage=%s&sortField=0&searchFields=1&lang=cn\" def start_requests(self): keywords = set() try: f = open('vaccine/vaccine_name.txt', 'r') for line in f.readlines(): line = line.strip('\\n') # 去除换行符号 keywords.add(line) finally: if f: f.close() url_format: 根据自己不同的目标网址，进行构造请求url def parse_json(self, response): print(\"aa\") print(response.body) newJson = json.loads(response.body) print(newJson['code']) newContent = newJson['content'] results = newContent['results'] pageSize = newContent['pageCount'] curPage = newContent['curPage'] keyWord = newContent['keyword'] newSets = set() for result in results: url = result['url'] if 'm.xinhuanet.com' in url and result['des']: html_remove = re.compile(r']+>', re.S) # 构建匹配模式 # dd = dr.sub('', string) # 去除html标签 title = html_remove.sub('', result['title']) print(\"title\"+title) vaccineNewsItem = VaccineNewsItem() vaccineNewsItem['title'] = title.lstrip() vaccineNewsItem['from_url'] = url # result['sitename'] vaccineNewsItem['from_source'] = '新华网' vaccineNewsItem['summary'] = result['des'].lstrip() vaccineNewsItem['create_date'] = result['pubtime'] vaccineNewsItem['md5'] = self.md5_str(str=title) vaccineNewsItem['keyword'] = keyWord newSets.add(vaccineNewsItem) yield vaccineNewsItem if curPage < pageSize: curPage = curPage+1 url = self.url_format % (keyWord, curPage) print(url) yield scrapy.Request(url, callback=self.parse_json) def md5_str(self, str): m = MD5.new() m.update(str.encode(\"utf-8\")) return m.hexdigest() 以上代码为请求 url 并解析请求数据。 5.2 写入数据库首先：VaccinePipeline.py 中配置通道相关信息，比如数据库链接配置，数据实体映射配置，插入数据执行sql配置等。 其次：setting.py 中 激活通道，并配置优先级 ITEM_PIPELINES = { 'vaccine.pipelines.MySQLPipeLine': 300, } 通过以上基本就可以跑起来了。运行中有任何问题欢迎反馈。","link":"/2020/08/12/Python-%E7%96%AB%E8%8B%97%E7%88%AC%E8%99%AB%E8%84%9A%E6%9C%AC/"},{"title":"Nginx 动态页面缓存方式实现","text":"具体实现步骤：依托于 OpenResty 框架实现 一、Openresty安装mkdir ‐p /usr/servers cd /usr/servers/ yum install‐y readline‐devel pcre‐devel openssl‐devel gcc wget http://openresty.org/download/ngx_openresty‐1.7.7.2.tar.gz tar ‐xzvf ngx_openresty‐1.7.7.2.tar.gz cd /usr/servers/ngx_openresty‐1.7.7.2/ cd bundle/LuaJIT‐2.1‐20150120/ make clean && make&& make install ln ‐sf luajit‐2.1.0‐alpha /usr/local/bin/luajit cd /usr/servers/ngx_openresty‐1.7.7.2/bundle wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz tar‐xvf 2.3.tar.gz cd bundle wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz tar ‐xvf v0.3.0.tar.gz cd /usr/servers/ngx_openresty‐1.7.7.2 ./configure‐‐prefix=/usr/servers‐‐with‐http_realip_module‐‐with‐pcre‐‐with‐luajit‐‐add‐module=./bundle/n gx_cache_purge‐2.3/ ‐‐add‐module=./bundle/nginx_upstream_check_module‐0.3.0/ ‐j2 24 make&&makeinstall cd /usr/servers/ ll /usr/servers/luajit /usr/servers/lualib /usr/servers/nginx /usr/servers/nginx/sbin/nginx ‐V ##启动nginx: /usr/servers/nginx/sbin/nginx 二、 Nginx + lua 开发 hello worldcd /usr/servers/nginx/conf vim lua.conf server{ listen 80; server_name _; location /lua { default_type 'text/html'; content_by_lua 'ngx.say(\"helloworld\")'; } } vim nginx.conf ##在http部分引入lua包 lua_package_path\"/usr/servers/lualib/?.lua;;\"; lua_package_cpath\"/usr/servers/lualib/?.so;;\"; include lua.conf; ../sbin/nginx‐sreload #访问:http://192.168.0.60/lua 三、Nginx+lua开发的流量分发逻辑，可以根据商品id 进行 hash 然后取模分发。#流量分发的nginx，会发送http请求到后端的应用层nginx上去，所以要先引入lua http lib包 cd /usr/servers/lualib/resty wget https://raw.githubusercontent.com/pintsized/lua‐resty‐http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua‐resty‐http/master/lib/resty/http.lua cd /usr/servers/nginx/conf lua-distribution.lua vim lua‐distribution.lua # lua-distribution.lua local uri_args=ngx.req.get_uri_args() local productId=uri_args[\"productId\"] local host={\"192.168.0.61\",\"192.168.0.62\"} local hash=ngx.crc32_long(productId) hash = (hash%2)+1 backend=\"http://\"..host[hash] local method = uri_args[\"method\"] local requestBody=\"/\"..method..\"?productId=\"..productId local http=require(\"resty.http\") localhttpc=http.new() local resp,err=httpc:request_uri(backend,{ method = \"GET\", path = requestBody, keepalive=false }) if not resp then ngx.say(\"request error :\", err) return end ngx.say(resp.body) httpc:close() 打开 lua.conf vim lua.conf 在server部分加入 location/product{ default_type 'text/html'; content_by_lua_file /usr/servers/nginx/conf/lua‐distribution.lua; } ../sbin/nginx‐sreload #访问:http://192.168.0.60/lua?productId=XX 会根据productId将请求分发到不同的应用层nginx 四、Nginx+lua开发应用层页面缓存与模板动态渲染逻辑##应用层需要访问服务http接口，所以也需要引入lua http lib包 cd /usr/servers/lualib/resty wget https://raw.githubusercontent.com/pintsized/lua‐resty‐http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua‐resty‐http/master/lib/resty/http.lua ##应用层还需要用到模板动态渲染技术，所以还需要引入lua的template包 cd /usr/servers/lualib/resty wget https://raw.githubusercontent.com/bungle/lua‐resty‐template/master/lib/resty/template.lua mkdir/usr/servers/lualib/resty/html cd /usr/servers/lualib/resty/html wget https://raw.githubusercontent.com/bungle/lua‐resty‐template/master/lib/resty/template/html.lua cd /usr/servers/templates vim product.html 增加html静态模板 &lt;html> &lt;head> &lt;meta http‐equiv=\"Content‐Type\" content=\"text/html; charset=utf‐8\" /> &lt;/head> &lt;body> 商品id: {* productId *}&lt;br/> 商品名称: {* productName *}&lt;br/> 商品原价: {* productPrice *}&lt;br/> 商品现价: {* productNowPrice *}&lt;br/> 商品库存: {* productStock *}&lt;br/> 商品描述: {* productHTML *}&lt;br/> &lt;/body> &lt;/html> cd /usr/servers/nginx/conf vim lua.conf ## 在server部分加入 set $template_location\"/templates\"; set $template_root\"/usr/servers/templates\"; ## 增加ngxin缓存配置 vim lua.conf ## 在最前面加入 lua_shared_dictmy_cache 128m; ## 增加商品详情页渲染的lua脚本 vim lua.conf ## 在server部分增加 location/product{ default_type 'text/html'; content_by_lua_file /usr/servers/nginx/conf/product.lua; } ## 最后，编写商品详情页渲染lua脚本 vim product.lua local uri_args=ngx.req.get_uri_args() local productId=uri_args[\"productId\"] local cache_ngx=ngx.shared.my_cache local productCacheKey=\"product_info_\"..productId local productCache=cache_ngx:get(productCacheKey) if productCache == \"\" or productCache == nil then local http=require(\"resty.http\") local httpc=http.new() local resp,err=httpc:request_uri(\"http://192.168.0.175:8080\",{ method=\"GET\", path=\"/shop‐web/product/cache/\"..productId,keepalive=false }) productCache = resp.body local expireTime = math.random(600,1200) cache_ngx:set(productCacheKey,productCache,expireTime) end ngx.log(ngx.ERR,\"json‐‐‐‐‐‐‐‐2\",productCache) local cjson=require(\"cjson\") local productCacheJSON=cjson.decode(productCache) local context={ productId=productCacheJSON.id, productName=productCacheJSON.name, productPrice=productCacheJSON.price, productNowPrice=productCacheJSON.nowPrice, productStock=productCacheJSON.stock, productHTML=productCacheJSON.productHTML } local template=require(\"resty.template\") template.render(\"product.html\",context) ../sbin/nginx‐sreload ## 访问:http://192.168.0.60/product?productId=XX&method=product ## 会根据productId将请求分发到不同的应用层nginx获取相应的商品详情页面","link":"/2020/06/25/Nginx-%E5%8A%A8%E6%80%81%E9%A1%B5%E9%9D%A2%E7%BC%93%E5%AD%98%E6%96%B9%E5%BC%8F%E5%AE%9E%E7%8E%B0/"},{"title":"Zookeeper 核心原理详解","text":"一、Zookeeper 简介Zookeeper 是一个开源的分布式应用程序协调服务器，是 Hadoop 的重要组件。ZooKeeper 是一个分布式的，开放源码的分布式应用程序协调服务器，是 Google 的Chubby 一个开源的实现，是 Hadoop 和 Hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、集群管理等。ZooKeeper的目标就是封装复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 ZK 的一特性 顺序一致性从同一客户端发起的多个事务请求（写操作请求），最终会严格按照发起顺序存储到 zk中。 原子性所有的事务请求结果，在集群中所有 Server 上的应用情况是一致的，要么全部成功要么都不成功，不会出现部分成功，部分失败的情况。 单一视图无论客户端连接集群中的任意服务器，其读取到的数据模型都是一致的。 可靠性一旦某事务成功应用到zk ，则会一直保留下来，除非另一个事务将其修改。 最终一致性zk 可以保证在一段较短的时间内，客户端最终一定能够从服务端读取到最新的数据。但不保证实时性。 以上特性 则由 zab 协议保证，后面会说。 二、数据模型Zookeeper 数据模型，是一个树状目录结构（类似于我们的文件系统），最小单位为 znode. Znode 对象包含多个属性字段，其对象结构如下。 四种节点类型2 大类、四种类型： 持久、临时、持久有序、临时有序 PERSISTENT 持久类型，如果不手动删除 是一直存在的 PERSISTENT_SEQUENTIAL 持久有序 EPHEMERAL 临时 客户端 session 失效就会随着删除节点 没有子节点 EPHEMERAL_SEQUENTIAL 临时有序 自增 注意：有序节点 在创建 znode 时设置顺序标识，znode 名称后会附加一个值顺序号是一个单调递增的计数器，由父节点维护在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 三、 Watcher 机制Watcher实现由三个部分组成： Zookeeper服务端； Zookeeper客户端（常用 Zkclient、Curator）； 客户端的ZKWatchManager对象； 客户端首先将Watcher注册到服务端，同时将Watcher 对象保存到客户端的 Watch管理器中。当 ZooKeeper 服务端监听的数据状态发生变化时，服务端会主动通知客户端，接着客户端的Watch管理器会触发相关Watcher来回调相应处理逻辑，从而完成整体的数据发布/订阅流程。 Watcher特性 一次性：Watcher是一次性的，一旦被触发就会移除，再次使用时需要重新注册。 客户端顺序回调： Watcher回调是顺序串行化执行的，只有回调后客户端才能看到最新的数据状态。一个Watcher回调逻辑不应该太多，以免影响别的watcher执行 轻量级： WatchEvent是最小的通信单元，结构上只包含通知状态、事件类型和节点路径，并不会告诉数据节点变化前后的具体内容； 时效性：Watcher只有在当前session彻底失效时才会无效，若在 session 有效期内快速重连成功，则 watcher 依然存在，仍可接收到通知； 四、一致性 协议说起 一致性协议，则不得不说，Paxos 算法 。常用一致性协议包含 Paxos / Raft / zab 等协议，而 Raft，zab 其实就是 Paxos 的变种。 Paxos 算法由图灵奖获得者 Leslie Lamport 于 1990 年提出的一种基于消息传递且具有高度容错的特性的一致性算法。Paxos的出现就是为了解决 一致性问题。 Paxos 有三种角色 倡议者（Proposer）：倡议者可以提出提案（数值或者操作命令）以供投票表决 接受者（Acceptor）：接受者可以对倡议者提出的提议进行投票表决，提议有超半数的接受者投票即被选中。 学习者（Learner）：学习者无投票权，只是从接受者那里获知哪个提议被选中 。 Paxos 算法明确提出： 一个或多个倡议者发出提案。 必须针对所有提案中的某个提案达成一致也就是（接受者）半数以上；最多只能对一个确定的提案达成一致。 只要超半数的节点存活且可互相通信，整个系统一定能达成一致状态。 详细可查看： 一致性算法 Paxos ZAB 协议Zookeeper 的核心是广播，这个机制保证了各个 zkServer 间数据的同步，即数据的一致性。实现这个机制的协议叫做 ZAB 协议，即 Zookeeper Atomic Broadcast，Zookeeper 原子广播协议。 ZAB 协议有三种模式：恢复模式、同步模式 和 广播模式。 恢复模式：在服务重启过程中，或在 Leader 崩溃后，就进入了恢复模式，要恢复到 zk集群正常的工作状态。 同步模式：在所有的 zkServer 启动完毕，或 Leader 崩溃后又被选举出来时，就进入了同步模式，各个 Follower 需要马上将 Leader 中的数据同步到自己的主机中。当大多数zkServer 完成了与 Leader 的状态同步以后，恢复模式就结束了。所以，同步模式包含在恢复模式过程中。 广播模式：当 Leader 的提议被大多数 zkServer 同意后，Leader 会修改自身数据，然后会将修改后的数据广播给其它 Follower。 五、CAP 原则 分布式领域 CAP 理论 Consistency(一致性), 数据一致更新，所有数据变动都是同步的 Availability(可用性), 好的响应性能 Partition tolerance(分区容忍性) 可靠性 定理：任何分布式系统只可同时满足二点，没法三者兼顾。 zookeeper是需要提供强一致性的服务，但是在zk集群中如果leader宕机，会进入leader选举过程，而在这个过程中zk不对外提供服务的，这会造成注册服务短时不可用。所以在分区容错性和数据一致性上得到保证的前提下，对 Availability(可用性) 做了折中。 所以 ZooKeeper 是个 CP 的，即任何时刻对 ZooKeeper 的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性。也就是在极端环境下，ZooKeeper 可能会丢弃一些请求，消费者程序需要重新请求才能获得结果 六、选举机制三类角色为了避免 Zookeeper 的单点问题，zk 也是以集群的形式出现的。zk 集群中的角色主要有以下三类： Leader：zk 集群写请求的唯一处理者，并负责进行投票的发起和决议，更新系统状态。Leader 是很民主的，并不是说其在接收到写请求后马上就修改其中保存的数据，而是首先根据写请求提出一个提议，在大多数 zkServer 均同意时才会做出修改。 Follower：接收客户端请求，处理读请求，并向客户端返回结果；将写请求转给 Leader；在选主(选 Leader)过程中参与投票。 Observer：可以理解为无选主投票权的 Flollower，其主要是为了协助 Follower 处理更多的读请求。如果 Zookeeper 集群的读请求负载很高，或者客户端非常非常多，多到跨机房，则可以设置一些 Observer 服务器，以提高读取的吞吐量。 四个数据 myid 这是 zk 集群中服务器的唯一标识，称为 myid。例如，有三个 zk 服务器，那么编号分别是 1,2,3。 zxid zxid 为 Long 类型，其中高 32 位表示 epoch，低 32 位表示 xid。即 zxid 由两部分构成：epoch 与 xid。每个 Leader 都会具有一个不同的 epoch 值，表示一个时期、时代。新的 Leader 产生，则会更新所有 zkServer 的 zxid 中的 epoch。 xid 则为 zk 的事务id，只会增加，每一个写操作都是一个事务，都会有一个 xid。每一个写操作都需要由 Leader 发起一个提议，由所有 Follower 表决是否同意本次写操作。 逻辑时钟 逻辑时钟，Logicalclock，是一个整型数，该概念在选举时称为 logicalclock，而在 zxid 中则为 epoch 的值。即 epoch 与 logicalclock 是同一个值，在不同情况下的不同名称。 四种状态根据不同角色和进行中的流程，每个服务器存在不同的状态，包括以下 4 中。 LOOKING，选举状态(查找 Leader 的状态)。 FOLLOWING，随从状态，同步 leader 状态。处于该状态的服务器称为 Follower。 OBSERVING，观察状态，同步 leader 状态。处于该状态的服务器称为 Observer。 LEADING，领导者状态。处于该状态的服务器称为 Leader。 七、应用场景 分布式配置中心 分布式锁 服务注册中心 分布式通知 八、引用图解 Paxos 一致性协议 zooKeeper中的CAP模型以及它作为服务发现的缺点","link":"/2019/12/05/Zookeeper-%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"},{"title":"Hexo 使用说明","text":"Quick Start Create a new post Run server Generate static files Deploy to remote sites Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","link":"/2019/01/08/hello-world/"},{"title":"Mysql explain详解和最佳实践","text":"使用EXPLAIN关键字可以模拟优化器执行SQL语句，从而知道MySQL是 如何处理你的SQL语句的。分析你的查询语句或是结构的性能瓶颈。 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询时，会返回执行计划的信息，而不是执行这条SQL（如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中） 建表DROP TABLE IF EXISTS `actor`; CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,'a','2017-12-22 15:27:18'), (2,'b','2017-12-22 15:27:18'), (3,'c','2017-12-22 15:27:18'); DROP TABLE IF EXISTS `film`; CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film` (`id`, `name`) VALUES (3,'film0'),(1,'film1'),(2,'film2'); DROP TABLE IF EXISTS `film_actor`; CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, `remark` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1); Explainmysql> explain select * from actor; 在查询中的每个表会输出一行，如果有两个表通过 join 连接查询，那么会输出两行。表的意义相当广泛：可以是子查询、一个 union 结果等。 Explain 有两个变种：1. explain extended：会在 explain 的基础上额外提供一些查询优化的信息。紧随其后通过 show warnings 命令可以 得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个半分比的值，rows * filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的表）。 mysql&gt; explain extended select * from film where id = 1; mysql&gt; show warnings; 2. explain partitions：相比 explain 多了个 partitions 字段，如果查询是基于分区表的话，会显示查询将访问的分区。 Explain 中列的含义Id 列id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。MySQL将 select 查询分为简单查询(SIMPLE)和复杂查询(PRIMARY)。复杂查询分为三类：简单子查询、派生表（from语句中的子查询）、union 查询。id列越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。 简单子查询 mysql&gt; explain select (select 1 from actor limit 1) from film; from子句中的子查询 mysql&gt; explain select id from (select id from film) as der; 这个查询执行时有个临时表别名为der，外部 select 查询引用了这个临时表. union查询 mysql&gt; explain select 1 union all select 1; union结果总是放在一个匿名临时表中，临时表不在SQL中出现，因此它的 id 是 NULL。 select_type列select_type 表示对应行是简单还是复杂的查询，如果是复杂的查询，又是上述三种复杂查询中的哪一种。 simple：简单查询。查询不包含子查询和union mysql&gt; explain select * from film where id = 2; primary：复杂查询中最外层的 select subquery：包含在 select 中的子查询（不在 from 子句中） derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义）用这个例子来了解 primary、subquery 和 derived 类型 mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der; union：在 union 中的第二个和随后的 select union result：从 union 临时表检索结果的 select用这个例子来了解 union 和 union result 类型： mysql&gt; explain select 1 union all select 1; table 列这一列表示 explain 的一行正在访问哪个表。当 from 子句中有子查询时，table 列是 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查询。当有 union 时，UNION RESULT 的 table 列的值为&lt;union1,2&gt;，1和2表示参与 union 的 select 行 id。 type列这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行记录的大概范围。依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; range &gt; index &gt; ALL一般来说，得保证查询达到range级别，最好达到ref. NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表. mysql&gt; explain select min(id) from film; const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。system是const的特例，表里只有一条元组匹配时为system mysql&gt; explain extended select * from (select * from film where id = 1) tmp; mysql&gt; show warnings; eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id; ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。 简单 select 查询，name是普通索引（非唯一索引） mysql&gt; explain select * from film where name = &quot;film1&quot;; 关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。 mysql&gt; explain select film_id from film left join film_actor on film.id = film_actor.film_id; range：范围扫描通常出现在 in(), between ,&gt; ,&lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。 mysql&gt; explain select * from actor where id &gt; 1; index：扫描全表索引，这通常比ALL快一些。（index是从索引中读取的，而all是从硬盘中读取） mysql&gt; explain select * from film; ALL：即全表扫描，意味着mysql需要从头到尾去查找所需要的行。通常情况下这需要增加索引来进行优化了 mysql&gt; explain select * from actor; possible_keys列这一列显示查询可能使用哪些索引来查找。explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果。 key列 这一列显示mysql实际采用哪个索引来优化对该表的访问。如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 key_len列 这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。 mysql&gt; explain select * from film_actor where film_id = 2; key_len计算规则如下：字符串char(n)：n字节长度varchar(n)：2字节存储字符串长度，如果是utf-8，则长度 3n + 2 数值类型:tinyint：1字节 smallint：2字节 int：4字节bigint：8字节 时间类型 date：3字节timestamp：4字节datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。 ref列这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），字段名（例：film.id） rows列这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 Extra列这一列展示的是额外信息。常见的重要值如下： Using index：查询的列被索引覆盖，并且where筛选条件是索引的前导列，是性能高的表现。一般是使用了覆盖索引(索引包含了所有查询的字段)。对于innodb来说，如果是辅助索引性能会有不少提高 mysql&gt; explain select film_id from film_actor where film_id = 1; Using where：查询的列未被索引覆盖，where筛选条件非索引的前导列 mysql&gt; explain select * from actor where name = 'a'; Using where Using index：查询的列被索引覆盖，并且==where筛选条件是索引列之一但是不是索引的前导列==，意味着无法直接通过索引查找来查询到符合条件的数据 mysql&gt; explain select film_id from film_actor where actor_id = 1; NULL：查询的列未被索引覆盖，并且where筛选条件是索引的前导列，意味着用到了索引，但是部分字段未被索引覆盖，必须通过“回表”来实现，不是纯粹地用到了索引，也不是完全没用到索引 mysql&gt;explain select * from film_actor where film_id = 1; Using index condition：与Using where类似，查询的列不完全被索引覆盖，where条件中是一个前导列的范围； mysql&gt; explain select * from film_actor where film_id &gt; 1; Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。 actor.name 没有索引，此时创建了张临时表来 distinct mysql&gt; explain select distinct name from actor; film.name 建立了idx_name索引，此时查询时extra是using index,没有用临时表 mysql&gt; explain select distinct name from film; Using filesort：mysql 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。此时mysql会根据联接类型浏览所有符合条件的记录，并保存排序关键字和行指针，然后排序关键字并按顺序检索行信息。这种情况下一般也是要考虑使用索引来优化的。 actor.name 未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录 mysql&gt; explain select * from actor order by name; film.name 建立了idx_name索引,此时查询时extra是using index mysql&gt; explain select * from film order by name; 索引最佳实践使用的表： CREATE TABLE `employees` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(24) NOT NULL DEFAULT '' COMMENT '姓名', `age` int(11) NOT NULL DEFAULT '0' COMMENT '年龄', `position` varchar(20) NOT NULL DEFAULT '' COMMENT '职位', `hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间', PRIMARY KEY (`id`), KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8 COMMENT='员工记录表'; INSERT INTO employees(name,age,position,hire_time) VALUES('LiLei',22,'manager',NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES('HanMeimei', 23,'dev',NOW()); INSERT INTO employees(name,age,position,hire_time) VALUES('Lucy',23,'dev',NOW()); 最佳实践规则1. 全值匹配EXPLAIN SELECT * FROM employees WHERE name= 'LiLei'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22 AND position ='manager'; 2. 最佳左前缀法则如果索引了多列，要遵守最左前缀法则。指的是查询从引的最左前列开始并且不跳过索引中的列。 EXPLAIN SELECT * FROM employees WHERE age = 22 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE position = 'manager'; EXPLAIN SELECT * FROM employees WHERE name = 'LiLei'; 3. 不在索引列上做任何操作（计算、函数、（自动or手动）类型转换），会导致索引失效而转向全表扫描EXPLAIN SELECT * FROM employees WHERE name = 'LiLei'; EXPLAIN SELECT * FROM employees WHERE left(name,3) = 'LiLei'; 4. 存储引擎不能使用索引中范围条件右边的列EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 22 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age &gt; 22 AND position ='manager'; 5. 尽量使用覆盖索引（只访问索引的查询（索引列包含查询列）），减少select *语句EXPLAIN SELECT name,age FROM employees WHERE name= 'LiLei' AND age = 23 AND position ='manager'; EXPLAIN SELECT * FROM employees WHERE name= 'LiLei' AND age = 23 AND position ='manager'; 6. mysql在使用不等于（！=或者&lt;&gt;）的时候无法使用索引会导致全表扫描EXPLAIN SELECT * FROM employees WHERE name != 'LiLei' 7. is null,is not null 也无法使用索引EXPLAIN SELECT * FROM employees WHERE name is null 8. like以通配符开头（’$abc…’）mysql索引失效会变成全表扫描操作EXPLAIN SELECT * FROM employees WHERE name like '%Lei' EXPLAIN SELECT * FROM employees WHERE name like 'Lei%' 问题：解决like’%字符串%’索引不被使用的方法？ a. 使用覆盖索引，查询字段必须是建立覆盖索引字段 EXPLAIN SELECT name,age,position FROM employees WHERE name like '%Lei%'; b. 当覆盖索引指向的字段是 varchar(380) 及 380 以上的字段时，覆盖索引会失效！ 9. 字符串不加单引号索引失效EXPLAIN SELECT * FROM employees WHERE name = '1000'; EXPLAIN SELECT * FROM employees WHERE name = 1000; 10. 少用or,用它连接时很多情况下索引会失效EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' or name = 'HanMeimei'; 实践总结： like KK%相当于=常量，%KK和%KK% 相当于范围","link":"/2020/08/05/Mysql-explain%E8%AF%A6%E8%A7%A3%E5%92%8C%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"title":"kafka 集群搭建","text":"1. 安装前的环境准备由于Kafka是用Scala语言开发的，运行在JVM上，因此在安装Kafka之前需要先安装JDK。 # yum install java-1.8.0-openjdk* -y kafka依赖zookeeper，所以需要先安装zookeeper # wget http://mirror.bit.edu.cn/apache/zookeeper/stable/zookeeper-3.4.12.tar.gz # tar -zxvf zookeeper-3.4.12.tar.gz # cd zookeeper-3.4.12 # cp conf/zoo_sample.cfg conf/zoo.cfg 启动zookeeper # bin/zkServer.sh start # bin/zkCli.sh # ls / #查看zk的根目录相关节点 2. 安装第一步：下载安装包下载1.1.0 release版本，并解压： # wget https://archive.apache.org/dist/kafka/1.1.0/kafka_2.11-1.1.0.tgz # tar -xzf kafka_2.11-1.1.0.tgz # cd kafka_2.11-1.1.0 第二步：启动服务现在来启动kafka服务：启动脚本语法： kafka-server-start.sh [-daemon] server.properties 可以看到，server.properties的配置路径是一个强制的参数，-daemon表示以后台进程运行，否则ssh客户端退出后，就会停止服务。(注意，在启动kafka时会使用linux主机名关联的ip地址，所以需要把主机名和linux的ip映射配置到本地host里，用vim /etc/hosts) # bin/kafka-server-start.sh -daemon config/server.properties 我们进入zookeeper目录通过zookeeper客户端查看下zookeeper的目录树 # bin/zkCli.sh # ls / #查看zk的根目录kafka相关节点 # ls /brokers/ids #查看kafka节点 3. 创建主题现在我们来创建一个名字为“test”的Topic，这个topic只有一个partition，并且备份因子也设置为1： # bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 现在我们可以通过以下命令来查看kafka中目前存在的topic # bin/kafka-topics.sh --list --zookeeper localhost:2181 除了我们通过手工的方式创建Topic，我们可以配置broker，当producer发布一个消息某个指定的Topic，但是这个Topic并不存在时，就自动创建。 4. 发送消息kafka自带了一个producer命令客户端，可以从本地文件中读取内容，或者我们也可以以命令行中直接输入内容，并将这些内容以消息的形式发送到kafka集群中。在默认情况下，每一个行会被当做成一个独立的消息。首先我们要运行发布消息的脚本，然后在命令中输入要发送的消息的内容： # bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test &gt;this is a msg &gt;this is a another msg 5. 消费消息对于consumer，kafka同样也携带了一个命令行客户端，会将获取到内容在命令中进行输出： # bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning #老版本 # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --consumer-property group.id=testGroup --consumer-property client.id=consumer-1 --topic test #新版本 如果你是通过不同的终端窗口来运行以上的命令，你将会看到在producer终端输入的内容，很快就会在consumer的终端窗口上显示出来。以上所有的命令都有一些附加的选项；当我们不携带任何参数运行命令的时候，将会显示出这个命令的详细用法。还有一些其他命令如下：查看组名 # bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list --new-consumer 查看消费者的消费偏移量 # bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group testGroup 消费多主题 # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --whitelist &quot;test|test-2&quot; 5.1 单播消费一条消息只能被某一个消费者消费的模式，类似queue模式，只需让所有消费者在同一个消费组里即可分别在两个客户端执行如下消费命令，然后往主题里发送消息，结果只有一个客户端能收到消息 # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --consumer-property group.id=testGroup --topic test 5.2 多播消费一条消息能被多个消费者消费的模式，类似publish-subscribe模式费，针对Kafka同一条消息只能被同一个消费组下的某一个消费者消费的特性，要实现多播只要保证这些消费者属于不同的消费组即可。我们再增加一个消费者，该消费者属于testGroup-2消费组，结果两个客户端都能收到消息 # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --consumer-property group.id=testGroup-2 --topic test 6. kafka集群配置到目前为止，我们都是在一个单节点上运行broker，这并没有什么意思。对于kafka来说，一个单独的broker意味着kafka集群中只有一个接点。要想增加kafka集群中的节点数量，只需要多启动几个broker实例即可。为了有更好的理解，现在我们在一台机器上同时启动三个broker实例。 6.1 第一步配置文件我们需要建立好其他2个broker的配置文件： # cp config/server.properties config/server-1.properties # cp config/server.properties config/server-2.properties 配置文件的内容分别如下： config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dir=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dir=/tmp/kafka-logs-2 broker.id 属性在kafka集群中必须要是唯一的。 我们需要重新指定port和log目录，因为我们是在同一台机器上运行多个实例。如果不进行修改的话，consumer 只能获取到一个 instance 实例的信息，或者是相互之间的数据会被影响。目前我们已经有一个zookeeper实例和一个broker实例在运行了. 6.2 第二步 启动实例现在我们只需要在启动2个broker实例即可： # bin/kafka-server-start.sh -daemon config/server-1.properties # bin/kafka-server-start.sh -daemon config/server-2.properties 6.3 第三步 创建新的topic现在我们创建一个新的topic，备份因子设置为3： # bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 现在我们已经有了集群，并且创建了一个3个备份因子的topic，但是到底是哪一个broker在为这个topic提供服务呢(因为我们只有一个分区，所以肯定同时只有一个broker在处理这个topic)？ # bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic 以下是输出内容的解释，第一行是所有分区的概要信息，之后的每一行表示每一个partition的信息。因为目前我们只有一个partition，因此关于partition的信息只有一行。leader节点负责给定partition的所有读写请求。replicas 表示某个partition在哪几个broker上存在备份。不管这个几点是不是”leader“，甚至这个节点挂了，也会列出。isr 是replicas的一个子集，它只列出当前还存活着的，并且备份了该partition的节点。 现在我们的案例中，0号节点是leader，即使用server.properties 启动的那个进程。 6.4 第四步 发送消息现在我们向新建的topic中发送一些message： # bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic &gt;my test msg 1 &gt;my test msg 2 6.5 第五步 消费消息# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic my test msg 1 my test msg 2 6.6 第六步 测试容错性现在我们来测试我们容错性，因为broker0目前是leader，所以我们要将其kill # ps -ef | grep server.properties # kill -9 1177 现在再执行命令： # bin/kafka-topics.sh --describe --zookeeper localhost:9092 --topic my-replicated-topic 我们可以看到，leader节点已经变成了broker 2.要注意的是，在Isr中，已经没有了0号节点。leader的选举也是从ISR(in-sync replica)中进行的。 此时，我们依然可以 消费新消息 # bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic my test msg 1 my test msg 2 查看主题分区对应的leader信息： 需要通过zk 客户端查看：","link":"/2020/07/25/kafka-%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"title":"Redis5 集群搭建","text":"1、 环境信息centos7 redis5 2、整体集群信息# 以直接在一台机器上实现上述的伪集群，因为端口号特意设置为不同的。 # 重点：不论机器多少，对于部署过程都是一样的，只不过是在不同机器启动redis-server而已 192.168.100.242 (6381- 6386共6个端口） # 注意事项：如果你的服务器有多个IP，那你操作下面步骤时，尽量使用你的客户端能够访问的IP 3、每台服务器上面都要下载安装wget http://download.redis.io/releases/redis-5.0.3.tar.gz tar -zxvf redis-5.0.3.tar.gz cd redis-5.0.3 make # 安装到 /usr/local/redis 目录中 安装的文件只有一个bin目录 make install PREFIX=/usr/local/redis/ # 创建配置文件和data存放目录 mkdir /usr/local/redis/conf /usr/local/redis/data 4、准备6个redis.conf配置文件（为了方便学习，redis.conf根据不同端口来命名，方便一台机器上构建伪集群）# 配置文件进行了精简，完整配置可自行和官方提供的完整conf文件进行对照。端口号自行对应修改 #后台启动的意思 daemonize yes #端口号 port 6381 # IP绑定，redis不建议对公网开放，直接绑定0.0.0.0没毛病 bind 0.0.0.0 # redis数据文件存放的目录 dir /usr/local/redis/data # 开启AOF appendonly yes # 开启集群 cluster-enabled yes # 会自动生成在上面配置的dir目录下 cluster-config-file nodes-6381.conf cluster-node-timeout 5000 # 这个文件会自动生成 pidfile /var/run/redis_6381.pid 5、启动6个Redis实例# 一定要注意每个配置文件中的端口号哦 /usr/local/redis/bin/redis-server /usr/local/redis/conf/6381.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6382.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6383.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6384.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6385.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/6386.conf 6、 创建cluster# 5.0版本的方式 /usr/local/redis/bin/redis-cli --cluster create 192.168.100.242:6381 192.168.100.242:6382 \\ 192.168.100.242:6383 192.168.100.242:6384 192.168.100.242:6385 192.168.100.242:6386 \\ --cluster-replicas 1 # 自动设置主从，而且会提示你，是否运行使用自动的配置 Can I set the above configuration? (type 'yes' to accept): yes # 执行后的信息 >>> Performing hash slots allocation on 6 nodes... Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 192.168.100.242:6384 to 192.168.100.242:6381 Adding replica 192.168.100.242:6385 to 192.168.100.242:6382 Adding replica 192.168.100.242:6386 to 192.168.100.242:6383 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: 68326caa0238cb877afc3e6df23eb92558fcbc3c 192.168.100.242:6381 slots:[0-5460] (5461 slots) master M: 764500b86fadebd535ac2b5b778a73486fe7d2b7 192.168.100.242:6382 slots:[5461-10922] (5462 slots) master M: 93293699b8966ccc202bb29c659a9f60e26e4c86 192.168.100.242:6383 slots:[10923-16383] (5461 slots) master S: a842c5188c441453fd303520424132d45914fe5b 192.168.100.242:6384 replicates 764500b86fadebd535ac2b5b778a73486fe7d2b7 S: 0a7061773b2512c91b6173bc27451b19fe02f269 192.168.100.242:6385 replicates 93293699b8966ccc202bb29c659a9f60e26e4c86 S: 42806ad3f740f639ec321b78ea492c42a4040176 192.168.100.242:6386 replicates 68326caa0238cb877afc3e6df23eb92558fcbc3c Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join ... >>> Performing Cluster Check (using node 192.168.100.242:6381) M: 68326caa0238cb877afc3e6df23eb92558fcbc3c 192.168.100.242:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: 0a7061773b2512c91b6173bc27451b19fe02f269 192.168.100.242:6385 slots: (0 slots) slave replicates 93293699b8966ccc202bb29c659a9f60e26e4c86 S: a842c5188c441453fd303520424132d45914fe5b 192.168.100.242:6384 slots: (0 slots) slave replicates 764500b86fadebd535ac2b5b778a73486fe7d2b7 M: 93293699b8966ccc202bb29c659a9f60e26e4c86 192.168.100.242:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) M: 764500b86fadebd535ac2b5b778a73486fe7d2b7 192.168.100.242:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: 42806ad3f740f639ec321b78ea492c42a4040176 192.168.100.242:6386 slots: (0 slots) slave replicates 68326caa0238cb877afc3e6df23eb92558fcbc3c [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 7、 集群检验和测试# 检查集群，查看所有节点信息 /usr/local/redis/bin/redis-cli -c -h 192.168.100.242 -p 6381 cluster nodes # 执行后的信息 [root@node3 redis]# /usr/local/redis/bin/redis-cli -c -h 192.168.100.242 -p 6381 cluster nodes 0a7061773b2512c91b6173bc27451b19fe02f269 192.168.100.242:6385@16385 slave 93293699b8966ccc202bb29c659a9f60e26e4c86 0 1550635081000 5 connected a842c5188c441453fd303520424132d45914fe5b 192.168.100.242:6384@16384 slave 764500b86fadebd535ac2b5b778a73486fe7d2b7 0 1550635081565 4 connected 93293699b8966ccc202bb29c659a9f60e26e4c86 192.168.100.242:6383@16383 master - 0 1550635081665 3 connected 10923-16383 764500b86fadebd535ac2b5b778a73486fe7d2b7 192.168.100.242:6382@16382 master - 0 1550635081000 2 connected 5461-10922 68326caa0238cb877afc3e6df23eb92558fcbc3c 192.168.100.242:6381@16381 myself,master - 0 1550635080000 1 connected 0-5460 42806ad3f740f639ec321b78ea492c42a4040176 192.168.100.242:6386@16386 slave 68326caa0238cb877afc3e6df23eb92558fcbc3c 0 1550635080664 6 connected # 节点id ip+端口 角色 masterid 处理的ping数量 最后一个pong时间 节点配置版本 节点连接状态 slot槽分配情况 # 测试Redis Cluster的一种简单方法是使用redis-cli命令行实用程序 # -c 是支持cluster重定向 [root@node3 redis]# /usr/local/redis/bin/redis-cli -c -h 192.168.100.242 -p 6381 192.168.100.242:6381> set a 1 -> Redirected to slot [15495] located at 192.168.100.242:6383 OK 192.168.100.242:6383> get a \"1\" 192.168.100.242:6383> set hello tony -> Redirected to slot [866] located at 192.168.100.242:6381 OK 192.168.100.242:6381> get hello \"tony\" 192.168.100.242:6381> get a -> Redirected to slot [15495] located at 192.168.100.242:6383 \"1\" # 查看一个key属于哪一个节点 CLUSTER KEYSLOT key 8、集群slot数量整理 reshard# /usr/local/redis/bin/redis-cli --cluster help 可以查看所有这个命令和子命令的帮助信息 # 默认是master平均分了0-16383的所有虚拟slot # 可以进行调整，部分节点放多一点slot(槽或者位置)。 /usr/local/redis/bin/redis-cli --cluster reshard &lt;host>:&lt;port> --cluster-from &lt;node-id> --cluster-to &lt;node-id> --cluster-slots &lt;number of slots> --cluster-yes # 重新检查集群 [root@node3 redis]# /usr/local/redis/bin/redis-cli --cluster check 192.168.100.242:6382 192.168.100.242:6382 (764500b8...) -> 0 keys | 5462 slots | 1 slaves. 192.168.100.242:6383 (93293699...) -> 1 keys | 5461 slots | 1 slaves. 192.168.100.242:6381 (68326caa...) -> 1 keys | 5461 slots | 1 slaves. [OK] 2 keys in 3 masters. 0.00 keys per slot on average. >>> Performing Cluster Check (using node 192.168.100.242:6382) M: 764500b86fadebd535ac2b5b778a73486fe7d2b7 192.168.100.242:6382 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: a842c5188c441453fd303520424132d45914fe5b 192.168.100.242:6384 slots: (0 slots) slave replicates 764500b86fadebd535ac2b5b778a73486fe7d2b7 M: 93293699b8966ccc202bb29c659a9f60e26e4c86 192.168.100.242:6383 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 42806ad3f740f639ec321b78ea492c42a4040176 192.168.100.242:6386 slots: (0 slots) slave replicates 68326caa0238cb877afc3e6df23eb92558fcbc3c S: 0a7061773b2512c91b6173bc27451b19fe02f269 192.168.100.242:6385 slots: (0 slots) slave replicates 93293699b8966ccc202bb29c659a9f60e26e4c86 M: 68326caa0238cb877afc3e6df23eb92558fcbc3c 192.168.100.242:6381 slots:[0-5460] (5461 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 9、 测试自动故障转移# cluster集群不保证数据一致，数据也可能丢失 # 首先是运行客户端不断的写入或读取数据，以便能够发现问题 # 然后是模拟节点故障：找一个主节点关闭，主从故障切换的过程中，这个时间端的操作，客户端而言，只能是失败 # 官方描述 https://redis.io/topics/cluster-spec There is always a window of time when it is possible to lose writes during partitions. 分区的时间窗口内总是有可能丢失写操作。 10、手动故障转移# 可能某个节点需要维护（机器下线、硬件升级、系统版本调整等等场景），需要手动的实现转移 # 在slave节点上执行命令 CLUSTER FAILOVER # 注：CLUSTER help 可以看到帮助文档和简介。 相对安全的做法 11、扩容# 1、 启动新节点 /usr/local/redis/bin/redis-server /usr/local/redis/conf/6387.conf # 2、 加入到已经存在的集群作为master /usr/local/redis/bin/redis-cli --cluster add-node 192.168.100.242:6387 192.168.100.242:6382 # 本质就是发送一个新节点通过 CLUSTER MEET命令加入集群 # 新节点没有分配hash槽 # 3、 加入到已经存在的集群作为slave /usr/local/redis/bin/redis-cli --cluster add-node 192.168.100.242:7006 192.168.100.242:7000 --cluster-slave # 可以手工指定master，否则就是选择一个slave数量较少的master /usr/local/redis/bin/redis-cli --cluster add-node 192.168.100.242:7006 192.168.100.242:7000 --cluster-slave --cluster-master-id &lt;node-id> # 还可以将空master，转换为slave cluster replicate &lt;master-node-id> # 4、 检查集群 /usr/local/redis/bin/redis-cli --cluster check 192.168.100.242:6382 12、缩容（删除节点）# 注意：删除master的时候要把数据清空或者分配给其他主节点 /usr/local/redis/bin/redis-cli --cluster del-node 192.168.100.242:6381 &lt;node-id> 13、关心的问题# 1、 增加了slot槽的计算，是不是比单机性能差？ 共16384个槽，slots槽计算方式公开的，java客户端中就使用了：HASH_SLOT = CRC16(key) mod 16384 为了避免每次都需要服务器计算重定向，优秀的java客户端都实现了本地计算，和服务器slots分配进行映射，有变动时再更新本地内容。 # 2、 redis集群大小 理论是可以做到16384个槽，但是redis官方建议是最大1000个实例 # 3、 批量操作或者 # 4、cluster meet命令中的bus-port是什么？ MEET &lt;ip> &lt;port> [bus-port] 每个Redis群集节点都有一个额外的TCP端口，用于接收来自其他Redis群集节点的传入连接 # 5、集群节点间的通信方式 每个节点使用TCP连接与每个其他节点连接。 # 6、ask和moved重定向的区别 重定向包括两种情况 如果是确定slot不属于当前节点，redis会返回moved 如果当前redis节点正在处理slot迁移，则代表此处请求对应的key暂时不在此节点，返回ask，告诉客户端本次请求重定向 # 7、数据倾斜和访问倾斜的问题 解决办法 调整key的策略 + slot迁移 迁移过程如下，完整的迁移流程： 在迁移目的节点执行cluster setslot &lt;slot> IMPORTING &lt;node ID>命令，指明需要迁移的slot和迁移源节点。 在迁移源节点执行cluster setslot &lt;slot> MIGRATING &lt;node ID>命令，指明需要迁移的slot和迁移目的节点。 在迁移源节点执行cluster getkeysinslot获取该slot的key列表。 在迁移源节点执行对每个key执行migrate命令，该命令会同步把该key迁移到目的节点。 在迁移源节点反复执行cluster getkeysinslot命令，直到该slot的列表为空。 在迁移源节点和目的节点执行cluster setslot &lt;slot> NODE &lt;node ID>，完成迁移操作。","link":"/2020/02/10/redis5%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"},{"title":"kafka 概念和原理详解","text":"1. 简介Kafka是一种高吞吐量的分布式发布订阅消息系统，使用Scala编写。对于熟悉JMS（Java Message Service）规范的同学来说，消息系统已经不是什么新概念了(例如ActiveMQ，RabbitMQ等)。 Kafka 拥有作为一个消息系统应该具备的功能，但是确有着独特的设计。可以这样来说，Kafka 借鉴了JMS规范的思想，但是确并没有完全遵循JMS规范。 kafka是一个分布式的，分区的消息(官方称之为commit log)服务。它提供一个消息系统应该具备的功能，但是确有着独特的设计。 1.1 首先，让我们来看一下基础的消息(Message)相关术语 Topic: Kafka按照Topic分类来维护消息 Producer： 我们将发布(publish)消息到Topic的进程称之为生产者(producer) Consumer： 我们将订阅(subscribe)Topic并且处理Topic中消息的进程称之为消费者(consumer) Broker：Kafka以集群的方式运行，集群中的每一台服务器称之为一个代理(broker)。 因此，从一个较高的层面上来看，producers通过网络发送消息到Kafka集群，然后consumers来进行消费， 服务端(brokers)和客户端(producer、consumer)之间通信通过TCP协议来完成。Kafka提供了一个Java客户端，但是也可以使用其他语言编写的客户端。 1.1.1 Topic和Log让我们首先深入理解Kafka提出一个高层次的抽象概念-Topic。可以理解Topic是一个类别的名称，所有的message发送到Topic下面。对于每一个Topic，kafka集群按照如下方式维护一个分区(Partition,可以就理解为一个队列Queue)日志文件: partition是一个有序的message序列，这些message按顺序添加到一个叫做commit log的文件中。每个partition中的消息都有一个唯一的编号，称之为offset，用来唯一标示某个分区中的message。 提示：每个 partition ，都对应一个commit-log。一个 partition 中的message的offset都是唯一的，但是不同的partition中的message的offset可能是相同的。 kafka集群，在配置的时间范围内，维护所有的由producer生成的消息，而不管这些消息有没有被消费。例如日志保留( log retention )时间被设置为2天。kafka会维护最近2天生产的所有消息，而2天前的消息会被丢弃。==kafka 的性能与保留的数据量的大小没有关系==，因此保存大量的数据(日志信息)不会有什么影响。 关于 consumer ，每个consumer是基于自己在commit log 中的消费进度(offset)来进行工作的。在kafka中，offset由consumer来维护：一般情况下我们按照顺序逐条消费commit log中的消息，当然我可以通过指定offset来重复消费某些消息，或者跳过某些消息。 这意味 kafka 中的 consumer 对集群的影响是非常小的，添加一个或者减少一个consumer，对于集群或者其他consumer来说，都是没有影响的，因此==每个consumer维护各自的offset==。 对log进行分区（partitioned）目的是什么？ 首先，当log文件大小超过系统文件系统的限制时，可以自动拆分。每个partition对应的log都受到所在机器的文件系统大小的限制，但是一个Topic中是可以有很多分区的，因此可以处理任意数量的数据。 另一个方面，是为了提高并行度。 1.1.3 Distributionlog的partitions分布在kafka集群中不同的broker上，每个broker可以请求备份其他broker上partition上的数据。kafka集群支持配置一个partition备份的数量。 针对每个partition，都有一个broker起到“leader”的作用，0个多个其他的broker作为“follwers”的作用。leader处理所有的针对这个partition的读写请求，而==followers被动复制leader的结果==。如果这个leader失效了，其中的一个follower将会自动的变成新的leader。==每个broker都是自己所管理的partition的leader，同时又是其他broker所管理partitions的followers，kafka通过这种方式来达到负载均衡。== 1.1.3 Producers生产者将消息发送到topic中去，同时负责选择将message发送到topic的哪一个partition中。通过round-robin（轮询）做简单的负载均衡。也可以根据消息中的某一个关键字来进行区分，比如指定定key 通过 hash 取模。通常第二种方式使用的更多。 1.1.4 Consumers 和 Consumer Group传统的消息传递模式有2种：队列( queuing )和（ publish-subscribe）。在queuing模式中，多个consumer从服务器中读取数据，消息只会到达一个consumer。在 publish-subscribe 模型中，消息会被广播给所有的consumer。==Kafka基于这2种模式提供了一种consumer的抽象概念==：==consumer group==。 每个consumer都要标记自己属于哪一个 consumer group。发布到 topic 中的 message 中 message 会被传递到 consumer group 中的一个 consumer 实例。consumer实例可以运行在不同的进程上，也可以在不同的物理机器上。如果所有的consumer都位于同一个consumer group 下，这就类似于传统的queue模式，并在众多的consumer instance之间进行负载均衡。如果所有的consumer都有着自己唯一的consumer group，这就类似于传统的publish-subscribe模型。 更一般的情况是，通常一个topic会有几个consumer group，每个consumer group都是一个逻辑上的订阅者（ logical subscriber ）。每个consumer group由多个consumer instance 组成，从而达到可扩展和容灾的功能。这并没有什么特殊的地方，仅仅是将publish-subscribe模型中的运行在单个进程上的consumers中的consumer替换成一个consumer group。如下图所示： 说明：由2个broker组成的kafka集群，总共有4个 Parition (P0-P3)。这个集群由2个Consumer Group， A有2个 consumer instances ，而B有四个.consumer instances的个数不能大于对应topic 的 Parition 个数。 1.2 消费顺序Kafka 比传统的消息系统有着更强的顺序保证。 在传统的情况下：服务器按照顺序保留消息到队列，如果有多个consumer来消费队列中的消息，服务器 会接受消息的顺序向外提供消息。但是，尽管服务器是按照顺序提供消息，但是消息传递到每一个consumer是异步的，这可能会导致先消费的 consumer获取到消息时间可能比后消费的consumer获取到消息的时间长，导致不能保证顺序性。这表明，当进行并行的消费的时候，消息在多个 consumer之间可能会失去顺序性。消息系统通常会采取一种“ exclusive consumer”的概念，来确保同一时间内只有一个consumer能够从队列中进行消费，但是这实际上意味着在消息处理的过程中是不支持并行的。 Kafka：在这方面做的更好。通过Topic中并行度的概念，即partition，Kafka可以同时提供顺序性保证和多个consumer同时消费时的负载均衡。实现的原理是通过将一个topic中的partition分配给一个consumer group中的不同consumer instance。通过这种方式，我们可以保证一个partition在同一个时刻只有一个consumer instance在消息，从而保证顺序。虽然一个topic中有多个partition，但是一个consumer group中同时也有多个consumer instance，通过合理的分配依然能够保证负载均衡。需要注意的是，一个consumer group中的consumer instance的数量不能比一个Topic中的partition的数量多。 Kafka只在 partition 的范围内保证消息消费的局部顺序性，不能在同一个topic中的多个partition中保证总的消费顺序性。 通常来说，这已经可以满足大部分应用的需求。但是，如果的确有在总体上保证消费的顺序的需求的话，那么我们可以通过将 topic 的 partition 数量设置为1，将 consumer group 中的 consumer instance 数量也设置为1. 1.3 Guarantees（担保）从较高的层面上来说的话，Kafka提供了以下的保证： 发送顺序保证: 发送到一个Topic中的message会按照发送的顺序添加到commit log 中。意思是，如果消息 M1，M2 由同一个producer发送，M1 比 M2 发送的早的话，那么在 commit log 中，M1的 offset一定比commit M2的 offset 小。 消息容灾保证: 一个consumer在commit log中可以按照发送顺序来消费message如果一个 topic 的备份因子( replication factor )设置为N，那么Kafka可以容忍N-1个服务器的失败，而存储在commit log中的消息不会丢失。 2. Kafka设计原理分析2.1 kafka 拓扑结构 2.2 kafka zoopeeper 节点结构 topics 节点为永久节点类型（防止数据丢失）。 ids 为临时节点（节点挂掉，就需要删除id） 2.3 producer 发布消息2.3.1 写入方式producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。 2.3.2 消息路由producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为： 指定了 patition，则直接使用； 未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition patition 和 key 都未指定，使用轮询选出一个 patition。3、写入流程 producer 先从 zookeeper 的 “/brokers/…/state” 节点找到该 partition 的 leader producer 将消息发送给该 leader leader 将消息写入本地 log followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK 2.4 kafka分区leader选举原理 选举原理：是根据 zookeeper 的锁机制来实现，同一时间创建临时节点写入参选信息，只有一个flower 能写入成功，写入成功的即为 leader。","link":"/2020/07/22/kafka-%E6%A6%82%E5%BF%B5%E5%92%8C%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"title":"Redis哨兵高可用搭建","text":"Redis下载安装 下载redis https://redis.io/download # 下载 wget http://download.redis.io/releases/redis-5.0.3.tar.gz # Installation tar xzf redis-5.0.3.tar.gz cd redis-5.0.3 make # 创建文件夹 mkdir /usr/local/redis/conf mkdir /usr/local/redis/data mkdir /usr/local/redis/logs # run src/redis-server # warning 1 > 提示修改 linux内核参数 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. echo 1024 >/proc/sys/net/core/somaxconn # warn 2 > 提示如下 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. echo \"vm.overcommit_memory = 1\" >> /etc/sysctl.conf sysctl vm.overcommit_memory=1 # warning 3 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled echo never > /sys/kernel/mm/transparent_hugepage/enabled # 云服务器要注意ip要写对，端口要开放 # 虚拟机要注意防火墙要关闭 systemctl stop firewalld.service 准备配置文件# 配置文件进行了精简，完整配置可自行和官方提供的完整conf文件进行对照。端口号自行对应修改 #后台启动的意思 daemonize yes #端口号(如果同一台服务器上启动，注意要修改为不同的端口) port 6380 # IP绑定，redis不建议对公网开放，直接绑定0.0.0.0没毛病 bind 0.0.0.0 # 这个文件会自动生成(如果同一台服务器上启动，注意要修改为不同的端口) pidfile /var/run/redis_6380.pid 准备三个redis服务# 1、启动三个Redis /usr/local/redis/bin/redis-server /usr/local/redis/conf/redis-6380.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/redis-6381.conf /usr/local/redis/bin/redis-server /usr/local/redis/conf/redis-6382.conf # 2、配置为 1主2从 /usr/local/redis/bin/redis-cli -p 6381 slaveof 192.168.100.241 6380 /usr/local/redis/bin/redis-cli -p 6382 slaveof 192.168.100.241 6380 # 3、检查集群 /usr/local/redis/bin/redis-cli -p 6380 info Replication 准备哨兵配置文件# 配置文件：sentinel.conf，在sentinel运行期间是会被动态修改的 # sentinel如果重启时，根据这个配置来恢复其之前所监控的redis集群的状态 # 绑定IP bind 0.0.0.0 # 后台运行 daemonize yes # 默认yes，没指定密码或者指定IP的情况下，外网无法访问 protected-mode no # 哨兵的端口，客户端通过这个端口来发现redis port 26380 # 哨兵自己的IP，手动设定也可自动发现，用于与其他哨兵通信 # sentinel announce-ip # 临时文件夹 dir /tmp # 日志 logfile \"/usr/local/redis/logs/sentinel-26380.log\" # sentinel监控的master的名字叫做mymaster,初始地址为 192.168.100.241 6380,2代表两个及以上哨兵认定为死亡，才认为是真的死亡 sentinel monitor mymaster 192.168.100.241 6380 2 # 发送心跳PING来确认master是否存活 # 如果master在“一定时间范围”内不回应PONG 或者是回复了一个错误消息，那么这个sentinel会主观地(单方面地)认为这个master已经不可用了 sentinel down-after-milliseconds mymaster 1000 # 如果在该时间（ms）内未能完成failover操作，则认为该failover失败 sentinel failover-timeout mymaster 3000 # 指定了在执行故障转移时，最多可以有多少个从Redis实例在同步新的主实例，在从Redis实例较多的情况下这个数字越小，同步的时间越长，完成故障转移所需的时间就越长 sentinel parallel-syncs mymaster 1 启动哨兵集群/usr/local/redis/bin/redis-server /usr/local/redis/conf/sentinel-26380.conf --sentinel /usr/local/redis/bin/redis-server /usr/local/redis/conf/sentinel-26381.conf --sentinel /usr/local/redis/bin/redis-server /usr/local/redis/conf/sentinel-26382.conf --sentinel 测试# 停掉master，主从切换过程 启动哨兵(客户端通过哨兵发现Redis实例信息) 哨兵通过连接master发现主从集群内的所有实例信息 哨兵监控redis实例的健康状况 哨兵一旦发现master不能正常提供服务，则通知给其他哨兵 当一定数量的哨兵都认为master挂了 选举一个哨兵作为故障转移的执行者 执行者在slave中选取一个作为新的master 将其他slave重新设定为新master的从属 哨兵同步pubsub机制发出来的消息# https://redis.io/topics/sentinel#pubsub-messages +reset-master &lt;instance details> -- 当master被重置时. +slave &lt;instance details> -- 当检测到一个slave并添加进slave列表时. +failover-state-reconf-slaves &lt;instance details> -- Failover状态变为reconf-slaves状态时 +failover-detected &lt;instance details> -- 当failover发生时 +slave-reconf-sent &lt;instance details> -- sentinel发送SLAVEOF命令把它重新配置时 +slave-reconf-inprog &lt;instance details> -- slave被重新配置为另外一个master的slave，但数据复制还未发生时。 +slave-reconf-done &lt;instance details> -- slave被重新配置为另外一个master的slave并且数据复制已经与master同步时。 -dup-sentinel &lt;instance details> -- 删除指定master上的冗余sentinel时 (当一个sentinel重新启动时，可能会发生这个事件). +sentinel &lt;instance details> -- 当master增加了一个sentinel时。 +sdown &lt;instance details> -- 进入SDOWN状态时; -sdown &lt;instance details> -- 离开SDOWN状态时。 +odown &lt;instance details> -- 进入ODOWN状态时。 -odown &lt;instance details> -- 离开ODOWN状态时。 +new-epoch &lt;instance details> -- 当前配置版本被更新时。 +try-failover &lt;instance details> -- 达到failover条件，正等待其他sentinel的选举。 +elected-leader &lt;instance details> -- 被选举为去执行failover的时候。 +failover-state-select-slave &lt;instance details> -- 开始要选择一个slave当选新master时。 +no-good-slave &lt;instance details> -- 没有合适的slave来担当新master +selected-slave &lt;instance details> -- 找到了一个适合的slave来担当新master +promoted-slave -- 确认成功 +failover-state-reconf-slaves -- 开始对slaves进行reconfig操作 +slave-reconf-sent -- 向指定的slave发送“slaveof”指令，告知此slave跟随新的master +slave-reconf-inprog -- 此slave正在执行slaveof + SYNC过程，slave收到“+slave-reconf-sent”之后将会执行slaveof操作 +slave-reconf-done -- 此slave同步完成，此后leader可以继续下一个slave的reconfig操作 failover-state-send-slaveof-noone &lt;instance details> -- 当把选择为新master的slave的身份进行切换的时候。 failover-end-for-timeout &lt;instance details> -- failover由于超时而失败时。 failover-end &lt;instance details> -- failover成功完成,故障转移结束 switch-master &lt;master name> &lt;oldip> &lt;oldport> &lt;newip> &lt;newport> -- 当master的地址发生变化时。通常这是客户端最感兴趣的消息了。 +tilt -- 进入Tilt模式。 -tilt -- 退出Tilt模式。 哨兵日志分析sdow (sdown subjectively down) odown (objectively down) # 通过日志逐步分析 996:X 23 Nov 01:00:30.020 # +sdown master mymaster 60.205.209.106 6381 996:X 23 Nov 01:00:30.143 # +new-epoch 4 996:X 23 Nov 01:00:30.144 # +vote-for-leader 699538b978f33f677c8be471eed344b3933eca8c 4 996:X 23 Nov 01:00:31.111 # +odown master mymaster 60.205.209.106 6381 #quorum 3/2 996:X 23 Nov 01:00:31.111 # Next failover delay: I will not start a failover before Thu Nov 23 01:00:36 2017 996:X 23 Nov 01:00:31.200 # +config-update-from sentinel 699538b978f33f677c8be471eed344b3933eca8c 172.17.171.34 26381 @ mymaster 60.205.209.106 6381 996:X 23 Nov 01:00:31.200 # +switch-master mymaster 60.205.209.106 6381 60.205.209.106 6380 996:X 23 Nov 01:00:31.200 * +slave slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6380 996:X 23 Nov 01:00:31.200 * +slave slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 996:X 23 Nov 01:00:32.233 # +sdown slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 1073:X 23 Nov 01:00:30.087 # +sdown master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.139 # +odown master mymaster 60.205.209.106 6381 #quorum 3/2 1073:X 23 Nov 01:00:30.139 # +new-epoch 4 1073:X 23 Nov 01:00:30.139 # +try-failover master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.141 # +vote-for-leader 699538b978f33f677c8be471eed344b3933eca8c 4 1073:X 23 Nov 01:00:30.142 # 8412b6b2ac39a3d36c171590cd23cbe025517c15 voted for 8412b6b2ac39a3d36c171590cd23cbe025517c15 4 1073:X 23 Nov 01:00:30.144 # f8c7e052744926747ef1f31c27da4721fde3faf4 voted for 699538b978f33f677c8be471eed344b3933eca8c 4 1073:X 23 Nov 01:00:30.232 # +elected-leader master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.232 # +failover-state-select-slave master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.294 # +selected-slave slave 60.205.209.106:6380 60.205.209.106 6380 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.294 * +failover-state-send-slaveof-noone slave 60.205.209.106:6380 60.205.209.106 6380 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:30.356 * +failover-state-wait-promotion slave 60.205.209.106:6380 60.205.209.106 6380 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:31.153 # +promoted-slave slave 60.205.209.106:6380 60.205.209.106 6380 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:31.153 # +failover-state-reconf-slaves master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:31.200 * +slave-reconf-sent slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:32.149 * +slave-reconf-inprog slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:32.149 * +slave-reconf-done slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:32.220 # -odown master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:32.220 # +failover-end master mymaster 60.205.209.106 6381 1073:X 23 Nov 01:00:32.220 # +switch-master mymaster 60.205.209.106 6381 60.205.209.106 6380 1073:X 23 Nov 01:00:32.220 * +slave slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6380 1073:X 23 Nov 01:00:32.220 * +slave slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 1073:X 23 Nov 01:00:33.227 # +sdown slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 1009:X 23 Nov 01:00:30.039 # +sdown master mymaster 60.205.209.106 6381 1009:X 23 Nov 01:00:30.139 # +odown master mymaster 60.205.209.106 6381 #quorum 2/2 1009:X 23 Nov 01:00:30.139 # +new-epoch 4 1009:X 23 Nov 01:00:30.139 # +try-failover master mymaster 60.205.209.106 6381 1009:X 23 Nov 01:00:30.142 # +vote-for-leader 8412b6b2ac39a3d36c171590cd23cbe025517c15 4 1009:X 23 Nov 01:00:30.142 # 699538b978f33f677c8be471eed344b3933eca8c voted for 699538b978f33f677c8be471eed344b3933eca8c 4 1009:X 23 Nov 01:00:30.144 # f8c7e052744926747ef1f31c27da4721fde3faf4 voted for 699538b978f33f677c8be471eed344b3933eca8c 4 1009:X 23 Nov 01:00:31.200 # +config-update-from sentinel 699538b978f33f677c8be471eed344b3933eca8c 172.17.171.34 26381 @ mymaster 60.205.209.106 6381 1009:X 23 Nov 01:00:31.200 # +switch-master mymaster 60.205.209.106 6381 60.205.209.106 6380 1009:X 23 Nov 01:00:31.200 * +slave slave 60.205.209.106:6382 60.205.209.106 6382 @ mymaster 60.205.209.106 6380 1009:X 23 Nov 01:00:31.200 * +slave slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 1009:X 23 Nov 01:00:32.258 # +sdown slave 60.205.209.106:6381 60.205.209.106 6381 @ mymaster 60.205.209.106 6380 数据一致性的处理办法之一 # 这些配置仅当redis为master时才有效 # 当master不符合这些条件时，它将停止对外的服务。这种场景主要是用于master在网络上被孤立了。 min-slaves-to-write 1 min-slaves-max-lag 10","link":"/2020/02/06/redis%E5%93%A8%E5%85%B5%E9%AB%98%E5%8F%AF%E7%94%A8%E6%90%AD%E5%BB%BA/"},{"title":"关于大型系统架构的10个问题","text":"下面这些问题都是一线大厂的真实面试问题，不论是对你面试还是说拓宽知识面都很有帮助。 1. 你使用过哪些组件或者方法来提升网站性能,可用性以及并发量 提高硬件能力、增加系统服务器。（当服务器增加到某个程度的时候系统所能提供的并发访问量几乎不变，所以不能根本解决问题） 使用缓存（本地缓存：本地可以使用JDK自带的 Map、Guava Cache.分布式缓存：Redis、Memcache.本地缓存不适用于提高系统并发量，一般是用处用在程序中。比如Spring是如何实现单例的呢？大家如果看过源码的话，应该知道，S把已经初始过的变量放在一个Map中，下次再要使用这个变量的时候，先判断Map中有没有，这也就是系统中常见的单例模式的实现。） 消息队列 （解耦+削峰+异步） 采用分布式开发 （不同的服务部署在不同的机器节点上，并且一个服务也可以部署在多台机器上，然后利用 Nginx 负载均衡访问。这样就解决了单点部署(All In)的缺点，大大提高的系统并发量） 数据库分库（读写分离）、分表（水平分表、垂直分表） 采用集群 （多台机器提供相同的服务） CDN 加速 (将一些静态资源比如图片、视频等等缓存到离用户最近的网络节点) 浏览器缓存 使用合适的连接池（数据库连接池、线程池等等） 适当使用多线程进行开发。 2. 设计高可用系统的常用手段 降级： 服务降级是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。降级往往会指定不同的级别，面临不同的异常等级执行不同的处理。根据服务方式：可以拒接服务，可以延迟服务，也有时候可以随机服务。根据服务范围：可以砍掉某个功能，也可以砍掉某些模块。总之服务降级需要根据不同的业务需求采用不同的降级策略。主要的目的就是服务虽然有损但是总比没有好； 限流： 防止恶意请求流量、恶意攻击，或者防止流量超出系统峰值； 缓存： 避免大量请求直接落到数据库，将数据库击垮； 超时和重试机制： 避免请求堆积造成雪崩； 回滚机制： 快速修复错误版本。 3. 现代互联网应用系统通常具有哪些特点? 高并发，大流量； 高可用：系统7×24小时不间断服务； 海量数据：需要存储、管理海量数据，需要使用大量服务器； 用户分布广泛，网络情况复杂：许多大型互联网都是为全球用户提供服务的，用户分布范围广，各地网络情况千差万别； 安全环境恶劣：由于互联网的开放性，使得互联网更容易受到攻击，大型网站几乎每天都会被黑客攻击； 需求快速变更，发布频繁：和传统软件的版本发布频率不同，互联网产品为快速适应市场，满足用户需求，其产品发布频率是极高的； 渐进式发展：与传统软件产品或企业应用系统一开始就规划好全部的功能和非功能需求不同，几乎所有的大型互联网网站都是从一个小网站开始，渐进地发展起来。 4. 谈谈你对微服务领域的了解和认识现在大公司都在用并且未来的趋势都是 Spring Cloud，而阿里开源的 Spring Cloud Alibaba 也是 Spring Cloud 规范的实现 。 我们通常把 Spring Cloud 理解为一系列开源组件的集合，但是 Spring Cloud并不是等同于 Spring Cloud Netflix 的 Ribbon、Feign、Eureka（停止更新）、Hystrix 这一套组件，而是抽象了一套通用的开发模式。它的目的是通过抽象出这套通用的模式，让开发者更快更好地开发业务。但是这套开发模式运行时的实际载体，还是依赖于 RPC、网关、服务发现、配置管理、限流熔断、分布式链路跟踪等组件的具体实现。 Spring Cloud Alibaba 是官方认证的新一套 Spring Cloud 规范的实现,Spring Cloud Alibaba 是一套国产开源产品集合，后续还会有中文 reference 和一些原理分析文章，所以，这对于国内的开发者是非常棒的一件事。阿里的这一举动势必会推动国内微服务技术的发展，因为在没有 Spring Cloud Alibaba 之前，我们的第一选择是 Spring Cloud Netflix，但是它们的文档都是英文的，出问题后排查也比较困难， 在国内并不是有特别多的人精通。Spring Cloud Alibaba 由阿里开源组件和阿里云产品组件两部分组成，其致力于提供微服务一站式解决方案，方便开发者通过 Spring Cloud 编程模型轻松开发微服务应用。 另外，Apache Dubbo Ecosystem 是围绕 Apache Dubbo 打造的微服务生态，是经过生产验证的微服务的最佳实践组合。在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。阿里后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 5. 谈谈你对 Dubbo 和 Spring Cloud 的认识(两者关系)具体可以看公众号-阿里巴巴中间件的这篇文章:独家解读：Dubbo Ecosystem - 从微服务框架到微服务生态 Dubbo 与 Spring Cloud 并不是竞争关系，Dubbo 作为成熟的 RPC 框架，其易用性、扩展性和健壮性已得到业界的认可。未来 Dubbo 将会作为 Spring Cloud Alibaba 的 RPC 组件，并与 Spring Cloud 原生的 Feign 以及 RestTemplate 进行无缝整合，实现“零”成本迁移。 在阿里巴巴的微服务解决方案中，Dubbo、Nacos 和 Sentinel，以及后续将开源的微服务组件，都是 Dubbo EcoSystem 的一部分。我们后续也会将 Dubbo EcoSystem 集成到 Spring Cloud 的生态中。 6. 性能测试了解吗?说说你知道的性能测试工具?性能测试指通过自动化的测试工具模拟多种正常、峰值以及异常负载条件来对系统的各项性能指标进行测试。性能测试是总称，通常细分为： 基准测试： 在给系统施加较低压力时，查看系统的运行状况并记录相关数做为基础参考 负载测试： 是指对系统不断地增加压力或增加一定压力下的持续时间，直到系统的某项或多项性能指标达到安全临界值，例如某种资源已经达到饱和状态等 。此时继续加压，系统处理能力会下降。 压力测试： 超过安全负载情况下，不断施加压力（增加并发请求），直到系统崩溃或无法处理任何请求，依此获得系统最大压力承受能力。 稳定性测试： 被测试系统在特定硬件、软件、网络环境下，加载一定业务压力（模拟生产环境不同时间点、不均匀请求，呈波浪特性）运行一段较长时间，以此检测系统是否稳定。 后端程序员或者测试平常比较常用的测试工具是 JMeter（官网：https://jmeter.apache.org/）。Apache JMeter 是一款基于Java的压力测试工具(100％纯Java应用程序)，旨在加载测试功能行为和测量性能。它最初被设计用于 Web 应用测试但后来扩展到其他测试领域。 7. 对于一个单体应用系统,随着产品使用的用户越来越多,网站的流量会增加,最终单台服务器无法处理那么大的流量怎么办?这个时候就要考虑扩容了。《亿级流量网站架构核心技术》这本书上面介绍到我们可以考虑下面几步来解决这个问题： 第一步，可以考虑简单的扩容来解决问题。比如增加系统的服务器，提高硬件能力等等。 第二步，如果简单扩容搞不定，就需要水平拆分和垂直拆分数据／应用来提升系统的伸缩性，即通过扩容提升系统负载能力。 第三步，如果通过水平拆分／垂直拆分还是搞不定，那就需要根据现有系统特性，架构层面进行重构甚至是重新设计，即推倒重来。 对于系统设计，理想的情况下应支持线性扩容和弹性扩容，即在系统瓶颈时，只需要增加机器就可以解决系统瓶颈，如降低延迟提升吞吐量，从而实现扩容需求。 如果你想扩容，则支持水平/垂直伸缩是前提。在进行拆分时，一定要清楚知道自己的目的是什么，拆分后带来的问题如何解决，拆分后如果没有得到任何收益就不要为了拆而拆，即不要过度拆分，要适合自己的业务。 8. 大表优化的常见手段 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 垂直分区： 根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平分区： 保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以 水平拆分最好分库 。水平拆分能够 支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 9. 在系统中使用消息队列能带来什么好处?《大型网站技术架构》第四章和第七章均有提到消息队列对应用性能及扩展性的提升。 1) 通过异步处理提高系统性能如上图，在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 通过以上分析我们可以得出消息队列具有很好的削峰作用的功能——即通过异步处理，将短时间高并发产生的事务消息存储在消息队列中，从而削平高峰期的并发事务。 举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。 2) 降低系统耦合性我们知道模块分布式部署以后聚合方式通常有两种：1.分布式消息队列和2.分布式服务。 先来简单说一下分布式服务： 目前使用比较多的用来构建SOA（Service Oriented Architecture面向服务体系结构）的分布式服务框架是阿里巴巴开源的Dubbo.如果想深入了解Dubbo的可以看我写的关于Dubbo的这一篇文章：《高性能优秀的服务框架-dubbo介绍》：https://juejin.im/post/5acadeb1f265da2375072f9c 再来谈我们的分布式消息队列： 我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。 我们最常见的事件驱动架构类似生产者消费者模式，在大型网站中通常用利用消息队列实现事件驱动结构。如下图所示：消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。 消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。 另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。 备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的，比如在我们的ActiveMQ消息队列中还有点对点工作模式，具体的会在后面的文章给大家详细介绍，这一篇文章主要还是让大家对消息队列有一个更透彻的了解。 这个问题一般会在上一个问题问完之后，紧接着被问到。“使用消息队列会带来什么问题？”这个问题要引起重视，一般我们都会考虑使用消息队列会带来的好处而忽略它带来的问题！ 10. 说说自己对 CAP 定理,BASE 理论的了解CAP 定理在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistence） :所有节点访问同一份最新的数据副本 可用性（Availability）:每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 分区容错性（Partition tolerance） : 分布式系统在遇到某节点或网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务。 CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。 注意：不是所谓的3选2（不要被网上大多数文章误导了）: 大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。 当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。 我在网上找了很多文章想看一下有没有文章提到这个不是所谓的3选2，用百度半天没找到了一篇，用谷歌搜索找到一篇比较不错的，如果想深入学习一下CAP就看这篇文章把，我这里就不多BB了：《分布式系统之CAP理论》 ： http://www.cnblogs.com/hxsyl/p/4381980.html BASE 理论BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。 BASE理论的核心思想： 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。 BASE理论三要素： 基本可用： 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。 比如： ①响应时间上的损失:正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒；②系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面； 软状态： 软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时； 最终一致性： 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。 参考 《大型网站技术架构》 《亿级流量网站架构核心技术》 《Java工程师修炼之道》 https://www.cnblogs.com/puresoul/p/5456855.html","link":"/2020/08/21/%E5%85%B3%E4%BA%8E%E5%A4%A7%E5%9E%8B%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%9A%8410%E4%B8%AA%E9%97%AE%E9%A2%98/"},{"title":"多线程编程如何确定线程个数","text":"1. 多线程编程如何确定线程个数？在具体的优化场景中，可以分为以下两种场景着手。 CPU 密集型程序 I/O 密集型程序 1.1 CPU 密集型 单核CPU处理 CPU 密集型程序，这种情况并不太适合使用多线程。 如果是多核CPU 处理 CPU 密集型程序，我们完全可以最大化的利用 CPU 核心数，应用并发编程来提高效率 线程数量 = CPU 核数（逻辑） 就可以了，但是实际上，数量一般会设置为 CPU 核数（逻辑）+ 1, 这是因为：计算(CPU)密集型的线程恰好在某时因为发生一个页错误或者因其他原因而暂停，刚好有一个“额外”的线程，可以确保在这种情况下CPU周期不会中断工作。 1.2 I/O密集型 与 CPU 密集型程序相对，一个完整请求，CPU运算操作完成之后还有很多 I/O 操作要做，也就是说 I/O 操作占比很大部分 线程等待时间所占比例越高，需要越多线程；线程CPU时间所占比例越高，需要越少线程。 最佳线程数 = CPU核心数 * (1/CPU利用率) = CPU核心数 * (1 + (I/O耗时/CPU耗时)) 假如几乎全是 I/O耗时，所以纯理论你就可以说是 2N（N=CPU核数），当然也有说 2N + 1的。 1.3 问题 假设要求一个系统的 TPS（Transaction Per Second 或者 Task Per Second）至少为 20，然后假设每个Transaction由一个线程完成，继续假设平均每个线程处理一个Transaction的时间为4s 答案：因为 一个线程处理一个 Transaction 时间是4秒，那1秒 处理 0.25 个 Transaction； 所以 ： 20 / 0.25 = 80（个）。 但是，这是因为没有考虑到CPU数目。一般服务器的CPU核数为16或者32，如果有80个线程，那么肯定会带来太多不必要的线程上下文切换开销。 计算操作需要5ms，DB操作需要 100ms，对于一台 8个CPU的服务器，怎么设置线程数呢？ 线程数 = 8 * (1 + 100/5) = 168 (个) 那如果DB的 QPS（Query Per Second）上限是1000，此时这个线程数又该设置为多大呢？ 答案： 因为 1 s = 1000ms 完成一个完整的db 操作等于 100+5 =105 ms。 那一个线程一秒可以处理的任务数是 1000/105。168 个线程 就是 168 * 1000/105 = 1600（QPS）。 如果 db 的QPS 上限是 1000 ，则168*1000/1600 = 105 个。 1.4 增加 CPU 核数一定能解决问题吗即便我算出了理论线程数，但实际CPU核数不够，会带来线程上下文切换的开销，所以下一步就需要增加 CPU 核数，那我们盲目的增加 CPU 核数就一定能解决问题吗？ 假如我们的串行率是 5%，那么我们无论采用什么技术，最高也就只能提高 20 倍的性能。 所谓串行率： 临界区都是串行的，非临界区都是并行的，用单线程执行临界区的时间/用单线程执行(临界区+非临界区)的时间就是串行率 因为临界区的大小往往就是瓶颈问题的所在，所以尽可能的最小化临界区范围 2. 动态线程池所谓动态线程池，是动态设置线程池的核心线程数、最大线程数、有界队列。 2.1 动态设置核心线程数 corePoolSize 和 MaxmumPoolSizeThreadPoolExecutor.setCorePoolSize(); 并且 Spring 的 ThreadPoolTaskExecutor类 （就是对JDK ThreadPoolExecutor 的一层包装，可以理解为装饰者模式）也提供了的 setCorePoolSize ； ThreadPoolExecutor.setMaxmumPoolSize(); 调整的时候注意点：1. 调整核心线程数之后无效的情况这是因为，核心线程数大于最大线程数，在操作的过程中，有个增减操作，具体流程如下： 创建新的工作线程 worker，然后工作线程数进行加一操作。 运行创建的工作线程 worker，开始获取任务 task。 工作线程数量大于最大线程数，对工作线程数进行减一操作。 返回 null，即没有获取到 task。 清理该任务，流程结束。 针对这个情况，应该 setCorePoolSize和 setMaxmumPoolSize 一起调用。 2. 如果调整之后把活动线程数设置的值太大了，岂不是业务低峰期我们还需要人工把值调的小一点？当 allowCoreThreadTimeOut 参数设置为 true 的时候，核心线程在空闲了 keepAliveTime 的时间后也会被回收的，相当于线程池自动给你动态修改了。 3. 线程池被创建后里面有线程吗？如果没有的话，你知道有什么方法对线程池进行预热吗？线程池被创建后如果没有任务过来，里面是不会有线程的。如果需要预热的话可以调用下面的两个方法来进行预热： prestartAllCoreThreads(); prestartCoreThread(); 4. 核心线程数会被回收吗？需要什么设置？核心线程数默认是不会被回收的，如果需要回收核心线程数，需要调用下面的方法： allowCoreThreadTimeOut()// 该值默认为 false。 allowCoreThreadTimeOut 默认为false，设置true，会在过期时间之后回收 核心线程。 2.2 动态指定队列长度首先通过源码api，来看没有提供修改队列长度的方法。这是由于 ThreadPoolExecutor 内部的 capacity 变量被 final修饰。 由于这个因素，可以采用自定义队列的方式 可以把 LinkedBlockingQueue 粘贴一份出来，修改个名字，然后把 Capacity 参数的 final 修饰符去掉，并提供其对应的 get/set 方法。 然后在程序里面把原来的队列换掉新创建的自定义队列，达到动态设置队列长度的目的。","link":"/2020/06/05/%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9A%E7%BA%BF%E7%A8%8B%E4%B8%AA%E6%95%B0/"},{"title":"冰山模型","text":"一、冰山模型 动机可能就是天赋本身 知识和技能……… 能力知识技能属于特定领域，能力则是通用可迁移的。如果：沟通能力、学习能力、思考能力、交往能力。 价值观、性格、动机受基因、家庭教育、童年经历有关。后续改变相对较难。所以后续选择职业、婚姻或者其他的时候，最重要的是匹配，而不是强制改变。 工作跟伴侣是一样的，「匹配」比「优秀」更重要。 感觉一份工作不喜欢，有可能是： 缺乏知识技能，导致的慌乱和焦虑。 缺乏能力导致的挫败和低效。 价值观不匹配导致的矛盾和纠结。 动机不匹配导致的没热情。 性格不匹配导致的心累。 二、如何根据冰山模型确定职位一. 确定要分析的岗位。 二. 根据招聘网站搜索这个岗位的招聘要求。 三. 按照冰山模型，综合分析招聘要求。 四. 按照岗位需求模型与自己进行对比。 搞明白 「我应该选择什么养的工作」比「我应该如何在别人认为的好工作里面成功」更重要的多。 想要让自己获得成就感，就不应该把它绑在别人的记分牌上。 查理芒格在「穷查理宝典」中提到：掌握一定量的思维模型，能解决这世上90%的问题。 三、投入因素的不同整个冰山模型越往下的要素，越难培养。越难发现。投入不同要素上，会有什么结果。 首先知识单纯的知识储备，如果不能结合思维能力解决一些问题，很难有市场价值提升。 技能门槛高，不代表天花板也高，毕竟没人想要一直拿一个工作的门槛工资。 能力门槛不高，不代表天花板也不高。能力提升可以跨行业、职业。一旦积累到一定高度。哪怕行业不行，换个行业一样可以值钱。 自我发现人们总是倾向于做容易的事，而不是正确的事。 你选的不是一份工作，而是天花板。任何工作都是在解决问题，一家公司也是在解决某类用户的问题。 我们的时间永远应该花在正确的事情上，而不是容易的事情上。 虽然 成功的道路有千万条，但成功人士基本上都遵循了一个原则，就是将自身天赋发挥到极致。 天赋并不是少数人的专属，每个人都有自己的天赋。 天赋不是达到一定成绩后，才发挥作用的东西，而是一个在起点就发挥作用的因素。 四、天赋的四个表现 SIGN 自我效能 （self-efficacy）-对某件事特别有信心，觉得自己可以成功。 本能（Insict）- 让你迫不及待、跃跃欲试的事情。可能意味着天赋所在。 成长（Growth）- 在某一领域，你一接触就比别人进步更快一些。 满足（Needs）- 做完这件事后，就算感到疲倦，依然会有满足感。 五、三个知识内化成能力的方法 掌握 20% 核心- 一个领域 20% 核心内容，能解决80% 的问题。 知识和问题相互靠- 学一门知识的时候，要知道能解决什么问题。 做系统化训练-行为和思维的改变至少需要1个月。 我们习惯高估几天的变化，而低估几个月的变化。 大脑不单单是用来记忆的，还是用来思考的。 20% 核心举例子： 结构化思维：主题鲜明、归类分组、逻辑递进。ppt逼格：填充色半透明、边框细、行间距1.5。 六、三大系统 让系统化学习不依靠意志力 人的意志力是有限的额，任何让自己坚持的事情，都会消耗它。 当你坚持学习的时候，不是「学些」这件事拖累了你，而是「坚持」这件事累着你了。 人们醒着的时候，大约 1/4 的时间用来抵制欲望。 如果可以用环境，就不要用你的意志力来抵制欲望。 七、学习不靠意志力三个关键因素 理性上知道要学习 情感上愿意去学习啊。 情景上制造适合学习的场景。 发挥理性作用：树立明确目标。 发挥感性作用：利用情绪冲动。 发挥情景作用：创造学习环境 八、三个建议让你不做「定制化人才」 调整主题，给自己定好发展空间。 提升能力，让自己成为横向可迁移的人才。 提升认知高度，让自己成为纵向可深挖的人才。 社会发展到今天，你所遇到的几乎每个问题，这个世界上都有人曾经解决过它。我们要做的就是学习和发现这个答案。 教育是让一个人成为最好版本的自己—-马斯洛","link":"/2019/09/17/%E5%86%B0%E5%B1%B1%E6%A8%A1%E5%9E%8B/"},{"title":"单例模式","text":"单例概念特点确保某一个类只有一个实例，而且自行实例化并向整个系统提供这个实例，这个类称为单例类，它提供全局访问的方法。 三要点： 某个类只能有一个实例； 是它必须自行创建这个实例； 是它必须自行向整个系统提供这个实例。 实现过程： 在单例类的内部实现只生成一个实例，同时它提供一个静态的getInstance()工厂方法，让客户可以访问它的唯一实例；为了防止在外部对其实例化，将其构造函数设计为私有；在单例类内部定义了一个Singleton类型的静态对象，作为外部共享的唯一实例。 单例模式唯一实例为什么是静态的？ 因为程序调用类中方法只有两种方式 创建类的一个对象，用该对象去调用类中方法； 使用类名直接调用类中方法，格式“类名.方法名()”； 上面说了，构造函数私有化后第一种情况就不能用，只能使用第二种方法。 而使用类名直接调用类中方法，类中方法必须是静态的，而静态方法不能访问非静态成员变量，因此类自定义的实例变量也必须是静态的。 这就是单例模式唯一实例必须设置为静态的原因 实现方式饿汉式//饿汉式 class EagerSingleton { private static final EagerSingleton instance = new EagerSingleton(); private EagerSingleton() { } public static EagerSingleton getInstance() { return instance; } } 当类被加载时，静态变量instance会被初始化，此时类的私有构造函数会被调用，单例类的唯一实例将被创建。如果使用饿汉式单例来实现负载均衡器LoadBalancer类的设计，则不会出现创建多个单例对象的情况，可确保单例对象的唯一性。 懒汉式与线程锁定class LazySingleton { private static LazySingleton instance = null; private LazySingleton() { } synchronized public static LazySingleton getInstance() { if (instance == null) { instance = new LazySingleton(); } return instance; } } 懒汉式单例在第一次调用getInstance()方法时实例化，在类加载时并不自行实例化，这种技术又称为延迟加载(Lazy Load)技术，即需要的时候再加载实例，为了避免多个线程同时调用getInstance()方法，我们可以使用关键字synchronized 在getInstance()方法前面增加了关键字synchronized进行线程锁，以处理多个线程同时访问的问题。但是，上述代码虽然解决了线程安全问题，但是每次调用getInstance()时都需要进行线程锁定判断，在多线程高并发访问环境中，将会导致系统性能大大降低。如何既解决线程安全问题又不影响系统性能呢？ 我们继续对懒汉式单例进行改进。事实上，我们无须对整个getInstance()方法进行锁定，只需对其中的代码“instance = new LazySingleton();”进行锁定即可。因此getInstance()方法可以进行如下改进 public static LazySingleton getInstance() { if (instance == null) { synchronized (LazySingleton.class) { instance = new LazySingleton(); } } return instance; } 问题貌似得以解决，事实并非如此。如果使用以上代码来实现单例，还是会存在单例对象不唯一。 原因如下： 假如在某一瞬间线程A和线程B都在调用 getInstance() 方法，此时instance对象为null值，均能通过instance == null的判断。由于实现了synchronized加锁机制，线程A进入synchronized锁定的代码中执行实例创建代码，线程B处于排队等待状态，必须等待线程A执行完毕后才可以进入synchronized锁定代码。但当A执行完毕时，线程B并不知道实例已经创建，将继续创建新的实例，导致产生多个单例对象，违背单例模式的设计思想，因此需要进行进一步改进，在synchronized中再进行一次(instance == null)判断，这种方式称为**双重检查锁定(Double-Check Locking)**。使用双重检查锁定实现的懒汉式单例类完整代码如下所示： class LazySingleton { private volatile static LazySingleton instance = null; private LazySingleton() { } public static LazySingleton getInstance() { //第一重判断 if (instance == null) { //锁定代码块 synchronized (LazySingleton.class) { //第二重判断 if (instance == null) { instance = new LazySingleton(); //创建单例实例 } } } return instance; } } 需要注意的是 如果使用双重检查锁定来实现懒汉式单例类，需要在静态成员变量instance之前增加修饰符volatile，被volatile修饰的成员变量可以确保多个线程都能够正确处理，且该代码只能在JDK 1.5及以上版本中才能正确执行。由于 volatile 关键字会屏蔽 Java 虚拟机所做的一些代码优化，可能会导致系统运行效率降低。因此即使使用双重检查锁定来实现单例模式也不是一种完美的实现方式。 饿汉式单例类与懒汉式单例类比较饿汉式单例类在类被加载时就将自己实例化， 优点： 无须考虑多线程访问问题，可以确保实例的唯一性； 从调用速度和反应时间角度来讲，由于单例对象一开始就得以创建，因此要优于懒汉式单例。 缺点： 但是无论系统在运行时是否需要使用该单例对象，由于在类加载时该对象就需要创建，因此从资源利用效率角度来讲，饿汉式单例不及懒汉式单例，而且在系统加载时由于需要创建饿汉式单例对象，加载时间可能会比较长。 懒汉式单例类在第一次使用时创建，无须一直占用系统资源，实现了延迟加载 优点： 无须一直占用系统资源，实现了延迟加载。 缺点： 但是必须处理好多个线程同时访问的问题，特别是当单例类作为资源控制器，在实例化时必然涉及资源初始化，而资源初始化很有可能耗费大量时间，这意味着出现多线程同时首次引用此类的机率变得较大，需要通过双重检查锁定等机制进行控制，这将导致系统性能受到一定影响。 以上总结饿汉式单例类不能实现延迟加载，不管将来用不用始终占据内存； 懒汉式单例类线程安全控制烦琐，而且性能受影响。 可见，无论是饿汉式单例还是懒汉式单例都存在这样那样的问题,有没有更好的方式？ 静态内部类 Initialization Demand Holder (IoDH) class Singleton { private Singleton() { } private static class HolderClass { private final static Singleton instance = new Singleton(); } public static Singleton getInstance() { return HolderClass.instance; } public static void main(String args[]) { Singleton s1, s2; s1 = Singleton.getInstance(); s2 = Singleton.getInstance(); System.out.println(s1==s2); } } 由于静态单例对象没有作为Singleton的成员变量直接实例化，因此类加载时不会实例化Singleton，第一次调用getInstance()时将加载内部类HolderClass，在该内部类中定义了一个static类型的变量instance，此时会首先初始化这个成员变量，由Java虚拟机来保证其线程安全性，确保该成员变量只能初始化一次。由于getInstance()方法没有任何线程锁定，因此其性能不会造成任何影响。 通过使用IoDH，我们既可以实现延迟加载，又可以保证线程安全，不影响系统性能，不失为一种最好的Java语言单例模式实现方式（其缺点是与编程语言本身的特性相关，很多面向对象语言不支持IoDH）。","link":"/2019/12/08/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"title":"原型模式","text":"概念原型模式(Prototype Pattern)： 使用原型实例指定创建对象的种类，并且通过克隆拷贝的方式利用原型实例创建新的对象。原型模式是一种对象创建型模式。 通过克隆方法所创建的对象是全新的对象，它们在内存中拥有新的地址，通常对克隆所产生的对象进行修改对原型对象不会造成任何影响，每一个克隆对象都是相互独立的。通过不同的方式修改可以得到一系列相似但不完全相同的对象。 实现方式1. 通用实现方法：定义 Prototype 接口 和 clone() 方法。原型对象继承其接口，实现clone 方法。 interface Prototype{ Prototype clone(); } class ConcretePrototype implements Prototype { private String attr; //成员属性 public void setAttr(String attr) { this.attr = attr; } public String getAttr(){ return this.attr; } //克隆方法 public Prototype clone() { //创建新对象 Prototype prototype = new ConcretePrototype(); prototype.setAttr(this.attr); return prototype; } } 调用方通过下列方法，获得原型实例并得到克隆对象。 Prototype obj1 = new ConcretePrototype(); obj1.setAttr(\"Sunny\"); Prototype obj2 = obj1.clone(); 2. Java 语言提供的 clone 方法Java类都继承自java.lang.Object。事实上，Object类提供一个clone()方法，可以将一个Java对象复制一份。因此在Java中可以直接使用Object提供的clone()方法来实现对象的克隆，Java语言中的原型模式实现很简单。我们可以直接利用Object类的clone()方法，具体步骤如下： 在派生类中覆盖基类的clone()方法，并声明为public； 在派生类的clone()方法中，调用super.clone()； 派生类需实现Cloneable接口。 此时，Object类相当于抽象原型类，所有实现了Cloneable接口的类相当于具体原型类。 需要注意的是能够实现克隆的Java类必须实现一个标识接口Cloneable，表示这个Java类支持被复制。如果一个类没有实现这个接口但是调用了clone()方法，Java编译器将抛出一个CloneNotSupportedException异常。 class ConcretePrototype implements Cloneable { ..... public Prototype clone() { Object object = null; try { object = super.clone(); } catch (CloneNotSupportedException exception) { System.err.println(\"Not support cloneable\"); } return (Prototype )object; } ...... } 调用方： Prototype obj1 = new ConcretePrototype(); Prototype obj2 = obj1.clone(); Java 语言中的clone()方法满足： (1) 对任何对象x，都有x.clone() != x，即克隆对象与原型对象不是同一个对象； (2) 对任何对象x，都有x.clone().getClass() == x.getClass()，即克隆对象与原型对象的类型一样； (3) 如果对象x的equals()方法定义恰当，那么x.clone().equals(x)应该成立。 浅克隆、深克隆通过 Java 的clone 方式可以实现原型模式，但是通过深克隆实现还是浅克隆实现，所达到的结果是不一样的。因为在Java语言中，数据类型分为值类型（基本数据类型）和引用类型，值类型包括int、double、byte、boolean、char等简单数据类型，引用类型包括类、接口、数组等复杂类型。浅克隆和深克隆的主要区别在于是否支持引用类型的成员变量的复制。 1. 浅克隆在浅克隆中，如果原型对象的成员变量是值类型，将复制一份给克隆对象；如果原型对象的成员变量是引用类型，则将引用对象的地址复制一份给克隆对象，也就是说原型对象和克隆对象的成员变量指向相同的内存地址。 简单来说，在浅克隆中，当对象被复制时只复制它本身和其中包含的值类型的成员变量，而引用类型的成员对象并没有复制。 2. 深克隆深克隆中，无论原型对象的成员变量是值类型还是引用类型，都将复制一份给克隆对象，深克隆将原型对象的所有引用对象也复制一份给克隆对象。 简单来说，在深克隆中，除了对象本身被复制外，对象所包含的所有成员变量也将复制 实现深克隆方法：序列化(Serialization) 序列化就是将对象写到流的过程，写到流中的对象是原有对象的一个拷贝，而原对象仍然存在于内存中。通过序列化实现的拷贝不仅可以复制对象本身，而且可以复制其引用的成员对象，因此通过序列化将对象写到一个流中，再从流里将其读出来，可以实现深克隆。需要注意的是能够实现序列化的对象其类必须实现Serializable接口，否则无法实现序列化操作。 class WeeklyLog implements Serializable { private Attachment attachment; private String name; private String date; private String content; public void setAttachment(Attachment attachment) { this.attachment = attachment; } public void setName(String name) { this.name = name; } public void setDate(String date) { this.date = date; } public void setContent(String content){ this.content = content; } public Attachment getAttachment(){ return (this.attachment); } public(String getName() { return (this.name); } public String getDate() { return (this.date); } public String getContent() { return (this.content); } //使用序列化技术实现深克隆 public WeeklyLog deepClone() throws IOException, ClassNotFoundException, OptionalDataException { //将对象写入流中 ByteArrayOutputStream bao=new ByteArrayOutputStream(); ObjectOutputStream oos=new ObjectOutputStream(bao); oos.writeObject(this); //将对象从流中取出 ByteArrayInputStream bis=new ByteArrayInputStream(bao.toByteArray()); ObjectInputStream ois=new ObjectInputStream(bis); return (WeeklyLog)ois.readObject(); } } 总结： 原型模式的主要优点如下： (1) 当创建新的对象实例较为复杂时，使用原型模式可以简化对象的创建过程，通过复制一个已有实例可以提高新实例的创建效率。 (2) 扩展性较好，由于在原型模式中提供了抽象原型类，在客户端可以针对抽象原型类进行编程，而将具体原型类写在配置文件中，增加或减少产品类对原有系统都没有任何影响。 (3) 原型模式提供了简化的创建结构，工厂方法模式常常需要有一个与产品类等级结构相同的工厂等级结构，而原型模式就不需要这样，原型模式中产品的复制是通过封装在原型类中的克隆方法实现的，无须专门的工厂类来创建产品。 (4) 可以使用深克隆的方式保存对象的状态，使用原型模式将对象复制一份并将其状态保存起来，以便在需要的时候使用（如恢复到某一历史状态），可辅助实现撤销操作。 原型模式的主要缺点如下： (1) 需要为每一个类配备一个克隆方法，而且该克隆方法位于一个类的内部，当对已有的类进行改造时，需要修改源代码，违背了“开闭原则”。 (2) 在实现深克隆时需要编写较为复杂的代码，而且当对象之间存在多重的嵌套引用时，为了实现深克隆，每一层对象对应的类都必须支持深克隆，实现起来可能会比较麻烦。 在以下情况下可以考虑使用原型模式： (1) 创建新对象成本较大（如初始化需要占用较长的时间，占用太多的CPU资源或网络资源），新的对象可以通过原型模式对已有对象进行复制来获得，如果是相似对象，则可以对其成员变量稍作修改。 (2) 如果系统要保存对象的状态，而对象的状态变化很小，或者对象本身占用内存较少时，可以使用原型模式配合备忘录模式来实现。 (3) 需要避免使用分层次的工厂类来创建分层次的对象，并且类的实例对象只有一个或很少的几个组合状态，通过复制原型对象得到新实例可能比使用构造函数创建一个新实例更加方便。","link":"/2020/01/08/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"title":"大学","text":"大学之道，在明明德，在亲民，在止于至善。知止而后有定，定而后能静，静而后能安，安而后能虑，虑而后能得。物有本末，事有终始，知所先后，则近道矣。古之欲明明德于天下者，先治其国；欲治其国者，先齐其家；欲齐其家者，先脩其身；欲脩其身者，先正其心；欲正其心者，先诚其意；欲诚其意者，先致其知，致知在格物。物格而后知至，知至而后意诚，意诚而后心正，心正而后身脩，身脩而后家齐，家齐而后国治，国治而后天下平。自天子以至于庶人，壹是皆以脩身为本。其本乱而末治者否矣，其所厚者薄，而其所薄者厚，未之有也！此谓知本，此谓知之至也。 所谓诚其意者，毋自欺也，如恶恶臭，如好好色，此之谓自谦，故君子必慎其独也！小人闲居为不善，无所不至，见君子而后厌然，揜其不善，而著其善。人之视己，如见其肺肝然，则何益矣！此谓诚于中，形于外，故君子必慎其独也。曾子曰：「十目所视，十手所指，其严乎！」富润屋，德润身，心广体胖，故君子必诚其意。 《诗》云：「瞻彼淇澳，菉竹猗猗。有斐君子，如切如磋，如琢如磨。瑟兮僩兮，赫兮喧兮。有斐君子，终不可喧兮！」「如切如磋」者，道学也；「如琢如磨」者，自脩也；「瑟兮僩兮」者，恂慄也；「赫兮喧兮」者，威仪也；「有斐君子，终不可喧兮」者，道盛德至善，民之不能忘也。《诗》云：「於戏，前王不忘！」君子贤其贤而亲其亲，小人乐其乐而利其利，此以没世不忘也。 《康诰》曰：「克明德。」《太甲》曰：「顾諟天之明命。」《帝典》曰：「克明峻德。」皆自明也。 汤之《盘铭》曰：「茍日新，日日新，又日新。」《康诰》曰：「作新民。」《诗》曰：「周虽旧邦，其命维新。」是故君子无所不用其极。《诗》云：「邦畿千里，维民所止。」《诗》云：「缗蛮黄鸟，止于丘隅。」子曰：「於止，知其所止，可以人而不如鸟乎？」《诗》云：「穆穆文王，於缉熙敬止！」为人君，止于仁；为人臣止于敬；为人子，止于孝；为人父，止于慈；与国人交，止于信。 子曰：「听讼，吾犹人也，必也使无讼乎！」无情者不得尽其辞，大畏民志。此谓知本。 所谓脩身在正其心者，身有所忿懥，则不得其正；有所恐惧，则不得其正；有所好乐，则不得其正；有所忧患，则不得其正。心不在焉，视而不见，听而不闻，食而不知其味。此谓脩身在正其心。 所谓齐其家在脩其身者，人之其所亲爱而辟焉，之其所贱恶而辟焉，之其所畏敬而辟焉，之其所哀矜而辟焉，之其所敖惰而辟焉。故好而知其恶，恶而知其美者，天下鲜矣！故谚有之曰：「人莫知其子之恶，莫知其苗之硕。」此谓身不脩，不可以齐其家。 所谓治国必先齐其家者，其家不可教而能教人者，无之。故君子不出家而成教于国：孝者，所以事君也；弟者，所以事长也；慈者，所以使众也。《康诰》曰：「如保赤子」，心诚求之，虽不中，不远矣。未有学养子而后嫁者也！一家仁，一国兴仁；一家让，一国兴让；一人贪戾，一国作乱。其机如此。此谓一言偾事，一人定国。尧、舜率天下以仁，而民从之；桀、纣率天下以暴，而民从之。其所令反其所好，而民不从。是故君子有诸己而后求诸人，无诸己而后非诸人。所藏乎身不恕，而能喻诸人者，未之有也。故治国在齐其家。《诗》云：「桃之夭夭，其叶蓁蓁；之子于归，宜其家人。」宜其家人，而后可以教国人。《诗》云：「宜兄宜弟。」宜兄宜弟，而后可以教国人。《诗》云：「其仪不忒，正是四国。」其为父子兄弟足法，而后民法之也。此谓治国在齐其家。 所谓平天下在治其国者，上老老而民兴孝，上长长而民兴弟，上恤孤而民不倍，是以君子有絜矩之道也。所恶于上，毋以使下；所恶于下，毋以事上；所恶于前，毋以先后；所恶于后，毋以从前；所恶于右，毋以交于左；所恶于左，毋以交于右。此之谓絜矩之道。 《诗》云：「乐只君子，民之父母。」民之所好好之，民之所恶恶之，此之谓民之父母。《诗》云：「节彼南山，维石岩岩。赫赫师尹，民具尔瞻。」有国者不可以不慎，辟，则为天下戮矣。《诗》云：「殷之未丧师，克配上帝。仪监于殷，峻命不易。」道得众则得国，失众则失国。是故君子先慎乎德。有德此有人，有人此有土，有土此有财，有财此有用。德者本也，财者末也。外本内末，争民施夺。是故财聚则民散，财散则民聚。是故言悖而出者，亦悖而入；货悖而入者，亦悖而出。《康诰》曰：「惟命不于常！」道善则得之，不善则失之矣。《楚书》曰：「楚国无以为宝，惟善以为宝。」舅犯曰：「亡人无以为宝，仁亲以为宝。」《秦誓》曰：「若有一介臣，断断兮无他技，其心休休焉，其如有容焉。人之有技，若己有之；人之彦圣，其心好之，不啻若自其口出。实能容之，以能保我子孙黎民，尚亦有利哉！人之有技，媢嫉以恶之；人之彦圣，而违之俾不通。实不能容，以不能保我子孙黎民，亦曰殆哉！」唯仁人放流之，迸诸四夷，不与同中国，此谓唯仁人为能爱人，能恶人。见贤而不能举，举而不能先，命也；见不善而不能退，退而不能远，过也。 好人之所恶，恶人之所好，是谓拂人之性，菑必逮夫身。是故君子有大道，必忠信以得之，骄泰以失之。 生财有大道，生之者众，食之者寡，为之者疾，用之者舒，则财恒足矣。仁者以财发身，不仁者以身发财。未有上好仁而下不好义者也，未有好义其事不终者也，未有府库财非其财者也。 孟献子曰：「畜马乘，不察于鸡豚；伐冰之家，不畜牛羊；百乘之家，不畜聚敛之臣。与其有聚敛之臣，宁有盗臣。」此谓国不以利为利，以义为利也。长国家而务财用者，必自小人矣。彼为善之，小人之使为国家，菑害并至。虽有善者，亦无如之何矣！此谓国不以利为利，以义为利也。","link":"/2018/11/01/%E5%A4%A7%E5%AD%A6/"},{"title":"工厂方法","text":"背景简单工厂模式实现了对象的创建和使用分离，但是仍然存在如下两个问题： (1) 工厂类过于庞大，包含了大量的if…else…代码，导致维护和测试难度增大； (2) 系统扩展不灵活，当系统中需要引入新产品时，由于静态工厂方法通过所传入参数的不同来创建不同的产品，这必定要修改工厂类的源代码，将违背“开闭原则” 如何实现增加新产品而不影响已有代码？ 概念定义一个用于创建对象的接口，让子类决定将哪一个类实例化。工厂方法模式让一个类的实例化延迟到其子类。工厂方法模式又简称为工厂模式(Factory Pattern)，又可称作虚拟构造器模式(Virtual Constructor Pattern)或多态工厂模式(Polymorphic Factory Pattern) 特点不再提供一个统一的工厂类来创建所有的产品对象，而是针对不同的产品提供不同的工厂，系统提供一个与产品等级结构对应的工厂等级结构。 示例代码： //日志记录器接口：抽象产品 interface Logger { public void writeLog(); } //数据库日志记录器：具体产品 class DatabaseLogger implements Logger { public void writeLog() { System.out.println(\"数据库日志记录。\"); } } //文件日志记录器：具体产品 class FileLogger implements Logger { public void writeLog() { System.out.println(\"文件日志记录。\"); } } //日志记录器工厂接口：抽象工厂 interface LoggerFactory { public Logger createLogger(); } //数据库日志记录器工厂类：具体工厂 class DatabaseLoggerFactory implements LoggerFactory { public Logger createLogger() { //连接数据库，代码省略 //创建数据库日志记录器对象 Logger logger = new DatabaseLogger(); //初始化数据库日志记录器，代码省略 return logger; } } //文件日志记录器工厂类：具体工厂 class FileLoggerFactory implements LoggerFactory { public Logger createLogger() { //创建文件日志记录器对象 Logger logger = new FileLogger(); //创建文件，代码省略 return logger; } } //调用 class Client { public static void main(String args[]) { LoggerFactory factory; Logger logger; factory = new FileLoggerFactory(); //可引入配置文件实现 logger = factory.createLogger(); logger.writeLog(); } } 工厂方法模式总结工厂方法模式是简单工厂模式的延伸，它继承了简单工厂模式的优点，同时还弥补了简单工厂模式的不足。工厂方法模式是使用频率最高的设计模式之一，是很多开源框架和API类库的核心模式。 主要优点 工厂方法模式的主要优点如下： (1) 在工厂方法模式中，工厂方法用来创建客户所需要的产品，同时还向客户隐藏了哪种具体产品类将被实例化这一细节，用户只需要关心所需产品对应的工厂，无须关心创建细节，甚至无须知道具体产品类的类名。 (2) 基于工厂角色和产品角色的多态性设计是工厂方法模式的关键。它能够让工厂可以自主确定创建何种产品对象，而如何创建这个对象的细节则完全封装在具体工厂内部。工厂方法模式之所以又被称为多态工厂模式，就正是因为所有的具体工厂类都具有同一抽象父类。 (3) 使用工厂方法模式的另一个优点是在系统中加入新产品时，无须修改抽象工厂和抽象产品提供的接口，无须修改客户端，也无须修改其他的具体工厂和具体产品，而只要添加一个具体工厂和具体产品就可以了，这样，系统的可扩展性也就变得非常好，完全符合“开闭原则”。 主要缺点 工厂方法模式的主要缺点如下： (1) 在添加新产品时，需要编写新的具体产品类，而且还要提供与之对应的具体工厂类，系统中类的个数将成对增加，在一定程度上增加了系统的复杂度，有更多的类需要编译和运行，会给系统带来一些额外的开销。 (2) 由于考虑到系统的可扩展性，需要引入抽象层，在客户端代码中均使用抽象层进行定义，增加了系统的抽象性和理解难度，且在实现时可能需要用到DOM、反射等技术，增加了系统的实现难度。 适用场景 在以下情况下可以考虑使用工厂方法模式： (1) 客户端不知道它所需要的对象的类。在工厂方法模式中，客户端不需要知道具体产品类的类名，只需要知道所对应的工厂即可，具体的产品对象由具体工厂类创建，可将具体工厂类的类名存储在配置文件或数据库中。 (2) 抽象工厂类通过其子类来指定创建哪个对象。在工厂方法模式中，对于抽象工厂类只需要提供一个创建产品的接口，而由其子类来确定具体要创建的对象，利用面向对象的多态性和里氏代换原则，在程序运行时，子类对象将覆盖父类对象，从而使得系统更容易扩展。","link":"/2019/09/06/%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95/"},{"title":"注解","text":"注解是代码里的特殊标记，这些标记可以在编译、类加载、运行时被读取。并执行相应的处理。 一 、注解分类1. Java 内置注解 @Override、 @Deprecated、@SuppressWarnings、@SafeVarargs(jdk 7 新增) 2. 标注注解的元注解元注解是用来修饰注解的注解，从而创建新的注解。 @Targe 注解取值是一个 ElementType 类型的数组。说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目的。 ElementType 说明 ElementType.TYPE|修饰类、接口或枚举类型。 ElementType.FIELD|修饰成员变量。 ElementType.METHOD|修饰方法。 ElementType.PARAMETER|修饰参数。 ElementType.CONSTRUCTOR|修饰构造方法。 ElementType.LOCAL_VARIABLE|修饰局部变量。 ElementType.ANNOTATION_TYPE|修饰注解。 ElementType.PACKAGE|修饰包。 ElementType.TYPE_PARAMETER|修饰参数声明。 ElementType.TYPE_USR|使用类型。 @Retention 3种类型，分别表示不同的保留周期定义了该Annotation被保留的时间长短，SOURCE、CLASS、RUNTIME（可通过反射获取内部属性）。 RetentionPolicy.SOURCE:源码级注解。 注解信息只会保留在.java 源码中，源码在编译后，注解信息会被丢弃，不会保留到.class 中。 RetentionPolicy.CLASS:编译时注解。 注解信息会保留在.java 源码以及.class 中。当运行java 程序时，JVM 会丢弃该注解信息，不会保留到 JVM 中。 RetentionPolicy.RUNTIME:运行时注解。 当运行 java 程序时，JVM 也会保留该注解信息，可以通过反射获取该注解信息。 @Documented 标记注解，可以工具文档化。 @Inherited 当前注解是否可以继承。 @Repeatable JDK 8 新增，允许一个注解在同一声明类型（类、属性或方法）上多次使用。 二、 注解定义 基本定义 定义新的注解类型使用 @interface 关键字。 public @interface Cup{ } 程序中使用该注解： @Cup public class AnnotationTest{ } 定义成员变量(注解的属性) 注解 只有成员变量，没有方法。注解的成员变量在注解定义中以“无形参的方法”形式来声明，其“方法名”定义了该成员变量的名字，其返回值定义了该成员变量的类型； public @interface Cup{ String name(); int price(); } 上面两个成员变量以方法的形式来定义。使用该注解时候就应该为该成员变量赋值 pubilc class AnnotationTest(){ @Cup(name=\"马克杯\",price=100) public void drink(){ } } 也可以在定义注解的时候通过 default 来指定默认值： public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 定义运行时注解 使用@Retention 来设定注解的保留策略 ，这三哥策略的生命周期长度为：SOURCE&lt;CLASS&lt;RUNTIME. @Retention(RetentionPolicy.RUNTIME) public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 4. 定义编译时注解 @Retention(RetentionPolicy.CLASS) public @interface Cup{ String name() default \"马克杯\"; int price() default 100; } 5. Repeatable 定义可重复注解 Repeatable 自然是可重复的意思。@Repeatable 是 Java 1.8 才加进来的，所以算是一个新的特性。 什么样的注解会多次应用呢？通常是注解的值可以同时取多个。 举个例子，一个人他既是程序员又是产品经理,同时他还是个画家。 @interface Persons { Person[] value(); } @Repeatable(Persons.class) @interface Person{ String role default \"\"; } @Person(role=\"artist\") @Person(role=\"coder\") @Person(role=\"PM\") public class SuperMan{ } 注意上面的代码，@Repeatable 注解了 Person。而 @Repeatable 后面括号中的类相当于一个容器注解。 什么是容器注解呢？就是用来存放其它注解的地方。它本身也是一个注解。 以下代码中就是相关容器注解。 @interface Persons { Person[] value(); } 基于容器的注解适用方式： @Persons({@Person(role=\"artist\"),@Person(role=\"coder\")}) public class SuperMan{ } 三、注解的提取注解通过反射获取。首先可以通过 Class 对象的 isAnnotationPresent() 方法判断它是否应用了某个注解 public boolean isAnnotationPresent(Class&lt;? extends Annotation> annotationClass) {} 然后通过 getAnnotation() / getAnnotationsByType()（since 1.8） 方法来获取 Annotation 对象。 public &lt;A extends Annotation> A getAnnotation(Class&lt;A> annotationClass) {} public &lt;A extends Annotation> A[] getAnnotationsByType(Class&lt;A> annotationClass {} 或者是 getAnnotations() 方法。 public Annotation[] getAnnotations() {} 前一种方法返回指定类型的注解，后一种方法返回注解到这个元素上的所有注解。 在处理的注解的过程中可能会用到以下方法 public Field getDeclaredField(String attrbuteName); public Field[] getDeclaredFields(); public Method getDeclaredMethod(String methodName); public Method[] getMethods(); 四、其他注解： 使用场景 分为 三类：编译前、编译时生成代码、运行时。 运行时注解：例如：ButterKnife ，黑科技、低性能 编译时注解：Dagger2：生成中间代码，所以性能高 注解处理器：编译时处理器、运行时处理器。","link":"/2019/09/06/%E6%B3%A8%E8%A7%A3/"},{"title":"正则","text":"正则表达式在几乎所有语言中都可以使用，无论是前端的 JavaScript、还是后端的 Java、c#。他们都提供相应的接口函数支持正则表达式。 但很神奇的是：无论你大学选择哪一门计算机语言，都没有关于正则表达式的课程给你修，在你学会正则之前，你只能看着那些正则大师们，写了一串外星文似的字符串，替代了你用一大篇幅的 if else代码来做一些数据校验。 既然喜欢，那就动手学呗，可当你百度出一一堆相关资料时，你发现无一不例外的枯燥至极，难以学习（实话说，当年不理君也是这样的心态😂😂）。 下面，不理君尝试用一种比较通俗点的方式讲一下正则，让你能在读完之后，能自己写出一些简单的正则，再不济，能看懂别人写的正则，那也不错了。 一、元字符万物皆有缘，正则也是如此，元字符是构造正则表达式的一种基本元素。我们先来记几个常用的元字符： 元字符 说明 . 匹配除换行符以外的任意字符 \\w 匹配字母或数字或下划线或汉字 \\s 匹配任意的空白符 \\d 匹配数字 \\b 匹配单词的开始或结束 ^ 匹配字符串的开始 $ 匹配字符串结束 有了元字符之后，我们就可以利用这些元字符来写一些简单的正则表达式了， 比如： 匹配有abc开头的字符串：\\babc或者^abc 匹配8位数字的QQ号码：^\\d\\d\\d\\d\\d\\d\\d\\d$ 匹配1开头11位数字的手机号码：^1\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d$ 二、重复限定符有了元字符就可以写不少的正则表达式了，但细心的你们可能会发现：别人写的正则简洁明了，而不理君写的正则一堆乱七八糟而且重复的元字符组成的。正则没提供办法处理这些重复的元字符吗？ 答案是有的！ 为了处理这些重复问题，正则表达式中一些重复限定符，把重复部分用合适的限定符替代，下面我们来看一些限定符： 语法 说明 |重复零次或更多次 |重复一次或更多次? |重复零次或一次{n} |重复n次{n,} |重复n次或更多次{n,m} |重复n到m次 有了这些限定符之后，我们就可以对之前的正则表达式进行改造了。 比如： 匹配8位数字的QQ号码：^\\d{8}$ 匹配1开头11位数字的手机号码：^1\\d{10}$ 匹配银行卡号是14~18位的数字：^\\d{14,18}$ 匹配以a开头的，0个或多个b结尾的字符串^ab*$ 三、 分组从上面的例子（4）中看到，限定符是作用在与他左边最近的一个字符，那么问题来了，如果我想要 ab 同时被限定那怎么办呢？ 正则表达式中用小括号 () 来做分组，也就是括号中的内容作为一个整体。 因此当我们要匹配多个 ab 时，我们可以这样。如： 匹配字符串中包含 0 到多个 ab 开头：^(ab)* 四、 转义我们看到正则表达式用小括号来做分组。 那么问题来了：如果要匹配的字符串中本身就包含小括号，那是不是冲突？应该怎么办？ 针对这种情况，正则提供了转义的方式，也就是要把这些元字符、限定符或者关键字转义成普通的字符，做法很简答，就是在要转义的字符前面加个斜杠，也就是\\即可。如： 要匹配以 (ab) 开头：1 ^(\\(ab\\))* 五、 条件或回到我们刚才的手机号匹配，我们都知道：国内号码都来自三大网，它们都有属于自己的号段，比如联通有 130/131/132/155/156/185/186/145/176 等号段，假如让我们匹配一个联通的号码，那按照我们目前所学到的正则，应该无从下手的，因为这里包含了一些并列的条件，也就是“或”，那么在正则中是如何表示“或”的呢？ 正则用符号 | 来表示或，也叫做分支条件，当满足正则里的分支条件的任何一种条件时，都会当成是匹配成功。那么我们就可以用或条件来处理这个问题： ^(130|131|132|155|156|185|186|145|176)\\d{8}$ 六、区间看到上面的例子，是不是看到有什么规律？是不是还有一种想要简化的冲动？ 实际是有的。 正则提供一个元字符中括号 [] 来表示区间条件。 限定 0 到 9 可以写成 [0-9] 限定 A-Z 写成 [A-Z] 限定某些数字 [165] 那上面的正则我们还改成这样： ^((13[0-2])|(15[56])|(18[5-6])|145|176)\\d{8}$ 七、零宽断言无论是零宽还是断言，听起来都古古怪怪的，那先解释一下这两个词。 断言：俗话的断言就是“我断定什么什么”，而正则中的断言，就是说正则可以指明在指定的内容的前面或后面会出现满足指定规则的内容，意思正则也可以像人类那样断定什么什么，比如”ss1aa2bb3”, 正则可以用断言找出 aa2 前面有 bb3，也可以找出 aa2 后面有 ss1. 零宽：就是没有宽度，在正则中，断言只是匹配位置，不占字符，也就是说，匹配结果里是不会返回断言本身。 意思是讲明白了，那他有什么用呢？我们来举个栗子：假设我们要用爬虫抓取 csdn 里的文章阅读量。通过查看源代码可以看到文章阅读量这个内容是这样的结构。 \"&lt;span class=\"read-count\">阅读数：641&lt;/span>\" 其中只有‘641’这个是一个变量，也就是不同文章有不同的值，当我们拿到这个字符串时，需要获得这里边的‘641’有很多种办法，但如果使用正则应该怎么匹配呢？ 下面先讲一下几种类型的断言： 1. 正向先行断言（正前瞻）语法：（?=pattern） 作用：++匹配 pattern 表达式的前面内容，不返回本身。++这样子说，还是一脸懵逼，好吧，回归刚才那个栗子，要取到阅读量，在正则表达式中就意味着要能匹配到&lt;/span&gt;前面的数字内容按照上所说的正向先行断言可以匹配表达式前面的内容，那意思就是:(?=&lt;/span&gt;) 就可以匹配到前面的内容了。匹配什么内容呢？如果要所有内容那就是： String reg=\".+(?=&lt;/span>)\"; String test = \"&lt;span class=\\\"read-count\\\">阅读数：641&lt;/span>\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(\"匹配结果：\") System.out.println(mc.group()); } 输出： # 匹配结果： &lt;span class=\"read-count\">阅读数：641 可是老哥我们要的只是前面的数字呀，那也简单咯，匹配数字 \\d, 那可以改成： String reg=\"\\\\d+(?=&lt;/span>)\"; String test = \"&lt;span class=\\\"read-count\\\">阅读数：641&lt;/span>\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(mc.group()); } 输出 //匹配结果： 641 大功告成！ 2. 正向后行断言（正后顾）语法：（?&lt;=pattern） 作用：匹配 pattern表达式的后面的内容，不返回本身。 有先行就有后行，先行是匹配前面的内容，那后行就是匹配后面的内容啦。 上面的栗子，我们也可以用后行断言来处理： (?&lt;=&lt;span class=\"read-count\">阅读数：)\\d+ String reg=\"(?&lt;=&lt;span class=\\\"read-count\\\">阅读数：)\\\\d+\"; String test = \"&lt;span class=\\\"read-count\\\">阅读数：641&lt;/span>\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(mc.group()); } 输出 # 匹配结果： 641 就这么简单。 3. 负向先行断言（负前瞻）语法：(?!pattern) 作用：匹配非 pattern 表达式的前面内容，不返回本身。 有正向也有负向，负向在这里其实就是非的意思。 举个栗子：比如有一句 “我爱祖国，我是祖国的花朵”现在要找到不是’的花朵’前面的祖国 用正则就可以这样写： 祖国(?!的花朵) 4. 负向后行断言（负后顾）语法：(?&lt;!pattern) 作用：匹配非 pattern 表达式的后面内容，不返回本身。 八、捕获和非捕获单纯说到捕获，他的意思是匹配表达式，但捕获通常和分组联系在一起，也就是“捕获组”。 捕获组：匹配子表达式的内容，把匹配结果保存到内存中中数字编号或显示命名的组里，以深度优先进行编号，之后可以通过序号或名称来使用这些匹配结果。 而根据命名方式的不同，又可以分为两种组： 1. 数字编号捕获组：语法：(exp) 解释：从表达式左侧开始，每出现一个左括号和它对应的右括号之间的内容为一个分组，在分组中，第 0 组为整个表达式，第一组开始为分组。比如固定电话的：020-85653333他的正则表达式为：(0\\d{2})-(\\d{8})按照左括号的顺序，这个表达式有如下分组： 序号 编号 分组 内容 0 0 (0\\d{2})-(\\d{8}) 020-85653333 1 1 (0\\d{2}) 020 2 2 (\\d{8}) 85653333 我们用Java来验证一下： String test = \"020-85653333\"; String reg=\"(0\\\\d{2})-(\\\\d{8})\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); for(int i=0;i&lt;=mc.groupCount();i++){ System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); } } 输出结果： 分组的个数有：2 第0个分组为：020-85653333 第1个分组为：020 第2个分组为：85653333 可见，分组个数是2，但是因为第0个为整个表达式本身，因此也一起输出了。 2. 命名编号捕获组语法：(?&lt;name&gt;exp) 解释：分组的命名由表达式中的 name 指定比如区号也可以这样写:(?&lt;quhao&gt;\\0\\d{2})-(?&lt;haoma&gt;\\d{8})，按照左括号的顺序，这个表达式有如下分组： 序号 名称 分组 内容 0 0 (0\\d{2})-(\\d{8}) 020-85653333 1 quhao (0\\d{2}) 020 2 haoma (\\d{8}) 85653333 用代码来验证一下： String test = \"020-85653333\"; String reg=\"(?&lt;quhao>0\\\\d{2})-(?&lt;haoma>\\\\d{8})\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); System.out.println(mc.group(\"quhao\")); System.out.println(mc.group(\"haoma\")); } 输出结果： 分组的个数有：2 分组名称为:quhao,匹配内容为：020 分组名称为:haoma,匹配内容为：85653333 3. 非捕获组语法：(?:exp) 解释：和捕获组刚好相反，它用来标识那些不需要捕获的分组，说的通俗一点，就是你可以根据需要去保存你的分组。 比如上面的正则表达式，程序不需要用到第一个分组，那就可以这样写： (?:\\0\\d{2})-(\\d{8}) 序号 |编号 |分组 |内容0 |0 |(0\\d{2})-(\\d{8}) |020-856533331 |1 |(\\d{8}) |85653333验证一下： String test = \"020-85653333\"; String reg=\"(?:0\\\\d{2})-(\\\\d{8})\"; Pattern pattern = Pattern.compile(reg); Matcher mc= pattern.matcher(test); if(mc.find()){ System.out.println(\"分组的个数有：\"+mc.groupCount()); for(int fi=0;i&lt;=mc.groupCount();i++){ System.out.println(\"第\"+i+\"个分组为：\"+mc.group(i)); } } 输出结果： 分组的个数有：1 第0个分组为：020-85653333 第1个分组为：85653333 九、 反向引用上面讲到捕获，我们知道：捕获会返回一个捕获组，这个分组是保存在内存中，不仅可以在正则表达式外部通过程序进行引用，也可以在正则表达式内部进行引用，这种引用方式就是反向引用。 根据捕获组的命名规则，反向引用可分为： 数字编号组反向引用：\\k 或\\number 命名编号组反向引用：\\k 或者'name’ 好了 讲完了，懂吗？不懂！！！ 可能连前面讲的捕获有什么用都还不懂吧？ 其实只是看完捕获不懂不会用是很正常的！ 因为捕获组通常是和反向引用一起使用的 上面说到捕获组是匹配子表达式的内容按序号或者命名保存起来以便使用。 注意两个字眼：“内容” 和 “使用”。这里所说的“内容”，是匹配结果，而不是子表达式本身，强调这个有什么用？嗯，先记住。 那这里所说的“使用”是怎样使用呢？ 因为它的作用主要是用来查找一些重复的内容或者做替换指定字符。 还是举栗子吧： 比如要查找一串字母”aabbbbgbddesddfiid”里成对的字母。 如果按照我们之前学到的正则，什么区间啊限定啊断言啊可能是办不到的，现在我们先用程序思维理一下思路： 匹配到一个字母 匹配第下一个字母，检查是否和上一个字母是否一样 如果一样，则匹配成功，否则失败 这里的思路 2 中匹配下一个字母时，需要用到上一个字母，那怎么记住上一个字母呢？？？ 这下子捕获就有用处啦，我们可以利用捕获把上一个匹配成功的内容用来作为本次匹配的条件。 好了，有思路就要实践 首先匹配一个字母：\\w，我们需要做成分组才能捕获，因此写成这样：(\\w)那这个表达式就有一个捕获组：（\\w）然后我们要用这个捕获组作为条件，那就可以：(\\w)\\1这样就大功告成了 可能有人不明白了，\\1 是什么意思呢？ 还记得捕获组有两种命名方式吗，一种是是根据捕获分组顺序命名，一种是自定义命名来作为捕获组的命名在默认情况下都是以数字来命名，而且数字命名的顺序是从 1 开始的。 因此要引用第一个捕获组，根据反向引用的数字命名规则 就需要 \\k&lt;1&gt;或者\\1，当然，通常都是是后者。 我们来测试一下： String test = \"aabbbbgbddesddfiid\"; Pattern pattern = Pattern.compile(\"(\\\\w)\\\\1\"); Matcher mc= pattern.matcher(test); while(mc.find()){ System.out.println(mc.group()); } 输出结果： 1aa 2bb 3bb 4dd 5dd 6ii 嗯，这就是我们想要的了。 在举个替换的例子，假如想要把字符串中abc换成a。 String test = \"abcbbabcbcgbddesddfiid\"; String reg=\"(a)(b)c\"; System.out.println(test.replaceAll(reg, \"$1\"));; 输出结果： abbabcgbddesddfiid 十、贪婪和非贪婪1. 贪婪我们都知道，贪婪就是不满足，尽可能多的要。在正则中，贪婪也是差不多的意思: 贪婪匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能多的字符，这匹配方式叫做贪婪匹配。 特性：一次性读入整个字符串进行匹配，每当不匹配就舍弃最右边一个字符，继续匹配，依次匹配和舍弃（这种匹配 - 舍弃的方式也叫做回溯），直到匹配成功或者把整个字符串舍弃完为止，因此它是一种最大化的数据返回，能多不会少。 前面我们讲过重复限定符，其实这些限定符就是贪婪量词，比如表达式： \\d{3,6} 用来匹配3到6位数字，在这种情况下，它是一种贪婪模式的匹配，也就是假如字符串里有6个个数字可以匹配，那它就是全部匹配到。 如: String reg=\"\\\\d{3,6}\"; String test=\"61762828 176 2991 871\"; System.out.println(\"文本：\"+test); System.out.println(\"贪婪模式：\"+reg); Pattern p1 =Pattern.compile(reg); Matcher m1 = p1.matcher(test); while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0)); } 输出结果： 文本：61762828 176 2991 44 871 贪婪模式：\\d{3,6} 匹配结果：617628 匹配结果：176 匹配结果：2991 匹配结果：871 由结果可见：本来字符串中的“61762828”这一段，其实只需要出现3个（617）就已经匹配成功了的，但是他并不满足，而是匹配到了最大能匹配的字符，也就是6个。 一个量词就如此贪婪了，那有人会问，如果多个贪婪量词凑在一起，那他们是如何支配自己的匹配权的呢？ 是这样的，多个贪婪在一起时，如果字符串能满足他们各自最大程度的匹配时，就互不干扰，但如果不能满足时，会根据深度优先原则，也就是从左到右的每一个贪婪量词，优先最大数量的满足，剩余再分配下一个量词匹配。 String reg=\"(\\\\d{1,2})(\\\\d{3,4})\"; String test=\"61762828 176 2991 87321\"; System.out.println(\"文本：\"+test); System.out.println(\"贪婪模式：\"+reg); Pattern p1 =Pattern.compile(reg); Matcher m1 = p1.matcher(test); while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0)); } 输出结果： 文本：61762828 176 2991 87321 贪婪模式：(\\d{1,2})(\\d{3,4}) 匹配结果：617628 匹配结果：2991 匹配结果：87321 “617628” 是前面的\\d{1,2}匹配出了 61，后面的匹配出了 7628“2991” 是前面的\\d{1,2}匹配出了 29 ，后面的匹配出了 91“87321”是前面的\\d{1,2}匹配出了 87，后面的匹配出了 321 2. 懒惰（非贪婪）懒惰匹配：当正则表达式中包含能接受重复的限定符时，通常的行为是（在使整个表达式能得到匹配的前提下）匹配尽可能少的字符，这匹配方式叫做懒惰匹配。 特性: 从左到右，从字符串的最左边开始匹配，每次试图不读入字符匹配，匹配成功，则完成匹配，否则读入一个字符再匹配，依此循环（读入字符、匹配）直到匹配成功或者把字符串的字符匹配完为止。懒惰量词是在贪婪量词后面加个“？” 代码 说明 *? 重复任意次，但尽可能少重复 +? 重复1次或更多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {n,m}? 重复n到m次，但尽可能少重复 {n,}? 重复n次以上，但尽可能少重复 String reg=\"(\\\\d{1,2}?)(\\\\d{3,4})\"; String test=\"61762828 176 2991 87321\"; System.out.println(\"文本：\"+test); System.out.println(\"贪婪模式：\"+reg); Pattern p1 =Pattern.compile(reg); Matcher m1 = p1.matcher(test); while(m1.find()){ System.out.println(\"匹配结果：\"+m1.group(0)); } 输出结果： 文本：61762828 176 2991 87321 贪婪模式：(\\d{1,2}?)(\\d{3,4}) 匹配结果：61762 匹配结果：2991 匹配结果：87321 解答： “61762” 是左边的懒惰匹配出 6，右边的贪婪匹配出 1762 “2991” 是左边的懒惰匹配出 2，右边的贪婪匹配出 991 “87321” 左边的懒惰匹配出 8，右边的贪婪匹配出 7321 十一、 反义前面说到元字符的都是要匹配什么什么，当然如果你想反着来，不想匹配某些字符，正则也提供了一些常用的反义元字符： 元字符 解释 \\W 匹配任意不是字母，数字，下划线，汉字的字符 \\S 匹配任意不是空白符的字符 \\D 匹配任意非数字的字符 \\B 匹配不是单词开头或结束的位置 [^x] 匹配除了x以外的任意字符 [^aeiou] 匹配除了aeiou这几个字母以外的任意字符 正则知识就讲到这里，正则是一门博大精深的语言，其实学会它的一些语法和知识点还算不太难，但想要做到真正学以致用能写出非常 6的正则。","link":"/2019/01/05/%E6%AD%A3%E5%88%99/"},{"title":"抽象工厂","text":"抽象工厂概念产品类型：单个产品类型。 产品族：包含多个产品类型。 抽象工厂模式为创建一组对象提供了一种解决方案。与工厂方法模式相比，抽象工厂模式中的具体工厂不只是创建一种产品，它负责创建一族产品。 提供一个创建一系列相关或相互依赖对象的接口，而无须指定它们具体的类。抽象工厂模式又称为Kit模式， 和工厂方法的不同抽象工厂模式是所有形式的工厂模式中最为抽象和最具一般性的一种形式。 抽象工厂模式与工厂方法模式最大的区别在于，工厂方法模式针对的是单个产品类型，而抽象工厂模式针对的是多个产品类型，一个工厂类型可以负责多个不同产品类型中的产品对象的创建。当一个工厂类型可以创建出分属于不同产品类型的一个产品族中的所有对象时，抽象工厂模式比工厂方法模式更为简单、更有效率 案例软件 换肤 功能。 graph LR; A[皮肤库]-->B[SPRING]; A[皮肤库]-->C[SUMMER]; B[SPRING]-->绿色按钮; B[SPRING]-->绿色文本框; B[SPRING]-->绿色边框组合框; C[SUMMER]-->蓝色按钮; C[SUMMER]-->蓝色文本框; C[SUMMER]-->蓝色边框组合框; 采用工厂方法模式设计的问题： (1) 当需要增加新的皮肤时，虽然不要修改现有代码，但是需要增加大量类，针对每一个新增具体组件都需要增加一个具体工厂，类的个数成对增加，这无疑会导致系统越来越庞大，增加系统的维护成本和运行开销； (2) 由于同一种风格的具体界面组件通常要一起显示，因此需要为每个组件都选择一个具体工厂，用户在使用时必须逐个进行设置，如果某个具体工厂选择失误将会导致界面显示混乱，虽然我们可以适当增加一些约束语句，但客户端代码和配置文件都较为复杂。 抽象工厂 示例代码： //按钮接口：抽象产品 interface Button { public void display(); } //Spring按钮类：具体产品 class SpringButton implements Button { public void display() { System.out.println(\"显示浅绿色按钮。\"); } } //Summer按钮类：具体产品 class SummerButton implements Button { public void display() { System.out.println(\"显示浅蓝色按钮。\"); } } //文本框接口：抽象产品 interface TextField { public void display(); } //Spring文本框类：具体产品 class SpringTextField implements TextField { public void display() { System.out.println(\"显示绿色边框文本框。\"); } } //Summer文本框类：具体产品 class SummerTextField implements TextField { public void display() { System.out.println(\"显示蓝色边框文本框。\"); } } //组合框接口：抽象产品 interface ComboBox { public void display(); } //Spring组合框类：具体产品 class SpringComboBox implements ComboBox { public void display() { System.out.println(\"显示绿色边框组合框。\"); } } //Summer组合框类：具体产品 class SummerComboBox implements ComboBox { public void display() { System.out.println(\"显示蓝色边框组合框。\"); } } //界面皮肤工厂接口：抽象工厂 interface SkinFactory { public Button createButton(); public TextField createTextField(); public ComboBox createComboBox(); } //Spring皮肤工厂：具体工厂 class SpringSkinFactory implements SkinFactory { public Button createButton() { return new SpringButton(); } public TextField createTextField() { return new SpringTextField(); } public ComboBox createComboBox() { return new SpringComboBox(); } } //Summer皮肤工厂：具体工厂 class SummerSkinFactory implements SkinFactory { public Button createButton() { return new SummerButton(); } public TextField createTextField() { return new SummerTextField(); } public ComboBox createComboBox() { return new SummerComboBox(); } } //客户端调用 class Client { public static void main(String args[]) { //使用抽象层定义 SkinFactory factory; Button bt; TextField tf; ComboBox cb; factory = (SkinFactory)XMLUtil.getBean(); bt = factory.createButton(); tf = factory.createTextField(); cb = factory.createComboBox(); bt.display(); tf.display(); cb.display(); } } 开闭原则 的倾斜性针对以上实现，如果我想添加新的组件（如：添加一个单选按钮），让这个组件也支持换肤功能。需要怎么做呢？ 首先需要修改抽象工厂接口SkinFactory，在其中新增声明创建单选按钮的方法，然后逐个修改具体工厂类，增加相应方法以实现在不同的皮肤中创建单选按钮，此外还需要修改客户端。 这时就发现：原有系统不能够在符合“开闭原则”的前提下增加新的组件。 在抽象工厂模式中，增加新的产品族很方便，但是增加新的产品类型很麻烦，抽象工厂模式的这种性质称为“开闭原则”的倾斜性。 “开闭原则”要求系统对扩展开放，对修改封闭，通过扩展达到增强其功能的目的，对于涉及到多个产品族与多个产品等级结构的系统，其功能增强包括两方面： (1) 增加产品族：对于增加新的产品族，抽象工厂模式很好地支持了“开闭原则”，只需要增加具体产品并对应增加一个新的具体工厂，对已有代码无须做任何修改。 (2) 增加新的产品类型：对于增加新的产品类型，需要修改所有的工厂角色，包括抽象工厂类，在所有的工厂类中都需要增加生产新产品的方法，违背了“开闭原则”。 正因为抽象工厂模式存在“开闭原则”的倾斜性，它以一种倾斜的方式来满足“开闭原则”，为增加新产品族提供方便，但不能为增加新产品类型提供这样的方便，因此要求设计人员在设计之初就能够全面考虑，不会在设计完成之后向系统中增加新的产品类型，也不会删除已有的产品类型，否则将会导致系统出现较大的修改，为后续维护工作带来诸多麻烦。 优缺点缺点： 增加新的产品类型麻烦，需要对原有系统进行较大的修改，甚至需要修改抽象层代码，这显然会带来较大的不便，违背了“开闭原则”。 优点： (1) 抽象工厂模式隔离了具体类的生成，使得客户并不需要知道什么被创建。由于这种隔离，更换一个具体工厂就变得相对容易，所有的具体工厂都实现了抽象工厂中定义的那些公共接口，因此只需改变具体工厂的实例，就可以在某种程度上改变整个软件系统的行为。 (2) 当一个产品族中的多个对象被设计成一起工作时，它能够保证客户端始终只使用同一个产品族中的对象。 (3) 增加新的产品族很方便，无须修改已有系统，符合“开闭原则”。 适用场景在以下情况下可以考虑使用抽象工厂模式： (1) 一个系统不应当依赖于产品类实例如何被创建、组合和表达的细节，这对于所有类型的工厂模式都是很重要的，用户无须关心对象的创建过程，将对象的创建和使用解耦。 (2) 系统中有多于一个的产品族，而每次只使用其中某一产品族。可以通过配置文件等方式来使得用户可以动态改变产品族，也可以很方便地增加新的产品族。 (3) 属于同一个产品族的产品将在一起使用，这一约束必须在系统的设计中体现出来。同一个产品族中的产品可以是没有任何关系的对象，但是它们都具有一些共同的约束，如同一操作系统下的按钮和文本框，按钮与文本框之间没有直接关系，但它们都是属于某一操作系统的，此时具有一个共同的约束条件：操作系统的类型。 (4) 产品类型稳定，设计完成之后，不会向系统中增加新的产品类型或者删除已有的产品类型。","link":"/2019/09/05/%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82/"},{"title":"批量添加Java文件头部注释","text":"背景在一般情况下，作为 Java 开发，如果事先在 Idea 或者 Eclipse中配置模版。在创建一个新类时，会自动生成 头部注释信息，比如下面这种。 /** * * @author name * @since xxxx-xx-xx */ 如果没有在 Idea 或 Eclipse 配置模版，需要手动插入。文件少还好，文件多的话那就mmp了。特别是历史遗留问题，有很多类文件没有加头部注释且编译时又做了代码检查，领导又让你加的时候。事实上本人就碰到了这个问题，大概两三百个文件。 如果你属于上面的情况，那恭喜你。 环境要求 python3 使用方法 拷贝py脚本到对应文件 执行以下命令 python3 addAnnotation.py ${authornName} 如：python3 addAnnotation.py brokge ${authornName} 默认值为 yusuzi 说明 只处理脚本所在目录以下目录和文件 且后缀为 .java 文件 文件如果存在 @author 或 Created by xxx 不处理 支持的注释残缺类型public class aa{} @xxxx public class aa{} /** * 这是个测试类 */ public class aa{} addAnnotation.py 代码#!usr/bin/python3 # -*- coding:UTF-8 -*- import pickle import pprint import re import time import os,sys #from dateutil.parser import parse ''' 使用方法： python3 addAnnotation.py ${authornName} ${authornName} 有默认值 ''' def eachFile(path, _authorName): #print(path) #authorName = _authorName print(\"authorName:\"+ authorName) files = os.listdir(path) for _file in files: #判断是否为文件夹 if (not os.path.isdir(path+'/'+_file) and _file.endswith('.java')): matchAndReplace(path+'/'+_file) print(_file) elif os.path.isdir(path+'/'+_file): eachFile(path+'/'+_file, authorName) def matchAndReplace(file): fo = open(file,\"r+\") #content = fo.read() print(\"#正在处理文件名:\",fo.name) lines = fo.readlines() if checkIsExist(lines): print(\" 已经存在注释\",fo.name) return isOnlyAuthor = checkInsertPosition(lines) position = getInsertPosition(lines) if position == 0: return print('插入行:%d'%position) templateStr = getTemplate('',isOnlyAuthor) lines.insert(position-1,templateStr) #content = content[:position+1]+ templateStr + content[position+1:] #content = templateStr content = ''.join(lines) fo.close() writeFile(content,file) def writeFile(content,file): fo = open(file,\"w\") fo.writelines(content) fo.close() def checkInsertPosition(lines): isOnlyAuthor = False #lines = fo.readlines() for line in lines: if line.startswith(' */'): isOnlyAuthor = True break return isOnlyAuthor def getInsertPosition(lines): positionTemp = 0 #lines = fo.readlines() for line in lines: positionTemp = positionTemp+1 if line.startswith(' */'): break elif line.startswith('@'): break elif line.startswith('public class') or line.startswith('public interface') or line.startswith('public enum'): break return positionTemp def getInsertPosition1(content): class_method = re.search(r'(?&lt;=public class=)\\w+(?=)', content) classPosition = class_method.start() atPosition = re.search() position = 0 if classPosition > atPosition: position = atPosition return position def checkIsExist(lines): isExist = False for line in lines: result = re.search(r'^\\s+\\* @author \\w+|^\\s+\\* Created by \\w+',line) if result is not None: isExist = True break return isExist ''' /** * * @author yusuzi * @since */ ''' def getTemplate(defValue, isOnlyAuthor): templateStr = '' if not isOnlyAuthor: templateStr = ('/**'+ '\\n *'+ defValue+ '\\n * @author '+authorName+ '\\n * @since '+ getCurrentDateTime()+ '\\n */\\n' ) else : templateStr = ( ' * @author '+authorName+ '\\n'+ ' * @since '+ getCurrentDateTime()+ '\\n' ) return templateStr def getCurrentDateTime(): #print(parse('2018-04-29T17:45:25Z')) print(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())) return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()) if __name__ == '__main__': path = sys.path[0] authorName = 'yusuzi' if len(sys.argv)>1: authorName = sys.argv[1] eachFile(path, authorName)","link":"/2020/01/08/%E6%89%B9%E9%87%8F%E6%B7%BB%E5%8A%A0Java%E6%96%87%E4%BB%B6%E5%A4%B4%E9%83%A8%E6%B3%A8%E9%87%8A/"},{"title":"简单工厂","text":"简单工厂概念定义一个工厂类，它可以根据参数的不同返回不同类的实例，被创建的实例通常都具有共同的父类。因为在简单工厂模式中用于创建实例的方法是静态(static)方法，因此简单工厂模式又被称为静态工厂方法(Static Factory Method)模式，它属于类创建型模式。 特点当你需要什么，只需要传入一个正确的参数，就可以获取你所需要的对象，而无须知道其创建细节。达到 创建和使用分离。 示例代码：//抽象图表接口：抽象产品类 interface Chart { public void display(); } //柱状图类：具体产品类 class HistogramChart implements Chart { public HistogramChart() { System.out.println(\"创建柱状图！\"); } public void display() { System.out.println(\"显示柱状图！\"); } } //饼状图类：具体产品类 class PieChart implements Chart { public PieChart() { System.out.println(\"创建饼状图！\"); } public void display() { System.out.println(\"显示饼状图！\"); } } //折线图类：具体产品类 class LineChart implements Chart { public LineChart() { System.out.println(\"创建折线图！\"); } public void display() { System.out.println(\"显示折线图！\"); } } //图表工厂类：工厂类 class ChartFactory { //静态工厂方法 public static Chart getChart(String type) { Chart chart = null; if (type.equalsIgnoreCase(\"histogram\")) { chart = new HistogramChart(); System.out.println(\"初始化设置柱状图！\"); } else if (type.equalsIgnoreCase(\"pie\")) { chart = new PieChart(); System.out.println(\"初始化设置饼状图！\"); } else if (type.equalsIgnoreCase(\"line\")) { chart = new LineChart(); System.out.println(\"初始化设置折线图！\"); } return chart; } } 简单工厂模式总结简单工厂模式提供了专门的工厂类用于创建对象，将对象的创建和对象的使用分离开。 优点 简单工厂模式的主要优点如下： (1) 工厂类包含必要的判断逻辑，可以决定在什么时候创建哪一个产品类的实例，客户端可以免除直接创建产品对象的职责，而仅仅“消费”产品，简单工厂模式实现了对象创建和使用的分离。 (2) 客户端无须知道所创建的具体产品类的类名，只需要知道具体产品类所对应的参数即可，对于一些复杂的类名，通过简单工厂模式可以在一定程度减少使用者的记忆量。 (3) 通过引入配置文件，可以在不修改任何客户端代码的情况下更换和增加新的具体产品类，在一定程度上提高了系统的灵活性。 缺点简单工厂模式的主要缺点如下： (1) 由于工厂类集中了所有产品的创建逻辑，职责过重，一旦不能正常工作，整个系统都要受到影响。 (2) 使用简单工厂模式势必会增加系统中类的个数（引入了新的工厂类），增加了系统的复杂度和理解难度。 (3) 系统扩展困难，一旦添加新产品就不得不修改工厂逻辑，在产品类型较多时，有可能造成工厂逻辑过于复杂，不利于系统的扩展和维护。 (4) 简单工厂模式由于使用了静态工厂方法，造成工厂角色无法形成基于继承的等级结构。 适用场景在以下情况下可以考虑使用简单工厂模式： (1) 工厂类负责创建的对象比较少，由于创建的对象较少，不会造成工厂方法中的业务逻辑太过复杂。 (2) 客户端只知道传入工厂类的参数，对于如何创建对象并不关心。","link":"/2019/09/06/%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82/"},{"title":"Android 线程池实践","text":"Android 线程池特点 Thread(),AsyncTask适合处理单个任务的场景，HandlerThread适合串行处理多任务的场景。当需要并行的处理多任务之时，ThreadPoolExecutor是更好的选择。 线程池可以避免线程的频繁创建和销毁，显然性能更好，但线程池并发的特性往往也是疑难杂症的源头，是代码降级和失控的开始。多线程并行导致的bug往往是偶现的，不方便调试，一旦出现就会耗掉大量的开发精力。 ThreadPool较之HandlerThread在处理多任务上有更高的灵活性，但也带来了更大的复杂度和不确定性。 常用创建线程池的方式public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable> workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) { //... } 具体参数代表的信息，可文章底部的说明。 线程池的分配遵循这样的规则 当线程池中的核心线程数量未达到最大线程数时，启动一个核心线程去执行任务； 如果线程池中的核心线程数量达到最大线程数时，那么任务会被插入到任务队列中排队等待执行； 如果在上一步骤中任务队列已满但是线程池中线程数量未达到限定线程总数，那么启动一个非核心线程来处理任务； 如果上一步骤中线程数量达到了限定线程总量，那么线程池则拒绝执行该任务，且ThreadPoolExecutor会调用RejectedtionHandler的rejectedExecution方法来通知调用者。 实践场景从我们现实的业务种，所用到的场景是大同小异。所以官方准备了几个不同类型的线程池。通过工厂方法创建以下类型： #newFixThreadPool通过Executors的newFixedThreadPool()方法创建，它是个线程数量固定的线程池，该线程池的线程全部为核心线程，它们没有超时机制且排队任务队列无限制，因为全都是核心线程，所以响应较快，且不用担心线程会被回收。 ExecutorService mExecutor = Executors.newFixedThreadPool(5); Runnable runnable = new Runnable() { @Override public void run () { //todo somthing } } mExecutor.execute(runnable); #newCachedThreadPool通过Executors的newCachedThreadPool()方法来创建，它是一个数量无限多的线程池，它所有的线程都是非核心线程，当有新任务来时如果没有空闲的线程则直接创建新的线程不会去排队而直接执行，并且超时时间都是60s，所以此线程池适合执行大量耗时小的任务。由于设置了超时时间为60s，所以当线程空闲一定时间时就会被系统回收，所以理论上该线程池不会有占用系统资源的无用线程。 public static ExecutorService new CachedThreadPool(){ return new ThreadPoolExecutor( 0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>() ); } 示例代码： ExecutorService mExecutor = Executors.new CachedThreadPool(); Runnable runnable = new Runnable() { @Override public void run () { //todo somthing } } mExecutor.execute(runnable); #newScheduledThreadPool通过Executors的newScheduledThreadPool()方法来创建，ScheduledThreadPool线程池像是上两种的合体，它有数量固定的核心线程，且有数量无限多的非核心线程，但是它的非核心线程超时时间是0s，所以非核心线程一旦空闲立马就会被回收。这类线程池适合用于执行定时任务和固定周期的重复任务。 public static ScheduledThreadPool newScheduledThreadPool(int corePoolSize){ return new ScheduledThreadPoolExecutor(corePoolSize); } public ScheduledThreadPoolExecutor(int corePoolSize){ super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); } 示例代码： ExecutorService mExecutor = Executors.newScheduledThreadPool(5); Runnable runnable = new Runnable() { @Override public void run () { //todo somthing } } mExecutor.execute(runnable); #newSingleThreadExecutor通过Executors的newSingleThreadExecutor()方法来创建，它内部只有一个核心线程，它确保所有任务进来都要排队按顺序执行。它的意义在于，统一所有的外界任务到同一线程中，让调用者可以忽略线程同步问题。 public static ExecutorService newSingleThreadExecutor(){ return new FinalizableDelegatedExecutorService( new ThreadPoolExecutor( 1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>())); } 示例代码： ExecutorService mExecutor = Executors.newSingleThreadExecutor(); Runnable runnable = new Runnable() { @Override public void run () { //todo somthing } } mExecutor.execute(runnable); ExecutorService通过以上代码我们可以看出，所返回的皆是 ExecutorService，一个接口且继承于 Executor; interface ExecutorService extends Executor 接口方法： shutDown()，关闭线程池，需要执行完已提交的任务； shutDownNow()，关闭线程池，并尝试结束已提交的任务； allowCoreThreadTimeOut(boolen)，允许核心线程闲置超时回收； execute()，提交任务无返回值； submit()，提交任务有返回值； 参数 corePoolSize：核心线程数，如果运行的线程少于corePoolSize，则创建新线程来执行新任务，即使线程池中的其他线程是空闲的 maximumPoolSize:最大线程数，可允许创建的线程数，corePoolSize和maximumPoolSize设置的边界自动调整池大小： corePoolSize &lt;运行的线程数&lt; maximumPoolSize:仅当队列满时才创建新线程corePoolSize=运行的线程数= maximumPoolSize：创建固定大小的线程池 keepAliveTime:如果线程数多于corePoolSize,则这些多余的线程的空闲时间超过keepAliveTime时将被终止 unit:keepAliveTime参数的时间单位 workQueue:保存任务的阻塞队列，与线程池的大小有关：当运行的线程数少于corePoolSize时，在有新任务时直接创建新线程来执行任务而无需再进队列； 当运行的线程数等于或多于corePoolSize，在有新任务添加时则选加入队列，不直接创建线程； 当队列满时，在有新任务时就创建新线程 threadFactory:使用ThreadFactory创建新线程，默认使用defaultThreadFactory创建线程 handle:定义处理被拒绝任务的策略，默认使用ThreadPoolExecutor.AbortPolicy,任务被拒绝时将抛出RejectExecutorException","link":"/2019/01/06/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"title":"深入理解Mysql索引底层数据结构与算法","text":"一、索引优化面试题分析 分析以下几条sql的索引使用情况 SELECT * FROM titles WHERE emp_no='10001' AND title='Senior Engineer' AND from_date='1986-06-26'; SELECT * FROM titles WHERE title='Senior Engineer' ; SELECT * FROM titles WHERE emp_no > ‘10001'; SELECT * FROM titles WHERE emp_no > ‘10001' and title='Senior Engineer'; SELECT * FROM titles WHERE emp_no > ‘10001' order BY title; 二、索引到底是什么索引是帮助MySQL高效获取数据的排好序的数据结构,索引存储在文件里。 索引结构 二叉树、红黑树、HASH、Btree(B+tree) 磁盘存储原理磁盘存储耗时分为两个部分： 寻道时间，指针移动时间(速度慢，费时) 旋转时间，磁盘旋转(速度较快) 所以从mysql索引的角度，要减少寻道时间。 磁盘IO 以页为单位（固定大小 4KB）。所以一次数据库查询或写入操作，要尽可能的减少IO 次数，则每页包含的尽量多的要操作的目标数据。 三、索引底层数据结构与算法树简单分为：二叉树、B树、B+树。度(Degree)-代表节点的数据存储个数，也指代节点最大的子节点个数。 B-Tree 每个节点都（包含叶子节点）存储数据 节点中的数据 key 从左到右递增排列 叶子节点不存储指针。 每个节点的子节点个数m/2&lt;= 个数 &lt;=m,但根节点的节点个数可以不超过m/2。 B+Tree(B-Tree变种) 一般情况下，根节点存储在内存中，其他节点存储在磁盘中 非叶子节点不存储数据，只存储索引，可以增大度，类似跳表。 叶子节点不存储指针，存储数据 每个节点的子节点个数m/2&lt;= 个数 &lt;=m,但根节点的节点个数可以不超过m/2。 叶子节点不存储指针 叶子节点是通过链表串在一起（方便区间查找）。 B与 B+树的区别 B+树中的节点不存储数据，只是索引,而 B 树中的节点存储数据。 B 树中的叶子节点不需要链表来串联。 性能分析B+Tree 索引的性能分析一般使用磁盘I/O次数评价索引结构的优劣 预读：磁盘一般会顺序向后读取一定长度的数据(页的整数倍)放入内存。 局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用B+Tree节点的大小设为等于一个页，每次新建节点直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，就实现了一个节点的载入只需一次I/O。 B+Tree的度d一般会超过100，因此h非常小(一般为3到5之间) 索引实现Mysql 常用存储引擎为：MyISAM 和 InnoDB。在创建表的时候指定存储引擎， MyISAM索引实现(非聚集)MyISAM索引文件和数据文件是分离的 InnoDB索引实现(聚集) 数据文件本身就是索引文件。 表数据文件本身就是按B+Tree组织的一个索引结构文件。 聚集索引-叶节点包含了完整的数据记录。 为什么 InnoDB表必须有主键，并且推荐使用整型的自增主键？ 因为主键索引存储的有数据，基于这个原因，必须要有主键，如果没主键会默认添加一个整型自增主键。 为什么非主键索引结构叶子节点存储的是主键值？基于一致性和节省存储空间的考虑，非主键索引节点存储主键值。 联合索引，索引最左前缀原理 如果索引了多列，要遵守最左前缀法则。指的是查询从引的最左前列开始并且不跳过索引中的列。 例：idx(a,b,c) select * from table where a=10002 and b='staff' and c='199600823';(Y) select * from table where a=10002;(Y) select * from table where a=10002 and c = '199600823';(部分索引Y,部分 N) select * from table where b='staff' and c = '199600823';(N) elect * from table where b='staff';(N) elect * from table where c = '199600823';(N) 语句 是否用到索引 使用的索引 where a=3 Y a where a=3 and b = 5 Y a，b where a=3 and b = 5 and c = 4 Y a，b, c where b=3 或者 where b = 3 and c = 4 或者 where c=4 N where a=3 and c = 5 Y 使用到 a,但 c 没有使用，b中间断了 where a=3 and b&gt;4 and c = 5 Y 使用到 a,b,但 c 不能用在范围之后，所以 b 断了。 where a=3 and b like ‘kk%’ and c = 5 Y 只用到 a，b ,c where a=3 and b like ‘%kk’ and c = 5 Y 只用到 a where a=3 and b like ‘%kk%’ and c = 5 Y 只用到 a where a=3 and b like ‘k%kk%’ and c = 5 Y 只用到 a,b,c 总结 MySQL支持两种方式的排序filesort和index，Using index是指MySQL扫描索引本身完成排序。index效率高，filesort效率低。 order by满足两种情况会使用Using index。#1.order by语句使用索引最左前列。#2.使用where子句与order by子句条件列组合满足索引最左前列。 尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最佳左前缀法则。 如果order by的条件不在索引列上，就会产生Using filesort。 group by与order by很类似，其实质是先排序后分组，遵照索引创建顺序的最佳左前缀法则。注意where高于having，能写在where中的限定条件就不要去having限定了。 通俗理解口诀： 全值匹配我最爱，最左前缀要遵守； 带头大哥不能死，中间兄弟不能断； 索引列上少计算，范围之后全失效； LIKE百分写最右，覆盖索引不写星； 不等空值还有or，索引失效要少用。 补充：in和exsits优化 原则：小表驱动大表，即小的数据集驱动大的数据集 in：当B表的数据集必须小于A表的数据集时，in优于exists select * from A where id in (select id from B) #等价于： for select id from B for select * from A where A.id = B.id exists：当A表的数据集小于B表的数据集时，exists优于in 将主查询A的数据，放到子查询B中做条件验证，根据验证结果（true或false）来决定主查询的数据是否保留 select * from A where exists (select 1 from B where B.id = A.id) #等价于 for select * from A for select * from B where B.id = A.id A表与B表的ID字段应建立索引 EXISTS (subquery)只返回TRUE或FALSE,因此子查询中的SELECT * 也可以是SELECT 1或select X,官方说法是实际执行时会忽略SELECT清单,因此没有区别 EXISTS子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比 EXISTS子查询往往也可以用JOIN来代替，何种最优需要具体问题具体分析","link":"/2020/08/05/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Mysql%E7%B4%A2%E5%BC%95%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"title":"深入理解MySql锁 和事物隔离级别","text":"一、概述定义锁是计算机协调多个进程或线程并发访问某一资源的机制。在数据库中，除了传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供需要用户共享的资源。如何保证数据并发访问的一致性、有效性是所有数据库必须解决的一个问题，锁冲突也是影响数据库并发访问性能的一个重要因素。从这个角度来说，锁对数据库而言显得尤其重要，也更加复杂。 锁的分类 从性能上分为乐观锁(用版本对比来实现)和悲观锁 从对数据库操作的类型分，分为读锁和写锁(都属于悲观锁) 读锁（共享锁）：针对同一份数据，多个读操作可以同时进行而不会互相影响 写锁（排它锁）：当前写操作没有完成前，它会阻断其他写锁和读锁 从对数据操作的粒度分，分为表锁和行锁 二、三种锁表锁（偏读）表锁偏向MyISAM存储引擎，开销小，加锁快，无思索，锁定粒度大，发生锁冲突的概率最高，并发度最低。 基本操作 建表SQLCREATE TABLE `mylock` ( `id` INT (11) NOT NULL AUTO_INCREMENT, `NAME` VARCHAR (20) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE = MyISAM DEFAULT CHARSET = utf8; 插入数据INSERT INTO`test`.`mylock` (`id`, `NAME`) VALUES ('1', 'a'); INSERT INTO`test`.`mylock` (`id`, `NAME`) VALUES ('2', 'b'); INSERT INTO`test`.`mylock` (`id`, `NAME`) VALUES ('3', 'c'); INSERT INTO`test`.`mylock` (`id`, `NAME`) VALUES ('4', 'd'); 手动增加表锁lock table 表名称 read(write),表名称2 read(write); 查看表上加过的锁show open tables; 删除表锁unlock tables; 案例分析(加读/写锁） 添加读锁当前session和其他session都可以读该表当前session中插入或者更新锁定的表都会报错，其他session插入或更新则会等待 加写锁当前session对该表的增删改查都没有问题，其他session对该表的所有操作被阻塞 案例结论MyISAM在执行查询语句(SELECT)前,会自动给涉及的所有表加读锁,在执行增删改操作前,会自动给涉及的表加写锁。 对MyISAM表的读操作(加读锁) ,不会阻寒其他进程对同一表的读请求,但会阻赛对同一表的写请求。只有当读锁释放后,才会执行其它进程的写操作。 对MylSAM表的写操作(加写锁) ,会阻塞其他进程对同一表的读和写操作,只有当写锁释放后,才会执行其它进程的读写操作 总结：简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。 行锁（偏写）行锁偏向InnoDB存储引擎，开销大，加锁慢，会出现死锁，锁定粒度最小，发生锁冲突的概率最低，并发度也最高。InnoDB 与 MYISAM 的最大不同有两点：一是支持事务（TRANSACTION）；二是采用了行级锁。 行锁支持事务 事务（Transaction）及其ACID属性 事务是由一组SQL语句组成的逻辑处理单元,事务具有以下4个属性,通常简称为事务的ACID属性。 **原子性(Atomicity)**：事务是一个原子操作单元,其对数据的修改,要么全都执行,要么全都不执行。 **一致性(Consistent)**：在事务开始和完成时,数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改,以保持数据的完整性;事务结束时,所有的内部数据结构(如B树索引或双向链表)也都必须是正确的。 **隔离性(Isolation)**：数据库系统提供一定的隔离机制,保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的,反之亦然。 **持久性(Durable)**：事务完成之后,它对于数据的修改是永久性的,即使出现系统故障也能够保持。 并发事务处理带来的问题 更新丢失（Lost Update）当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题–最后的更新覆盖了由其他事务所做的更新。 脏读(Dirty Reads)一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致的状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此作进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象的叫做“脏读”。==一句话==：事务A读取到了事务B已经修改但尚未提交的数据，还在这个数据基础上做了操作。此时，如果B事务回滚，A读取的数据无效，不符合一致性要求。 不可重读(Non-Repeatable Reads)一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。==一句话==：事务A读取到了事务B已经提交的修改数据，不符合隔离性。 幻读（Phantom Reads）一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。==一句话==：事务A读取到了事务B提交的新增数据，不符合隔离性，脏读是事务B里面修改了数据，幻读是事务B里面新增了数据。 事务隔离级别脏读”、“不可重复读”和“幻读”,其实都是数据库读一致性问题,必须由数据库提供一定的事务隔离机制来解决。 数据库的事务隔离越严格,并发副作用越小,但付出的代价也就越大,因为事务隔离实质上就是使事务在一定程度上“串行化”进行,这显然与“并发”是矛盾的。 同时,不同的应用对读一致性和事务隔离程度的要求也是不同的,比如许多应用对“不可重复读”和“幻读”并不敏感,可能更关心数据并发访问的能力。 常看当前数据库的事务隔离级别: show variables like 'tx_isolation'; 设置事务隔离级别：set tx_isolation=’REPEATABLE-READ’; 行锁案例分析结论在开启事务的情况下，Session_1 更新某一行，Session_2 更新同一行被阻塞，但是更新其他行正常。 隔离级别案例分析CREATE TABLE `account` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) DEFAULT NULL, `balance` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('lilei', '450'); INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('hanmei', '16000'); INSERT INTO `test`.`account` (`name`, `balance`) VALUES ('lucy', '2400'); 读未提交： 打开一个客户端A，并设置当前事务模式为read uncommitted（未提交读），查询表account的初始值： set tx_isolation='read-uncommitted'; 在客户端A的事务提交之前，打开另一个客户端B，更新表account： 这时，虽然客户端B的事务还没提交，但是客户端A就可以查询到B已经更新的数据： 一旦客户端B的事务因为某种原因回滚，所有的操作都将会被撤销，那客户端A查询到的数据其实就是脏数据： 在客户端A执行更新语句update account set balance = balance - 50 where id =1，lilei的balance没有变成350，居然是400，是不是很奇怪，数据不一致啊，==如果你这么想就太天真 了，在应用程序中，我们会用400-50=350，并不知道其他会话回滚了，要想解决这个问题可以采用读已提交的隔离级别== 读已提交 打开一个客户端A，并设置当前事务模式为read committed（未提交读），查询表account的所有记录： set tx_isolation='read-committed'; 在客户端A的事务提交之前，打开另一个客户端B，更新表account. 这时，客户端B的事务还没提交，客户端A不能查询到B已经更新的数据，解决了脏读问题： 客户端B的事务提交 5. 客户端A执行与上一步相同的查询，结果 与上一步不一致，即产生了不可重复读的问题 可重复读 打开一个客户端A，并设置当前事务模式为repeatable read，查询表account的所有记录 set tx_isolation='repeatable-read'; 在客户端A的事务提交之前，打开另一个客户端B，更新表account并提交 在客户端A查询表account的所有记录，与步骤（1）查询结果一致，没有出现不可重复读的问题 在客户端A，接着执行update balance = balance - 50 where id = 1，balance没有变成400-50=350，lilei的balance值用的是步骤（2）中的350来算的，所以是300，数据的一致性倒是没有被破坏。可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）；insert、update和delete会更新版本号，是当前读（当前版本）。 重新打开客户端B，插入一条新数据后提交 在客户端A查询表account的所有记录，没有 查出 新增数据，所以没有出现幻读 验证幻读在客户端A执行update account set balance=888 where id = 4;能更新成功，再次查询能查到客户端B新增的数据. 串行化 打开一个客户端A，并设置当前事务模式为serializable，查询表account的初始值： set tx_isolation='serializable'; mysql&gt; set session transaction isolation level serializable; Query OK, 0 rows affected (0.00 sec) mysql&gt; start transaction; Query OK, 0 rows affected (0.00 sec) mysql&gt; select * from account; +------+--------+---------+ | id | name | balance | +------+--------+---------+ | 1 | lilei | 10000 | | 2 | hanmei | 10000 | | 3 | lucy | 10000 | | 4 | lily | 10000 | +------+--------+---------+ 4 rows in set (0.00 sec) 打开一个客户端B，并设置当前事务模式为serializable，插入一条记录报错，表被锁了插入失败，mysql中事务隔离级别为serializable时会锁表，因此不会出现幻读的情况，这种隔离级别并发性极低，开发中很少会用到。 mysql&gt; set session transaction isolation level serializable; Query OK, 0 rows affected (0.00 sec) mysql&gt; start transaction; Query OK, 0 rows affected (0.00 sec) mysql&gt; insert into account values(5,'tom',0); ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction Mysql默认级别是repeatable-read，有办法解决幻读问题吗？ 间隙锁在某些情况下可以解决幻读问题, 要避免幻读可以用间隙锁在Session_1下面执行 update account set name = 'zhuge' where id &gt; 10 and id &lt;=20; 则其他Session没法插入这个范围内的数据 案例结论Innodb存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一下，但是在整体并发处理能力方面要远远优于MYISAM的表级锁定的。当系统并发量高的时候，Innodb的整体性能和MYISAM相比就会有比较明显的优势了。 但是，Innodb的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让Innodb的整体性能表现不仅不能比MYISAM高，甚至可能会更差。 行锁分析通过检查InnoDB_row_lock状态变量来分析系统上的行锁的争夺情况 show status like'innodb_row_lock%'; 对各个状态量的说明如下： Innodb_row_lock_current_waits: 当前正在等待锁定的数量 Innodb_row_lock_time: 从系统启动到现在锁定总时间长度 Innodb_row_lock_time_avg: 每次等待所花平均时间 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花时间 Innodb_row_lock_waits:系统启动后到现在总共等待的次数 对于这5个状态变量，比较重要的主要是： Innodb_row_lock_time_avg （等待平均时长） Innodb_row_lock_waits （等待总次数） Innodb_row_lock_time（等待总时长） 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化计划。 死锁set tx_isolation='repeatable-read'; Session_1执行：select * from account where id=1 for update; Session_2执行：select * from account where id=2 for update; Session_1执行：select * from account where id=2 for update; Session_2执行：select * from account where id=1 for update; 查看近期死锁日志信息： show engine innodb status; 大多数情况mysql可以自动检测死锁并回滚产生死锁的那个事务，但是有些情况mysql没法自动检测死锁 三、关于锁的优化建议 尽可能让所有数据检索都通过索引来完成，避免无索引行锁升级为表锁 合理设计索引，尽量缩小锁的范围 尽可能减少检索条件，避免间隙锁 尽量控制事务大小，减少锁定资源量和时间长度 尽可能低级别事务隔离","link":"/2020/08/05/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3MySql%E9%94%81-%E5%92%8C%E4%BA%8B%E7%89%A9%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"title":"线程通信的方式","text":"线程通信也就是线程之间的协作方式。 1. 线程协作的方式要想实现多个线程之间的通信协作如： 线程执行的先后顺序、获取某个线程执行的结果等。 涉及到线程之间的相互通信/协作，可以分为 以下四类： 文件共享 网络共享 共享变量 JDK 提供的线程协同 API suppend/resume (已经弃用) wait/notify park/unpark 管道通信 下面主要针对 共享变量 和 JDK 提供的线程协作 API 来做个梳理。 2. 共享变量所谓共享变量，即通过一个线程之间可以共享的变量来达到线程之间通信的目的。 根据 JVM 的特性，线程之间能够共享的变量方式，即 静态变量。 共享变量代码示例package com.study.hc.thread.chapter1.thread; public class MainTest { // 定义共享变量 public static String content = \"空\"; public static void main(String[] args) { // 线程1 - 写入数据 new Thread(() -> { try { while (true) { content = \"当前时间\" + String.valueOf(System.currentTimeMillis()); Thread.sleep(1000L); } } catch (Exception e) { e.printStackTrace(); } }).start(); // 线程2 - 读取数据 new Thread(() -> { try { while (true) { Thread.sleep(1000L); System.out.println(content); } } catch (Exception e) { e.printStackTrace(); } }).start(); } } 线程 1 对静态变量 content 做更改，则 线程 2 可以访问更改过的值。 3. JDK 提供的线程协同 APIJDK 中对于需要多线程协作完成某一任务的场景，提供了对应的 API 支持。多线程协作的典型场景是：生产者-消费者模型。（线程阻塞、线程唤醒）。 下面通过一个场景来看 jdk 提供的线程协同 api 并做个对比。 场景是这样子： 线程-1 去买包子，没有包子，则不再执行。线程-2 生产包子，通知线程继续执行，线程1收到通知继续买包子。 流程图如下 3.1 suspend/resumepublic class Demo6 { /** 包子店 */ public static Object baozidian = null; /** 正常的suspend/resume */ public void suspendResumeTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); Thread.currentThread().suspend(); } System.out.println(\"3、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); consumerThread.resume(); System.out.println(\"2、通知消费者\"); } 启动一个线程 consumerThread 购买包子，如果没包子，则进入等待 调用 suspend(),主线程 3秒之后生产一个包子，然后唤醒其他线程resume()。consumerThread 被唤醒继续执行 3、买到包子，回家。 按照正常逻辑以上代码没有问题，但是如果是以下方式呢？ /** 死锁的suspend/resume。 suspend并不会像wait一样释放锁，故此容易写出死锁代码 */ public void suspendResumeDeadLockTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); // 当前线程拿到锁，然后挂起 synchronized (this) { Thread.currentThread().suspend(); } } System.out.println(\"3、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); // 争取到锁以后，再恢复consumerThread synchronized (this) { consumerThread.resume(); } System.out.println(\"2、通知消费者\"); } /** 导致程序永久挂起的suspend/resume */ public void suspendResumeDeadLockTest2() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { System.out.println(\"1、没包子，进入等待\"); try { // 为这个线程加上一点延时 Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } // 这里的挂起执行在resume后面 Thread.currentThread().suspend(); } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); consumerThread.resume(); System.out.println(\"3、通知消费者\"); consumerThread.join(); } } } 以上两种方式都是死锁的写法。 suspendResumeDeadLockTest 写法中 suspend 并不释放锁，一直持有锁，导致主线程获取不到锁，不能执行 resume()唤醒操作， 故此容易写出死锁代码 。 suspendResumeDeadLockTest2 写法种，由于线程休眠的问题，导致resume() 再 suspend()之前调用，由于后续没有调用 resume()的操作，导致死锁。 3.2 notify/waitnotify / wait 必须再 synchronized 代码块中才会有效，而且 wait 和 notify() 调用之后会自动释放持有的锁。 /** 正常的wait/notify */ public void waitNotifyTest() throws Exception { // 启动线程 new Thread(() -> { synchronized (this) { while (baozidian == null) { // 如果没包子，则进入等待 try { System.out.println(\"1、进入等待\"); this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } System.out.println(\"3、买到包子，回家\"); }).start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); synchronized (this) { this.notifyAll(); System.out.println(\"2、通知消费者\"); } } 但是也会写出死锁的代码，比如以下写法： /** 会导致程序永久等待的wait/notify */ public void waitNotifyDeadLockTest() throws Exception { // 启动线程 new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 try { Thread.sleep(5000L); } catch (InterruptedException e1) { e1.printStackTrace(); } synchronized (this) { try { System.out.println(\"1、进入等待\"); this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } System.out.println(\"3、买到包子，回家\"); }).start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); synchronized (this) { this.notifyAll(); System.out.println(\"2、通知消费者\"); } } 虽然 wait() 会自动解锁，但是对顺序有要求。 如果 wait() 方法，是在 notify 之后调用，则线程会永远处于 waitting 状态。 3.3 park/unparkpark 是等待许可，unpark 是为指定线程提供许可，他的主要特性就是对顺序没有要求，调用park时候，如果调用过 unpark 则，线程直接执行。 /** 正常的park/unpark */ public void parkUnparkTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { while (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); LockSupport.park(); } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); LockSupport.unpark(consumerThread); System.out.println(\"3、通知消费者\"); } 但是 unpark 调用之后，许可只能使用一次。即只要调一次park，则许可就会失效。后续调用 park，则该线程会进入等待状态。 /** 死锁的park/unpark */ public void parkUnparkDeadLockTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); // 当前线程拿到锁，然后挂起 synchronized (this) { LockSupport.park(); } } System.out.println(\"3、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); // 争取到锁以后，再恢复consumerThread synchronized (this) { LockSupport.unpark(consumerThread); } System.out.println(\"2、通知消费者\"); } 3.4 所有代码package com.study.hc.thread.chapter1.thread; import java.util.concurrent.locks.LockSupport; /** 三种线程协作通信的方式：suspend/resume、wait/notify、park/unpark */ public class Demo6 { /** 包子店 */ public static Object baozidian = null; /** 正常的suspend/resume */ public void suspendResumeTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); Thread.currentThread().suspend(); } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); consumerThread.resume(); System.out.println(\"3、通知消费者\"); } /** 死锁的suspend/resume。 suspend并不会像wait一样释放锁，故此容易写出死锁代码 */ public void suspendResumeDeadLockTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); // 当前线程拿到锁，然后挂起 synchronized (this) { Thread.currentThread().suspend(); } } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); // 争取到锁以后，再恢复consumerThread synchronized (this) { consumerThread.resume(); } System.out.println(\"3、通知消费者\"); } /** 导致程序永久挂起的suspend/resume */ public void suspendResumeDeadLockTest2() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { System.out.println(\"1、没包子，进入等待\"); try { // 为这个线程加上一点延时 Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } // 这里的挂起执行在resume后面 Thread.currentThread().suspend(); } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); consumerThread.resume(); System.out.println(\"3、通知消费者\"); consumerThread.join(); } /** 正常的wait/notify */ public void waitNotifyTest() throws Exception { // 启动线程 new Thread(() -> { synchronized (this) { while (baozidian == null) { // 如果没包子，则进入等待 try { System.out.println(\"1、进入等待\"); this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } System.out.println(\"2、买到包子，回家\"); }).start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); synchronized (this) { this.notifyAll(); System.out.println(\"3、通知消费者\"); } } /** 会导致程序永久等待的wait/notify */ public void waitNotifyDeadLockTest() throws Exception { // 启动线程 new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 try { Thread.sleep(5000L); } catch (InterruptedException e1) { e1.printStackTrace(); } synchronized (this) { try { System.out.println(\"1、进入等待\"); this.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } System.out.println(\"2、买到包子，回家\"); }).start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); synchronized (this) { this.notifyAll(); System.out.println(\"3、通知消费者\"); } } /** 正常的park/unpark */ public void parkUnparkTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { while (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); LockSupport.park(); } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); LockSupport.unpark(consumerThread); System.out.println(\"3、通知消费者\"); } /** 死锁的park/unpark */ public void parkUnparkDeadLockTest() throws Exception { // 启动线程 Thread consumerThread = new Thread(() -> { if (baozidian == null) { // 如果没包子，则进入等待 System.out.println(\"1、进入等待\"); // 当前线程拿到锁，然后挂起 synchronized (this) { LockSupport.park(); } } System.out.println(\"2、买到包子，回家\"); }); consumerThread.start(); // 3秒之后，生产一个包子 Thread.sleep(3000L); baozidian = new Object(); // 争取到锁以后，再恢复consumerThread synchronized (this) { LockSupport.unpark(consumerThread); } System.out.println(\"3、通知消费者\"); } public static void main(String[] args) throws Exception { // 对调用顺序有要求，也要开发自己注意锁的释放。这个被弃用的API， 容易死锁，也容易导致永久挂起。 // new Demo6().suspendResumeTest(); // new Demo6().suspendResumeDeadLockTest(); // new Demo6().suspendResumeDeadLockTest2(); // wait/notify要求再同步关键字里面使用，免去了死锁的困扰，但是一定要先调用wait，再调用notify，否则永久等待了 // new Demo6().waitNotifyTest(); // new Demo6().waitNotifyDeadLockTest(); // park/unpark没有顺序要求，但是park并不会释放锁，所有再同步代码中使用要注意 // new Demo6().parkUnparkTest(); new Demo6().parkUnparkDeadLockTest(); } } 4. 总结 suspend 和 resume 容易写出死锁代码所以被弃用，导致死锁的原因： suspend 挂起之后不会释放锁；suspend 和 resume 先后顺序不保证。 wait 和 notify。只能由同一对象锁的持有者线程调用，也就是说写在代码同步块里，否则会抛出 IllegalMonitorStateException 异常。wait 方法导致当前线程等待，加入该对象的等待集合中，并且 放弃当前持有的对象锁。notify 和 notifyAll 方法唤醒一个或所有正在等待这个对象锁的线程。 注意：虽然 wait 会自动解锁，但是对顺序有要求。 如果 wait 方法，是在 notify 之后调用，则线程会永远处于 waitting 状态。 park / unpark park 是等待许可，unpark 是为指定线程提供许可，对顺序没有要求，调用park时候，如果调用过 unpark 则，线程直接执行。但由于许可的一次有效性，即只要调一次park，则许可就会失效。后续调用park，则该线程会进入等待状态。 伪唤醒问题 由于 jvm 在实现机制问题，通过以上三种方式会出现错误警报或伪唤醒的问题，这时并不是真正的满足条件唤醒。如果 通过 if 语句判断是否为空，来使线程进入等待状态，则会也会出现异常情况。 所以建议 通过 while 循环的写法，然后设置 flag 来检测线程的等待条件。","link":"/2020/07/31/%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%E7%9A%84%E6%96%B9%E5%BC%8F/"},{"title":"Java 线程池实践","text":"先来回答一个问题线程是不是越多越好？ 想必肯定会说，不是越多越好。但是为什么不是越多越好呢？ 线程在 Java 中是一个对象，更是操作系统的资源，线程创建、销毁需要时间。如果创建时间+销毁时间 &gt; 执行任务时间，这样就很不合算。 Java 对象占用堆内存，操作系统线程占用系统内存，根据 JVM 规范，一个线程默认最大栈大小为 1 M，这个栈空间是需要从系统内存中分配的。线程过多，会消耗很多的内存。 操作系统需要频繁切换线程上下文（大家都想被运行），影响性能。 线程池的推出，就是为了方便控制线程数量。 1. 线程池组成部分： 线程池管理器：用于创建并管理线程池，包括创建线程池，销毁线程池，添加新任务。 工作线程：线程池中线程，在没有任务时处于等待状态，可以循环的执行任务。 任务接口（Runnable）: 每个任务必须实现接口，以供工作线程调度任务执行，它主要规定了任务的入口、任务执行完后的收尾工作、任务的执行状态等。 任务队列：用于存放没有处理的任务。提供一种缓冲机制 2. 线程池 API-接口定义和实现类 类型 名称 描述 接口 Executor 最上层的接口，定义了执行任务的方法 execute() 接口 ExecutorService 继承Executor 接口，拓展了 Callable、Future、关闭方法 接口 ScheduledExecutorService 继承ExecutorService 接口，增加了定时任务相关的方法 实现类 ThreadPoolExecutor 基础、标准的线程池实现 实现类 ScheduledThreadPoolExecutor 继承了 ThreadPoolExecutor，实现了 ScheduledExecutorService中相关定时任务的方法 ScheduledThreadPoolExecutor是功能最为丰富的类。 3. 线程池原理-任务 execute 过程 是否达到核心线程数量？没达到，创建一个工作线程来执行任务。 工作队列是否已满？没满，则将新提交的任务存储在工作队列里。 是否达到线程池最大数量？没达到，则创建一个新的工作线程来执行任务。 最后，执行拒绝策略来处理这个任务。 4. 示例代码分析测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 public void testCommon(ThreadPoolExecutor threadPoolExecutor) throws Exception { // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { e.printStackTrace(); } } }); System.out.println(\"任务提交成功 :\" + i); } // 查看线程数量，查看队列等待数量 Thread.sleep(500L); System.out.println(\"当前线程池线程数量为：\" + threadPoolExecutor.getPoolSize()); System.out.println(\"当前线程池等待的数量为：\" + threadPoolExecutor.getQueue().size()); // 等待15秒，查看线程数量和队列数量（理论上，会被超出核心线程数量的线程自动销毁） Thread.sleep(15000L); System.out.println(\"当前线程池线程数量为：\" + threadPoolExecutor.getPoolSize()); System.out.println(\"当前线程池等待的数量为：\" + threadPoolExecutor.getQueue().size()); } 场景一： 线程池信息： 核心线程数量5，最大数量10，无界队列，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略。 private void threadPoolExecutorTest1() throws Exception { ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>()); testCommon(threadPoolExecutor); } 预计结果：线程池线程数量为：5,超出数量的任务，其他的进入队列中等待被执行 场景二： 线程池信息： 核心线程数量5，最大数量10，队列大小3，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略. private void threadPoolExecutorTest2() throws Exception { // 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。 // 默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); testCommon(threadPoolExecutor); } 预计结果： 5个任务直接分配线程开始执行 3个任务进入等待队列 队列不够用，临时加开5个线程来执行任务(5秒没活干就销毁) 队列和线程池都满了，剩下2个任务，没资源了，被拒绝执行。 任务执行，5秒后，如果无任务可执行，销毁临时创建的5个线程 场景三： 线程池信息： 核心线程数量5，最大数量5，无界队列，超出核心线程数量的线程存活时间：5秒 /** * 3、 线程池信息： 核心线程数量5，最大数量5，无界队列，超出核心线程数量的线程存活时间：5秒 * * @throws Exception */ private void threadPoolExecutorTest3() throws Exception { // 和Executors.newFixedThreadPool(int nThreads)一样的 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>()); testCommon(threadPoolExecutor); // } 预计结果：线程池线程数量为：5，超出数量的任务，其他的进入队列中等待被执行 场景四： 线程池信息：核心线程数量0，最大数量Integer.MAX_VALUE，SynchronousQueue队列，超出核心线程数量的线程存活时间：60秒 private void threadPoolExecutorTest4() throws Exception { ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>()); testCommon(threadPoolExecutor); Thread.sleep(60000L); System.out.println(\"60秒后，再看线程池中的数量：\" + threadPoolExecutor.getPoolSize()); } 预计结果： 线程池线程数量为：15，超出数量的任务，其他的进入队列中等待被执行 所有任务执行结束，60秒后，如果无任务可执行，所有线程全部被销毁，池的大小恢复为0 SynchronousQueue，实际上它不是一个真正的队列，因为它不会为队列中元素维护存储空间。与其他队列不同的是，它维护一组线程，这些线程在等待着把元素加入或移出队列。在使用SynchronousQueue作为工作队列的前提下，客户端代码向线程池提交任务时，而线程池中又没有空闲的线程能够从SynchronousQueue队列实例中取一个任务，那么相应的offer方法调用就会失败（即任务没有被存入工作队列）。此时，ThreadPoolExecutor会新建一个新的工作者线程用于对这个入队列失败的任务进行处理（假设此时线程池的大小还未达到其最大线程池大小maximumPoolSize）。和Executors.newCachedThreadPool()一样的 场景五： 定时执行线程池信息：3秒后执行，一次性任务，到点就执行核心线程数量5，最大数量Integer.MAX_VALUE，DelayedWorkQueue延时队列，超出核心线程数量的线程存活时间：0秒 private void threadPoolExecutorTest5() throws Exception { // 和Executors.newScheduledThreadPool()一样的 ScheduledThreadPoolExecutor threadPoolExecutor = new ScheduledThreadPoolExecutor(5); threadPoolExecutor.schedule(new Runnable() { @Override public void run() { System.out.println(\"任务被执行，现在时间：\" + System.currentTimeMillis()); } }, 3000, TimeUnit.MILLISECONDS); System.out.println( \"定时任务，提交成功，时间是：\" + System.currentTimeMillis() + \", 当前线程池中线程数量：\" + threadPoolExecutor.getPoolSize()); } 预计结果：任务在3秒后被执行一次 场景六： 定时执行线程池信息：线程固定数量5 , 核心线程数量5，最大数量Integer.MAX_VALUE，DelayedWorkQueue延时队列，超出核心线程数量的线程存活时间：0秒 private void threadPoolExecutorTest6() throws Exception { ScheduledThreadPoolExecutor threadPoolExecutor = new ScheduledThreadPoolExecutor(5); threadPoolExecutor.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { Thread.sleep(3000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务-1 被执行，现在时间：\" + System.currentTimeMillis()); } }, 2000, 1000, TimeUnit.MILLISECONDS); threadPoolExecutor.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { Thread.sleep(3000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务-2 被执行，现在时间：\" + System.currentTimeMillis()); } }, 2000, 1000, TimeUnit.MILLISECONDS); } 周期性执行某一个任务，线程池提供了两种调度方式 测试场景：提交的任务需要3秒才能执行完毕。看两种不同调度方式的区别效果1： 提交后，2秒后开始第一次执行，之后每间隔1秒，固定执行一次(如果发现上次执行还未完毕，则等待完毕，完毕后立刻执行)。 也就是说这个代码中是，3秒钟执行一次（计算方式：每次执行三秒，间隔时间1秒，执行结束后马上开始下一次执行，无需等待） 效果2： 提交后，2秒后开始第一次执行，之后每间隔1秒，固定执行一次(如果发现上次执行还未完毕，则等待完毕，等上一次执行完毕后再开始计时，等待1秒)。也就是说这个代码钟的效果看到的是：4秒执行一次。 （计算方式：每次执行3秒，间隔时间1秒，执行完以后再等待1秒，所以是 3+1） 线程终止线程池信息： 核心线程数量5，最大数量10，队列大小3，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略. shutdown() 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy private void threadPoolExecutorTest7() throws Exception { ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { System.out.println(\"异常：\" + e.getMessage()); } } }); System.out.println(\"任务提交成功 :\" + i); } // 1秒后终止线程池 Thread.sleep(1000L); threadPoolExecutor.shutdown(); // 再次提交提示失败 threadPoolExecutor.submit(new Runnable() { @Override public void run() { System.out.println(\"追加一个任务\"); } }); } 结果分析 10个任务被执行，3个任务进入队列等待，2个任务被拒绝执行 调用shutdown后，不接收新的任务，等待13任务执行结束 追加的任务在线程池关闭后，无法再提交，会被拒绝执行 shutdownNow() 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy private void threadPoolExecutorTest8() throws Exception { ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { System.out.println(\"异常：\" + e.getMessage()); } } }); System.out.println(\"任务提交成功 :\" + i); } // 1秒后终止线程池 Thread.sleep(1000L); List&lt;Runnable> shutdownNow = threadPoolExecutor.shutdownNow(); // 再次提交提示失败 threadPoolExecutor.submit(new Runnable() { @Override public void run() { System.out.println(\"追加一个任务\"); } }); System.out.println(\"未结束的任务有：\" + shutdownNow.size()); } 结果分析 10个任务被执行，3个任务进入队列等待，2个任务被拒绝执行 调用shutdownnow后，队列中的3个线程不再执行，10个线程被终止 追加的任务在线程池关闭后，无法再提交，会被拒绝执行 最后线程数量的选择 计算型任务： cpu 数量的 1-2 倍。 IO型任务：相对比计算型任务，需多一些线程，要根据具体的 IO阻塞时长进行考量决定。如 tomcat 中默认的最大线程数为：200 也可考虑根据需要在一个最小数量和最大数量间自动增减线程数。 以上测试完整代码： package com.study.hc.thread.chapter1.thread; import java.util.Collections; import java.util.List; import java.util.concurrent.LinkedBlockingQueue; import java.util.concurrent.RejectedExecutionHandler; import java.util.concurrent.ScheduledThreadPoolExecutor; import java.util.concurrent.SynchronousQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; /** 线程池的使用 */ public class Demo9 { /** * 测试： 提交15个执行时间需要3秒的任务,看线程池的状况 * * @param threadPoolExecutor 传入不同的线程池，看不同的结果 * @throws Exception */ public void testCommon(ThreadPoolExecutor threadPoolExecutor) throws Exception { // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { e.printStackTrace(); } } }); System.out.println(\"任务提交成功 :\" + i); } // 查看线程数量，查看队列等待数量 Thread.sleep(500L); System.out.println(\"当前线程池线程数量为：\" + threadPoolExecutor.getPoolSize()); System.out.println(\"当前线程池等待的数量为：\" + threadPoolExecutor.getQueue().size()); // 等待15秒，查看线程数量和队列数量（理论上，会被超出核心线程数量的线程自动销毁） Thread.sleep(15000L); System.out.println(\"当前线程池线程数量为：\" + threadPoolExecutor.getPoolSize()); System.out.println(\"当前线程池等待的数量为：\" + threadPoolExecutor.getQueue().size()); } /** * 1、线程池信息： 核心线程数量5，最大数量10，无界队列，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略的 * * @throws Exception */ private void threadPoolExecutorTest1() throws Exception { ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>()); testCommon(threadPoolExecutor); // 预计结果：线程池线程数量为：5,超出数量的任务，其他的进入队列中等待被执行 } /** * 2、 线程池信息： 核心线程数量5，最大数量10，队列大小3，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略的 * * @throws Exception */ private void threadPoolExecutorTest2() throws Exception { // 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。 // 默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); testCommon(threadPoolExecutor); // 预计结果： // 1、 5个任务直接分配线程开始执行 // 2、 3个任务进入等待队列 // 3、 队列不够用，临时加开5个线程来执行任务(5秒没活干就销毁) // 4、 队列和线程池都满了，剩下2个任务，没资源了，被拒绝执行。 // 5、 任务执行，5秒后，如果无任务可执行，销毁临时创建的5个线程 } /** * 3、 线程池信息： 核心线程数量5，最大数量5，无界队列，超出核心线程数量的线程存活时间：5秒 * * @throws Exception */ private void threadPoolExecutorTest3() throws Exception { // 和Executors.newFixedThreadPool(int nThreads)一样的 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 5, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable>()); testCommon(threadPoolExecutor); // 预计结：线程池线程数量为：5，超出数量的任务，其他的进入队列中等待被执行 } /** * 4、 线程池信息： * 核心线程数量0，最大数量Integer.MAX_VALUE，SynchronousQueue队列，超出核心线程数量的线程存活时间：60秒 * * @throws Exception */ private void threadPoolExecutorTest4() throws Exception { // SynchronousQueue，实际上它不是一个真正的队列，因为它不会为队列中元素维护存储空间。与其他队列不同的是，它维护一组线程，这些线程在等待着把元素加入或移出队列。 // 在使用SynchronousQueue作为工作队列的前提下，客户端代码向线程池提交任务时， // 而线程池中又没有空闲的线程能够从SynchronousQueue队列实例中取一个任务， // 那么相应的offer方法调用就会失败（即任务没有被存入工作队列）。 // 此时，ThreadPoolExecutor会新建一个新的工作者线程用于对这个入队列失败的任务进行处理（假设此时线程池的大小还未达到其最大线程池大小maximumPoolSize）。 // 和Executors.newCachedThreadPool()一样的 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable>()); testCommon(threadPoolExecutor); // 预计结果： // 1、 线程池线程数量为：15，超出数量的任务，其他的进入队列中等待被执行 // 2、 所有任务执行结束，60秒后，如果无任务可执行，所有线程全部被销毁，池的大小恢复为0 Thread.sleep(60000L); System.out.println(\"60秒后，再看线程池中的数量：\" + threadPoolExecutor.getPoolSize()); } /** * 5、 定时执行线程池信息：3秒后执行，一次性任务，到点就执行 &lt;br/> * 核心线程数量5，最大数量Integer.MAX_VALUE，DelayedWorkQueue延时队列，超出核心线程数量的线程存活时间：0秒 * * @throws Exception */ private void threadPoolExecutorTest5() throws Exception { // 和Executors.newScheduledThreadPool()一样的 ScheduledThreadPoolExecutor threadPoolExecutor = new ScheduledThreadPoolExecutor(5); threadPoolExecutor.schedule(new Runnable() { @Override public void run() { System.out.println(\"任务被执行，现在时间：\" + System.currentTimeMillis()); } }, 3000, TimeUnit.MILLISECONDS); System.out.println( \"定时任务，提交成功，时间是：\" + System.currentTimeMillis() + \", 当前线程池中线程数量：\" + threadPoolExecutor.getPoolSize()); // 预计结果：任务在3秒后被执行一次 } /** * 6、 定时执行线程池信息：线程固定数量5 ，&lt;br/> * 核心线程数量5，最大数量Integer.MAX_VALUE，DelayedWorkQueue延时队列，超出核心线程数量的线程存活时间：0秒 * * @throws Exception */ private void threadPoolExecutorTest6() throws Exception { ScheduledThreadPoolExecutor threadPoolExecutor = new ScheduledThreadPoolExecutor(5); // 周期性执行某一个任务，线程池提供了两种调度方式，这里单独演示一下。测试场景一样。 // 测试场景：提交的任务需要3秒才能执行完毕。看两种不同调度方式的区别 // 效果1： 提交后，2秒后开始第一次执行，之后每间隔1秒，固定执行一次(如果发现上次执行还未完毕，则等待完毕，完毕后立刻执行)。 // 也就是说这个代码中是，3秒钟执行一次（计算方式：每次执行三秒，间隔时间1秒，执行结束后马上开始下一次执行，无需等待） threadPoolExecutor.scheduleAtFixedRate(new Runnable() { @Override public void run() { try { Thread.sleep(3000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务-1 被执行，现在时间：\" + System.currentTimeMillis()); } }, 2000, 1000, TimeUnit.MILLISECONDS); // 效果2：提交后，2秒后开始第一次执行，之后每间隔1秒，固定执行一次(如果发现上次执行还未完毕，则等待完毕，等上一次执行完毕后再开始计时，等待1秒)。 // 也就是说这个代码钟的效果看到的是：4秒执行一次。 （计算方式：每次执行3秒，间隔时间1秒，执行完以后再等待1秒，所以是 3+1） threadPoolExecutor.scheduleWithFixedDelay(new Runnable() { @Override public void run() { try { Thread.sleep(3000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"任务-2 被执行，现在时间：\" + System.currentTimeMillis()); } }, 2000, 1000, TimeUnit.MILLISECONDS); } /** * 7、 终止线程：线程池信息： 核心线程数量5，最大数量10，队列大小3，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略的 * * @throws Exception */ private void threadPoolExecutorTest7() throws Exception { // 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。 // 默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { System.out.println(\"异常：\" + e.getMessage()); } } }); System.out.println(\"任务提交成功 :\" + i); } // 1秒后终止线程池 Thread.sleep(1000L); threadPoolExecutor.shutdown(); // 再次提交提示失败 threadPoolExecutor.submit(new Runnable() { @Override public void run() { System.out.println(\"追加一个任务\"); } }); // 结果分析 // 1、 10个任务被执行，3个任务进入队列等待，2个任务被拒绝执行 // 2、调用shutdown后，不接收新的任务，等待13任务执行结束 // 3、 追加的任务在线程池关闭后，无法再提交，会被拒绝执行 } /** * 8、 立刻终止线程：线程池信息： 核心线程数量5，最大数量10，队列大小3，超出核心线程数量的线程存活时间：5秒， 指定拒绝策略的 * * @throws Exception */ private void threadPoolExecutorTest8() throws Exception { // 创建一个 核心线程数量为5，最大数量为10,等待队列最大是3 的线程池，也就是最大容纳13个任务。 // 默认的策略是抛出RejectedExecutionException异常，java.util.concurrent.ThreadPoolExecutor.AbortPolicy ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(5, 10, 5, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;Runnable>(3), new RejectedExecutionHandler() { @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) { System.err.println(\"有任务被拒绝执行了\"); } }); // 测试： 提交15个执行时间需要3秒的任务，看超过大小的2个，对应的处理情况 for (int i = 0; i &lt; 15; i++) { int n = i; threadPoolExecutor.submit(new Runnable() { @Override public void run() { try { System.out.println(\"开始执行：\" + n); Thread.sleep(3000L); System.err.println(\"执行结束:\" + n); } catch (InterruptedException e) { System.out.println(\"异常：\" + e.getMessage()); } } }); System.out.println(\"任务提交成功 :\" + i); } // 1秒后终止线程池 Thread.sleep(1000L); List&lt;Runnable> shutdownNow = threadPoolExecutor.shutdownNow(); // 再次提交提示失败 threadPoolExecutor.submit(new Runnable() { @Override public void run() { System.out.println(\"追加一个任务\"); } }); System.out.println(\"未结束的任务有：\" + shutdownNow.size()); // 结果分析 // 1、 10个任务被执行，3个任务进入队列等待，2个任务被拒绝执行 // 2、调用shutdownnow后，队列中的3个线程不再执行，10个线程被终止 // 3、 追加的任务在线程池关闭后，无法再提交，会被拒绝执行 } public static void main(String[] args) throws Exception { // new Demo9().threadPoolExecutorTest1(); // new Demo9().threadPoolExecutorTest2(); // new Demo9().threadPoolExecutorTest3(); // new Demo9().threadPoolExecutorTest4(); // new Demo9().threadPoolExecutorTest5(); // new Demo9().threadPoolExecutorTest6(); // new Demo9().threadPoolExecutorTest7(); new Demo9().threadPoolExecutorTest8(); } }","link":"/2019/09/03/%E7%BA%BF%E7%A8%8B%E6%B1%A0%E8%AF%A6%E8%A7%A3/"},{"title":"网络协议梳理","text":"一、OSI 网络七层模型 应用层：决定向用户提供应用服务时通信的活动，此处的活动包括比如待传送数据的处理（应用内部生成特定格式的数据，后对数据进行标准协议的格式的编码）。 表示层负责数据格式转换、数据加密与解密、压缩解压缩等 会话层负责建立、管理和终止进程之间的会话和数据交换。 传输层：对上层应用层，提供处于网络连接种的两台计算机之间的数据传输（TCP、UDP）。 网络层：用来处理网络上流动的数据包，数据包是网络传输的最小数据单位。该层规定了通过怎样的路径到达对方计算机，并把数据包传送给对方。与对方计算机之间通过多台计算机或网络设备进行传输时，网络层所起的作用就是在众多的选项内选择一条传输路线。 链路层（又名数据链路层，网络接口层）：用来管理／操作连接网络的硬件部分。包括控制操作系统、硬件的设备驱动、网卡、光纤等物理可见部分。 二、 OSI 七层模型对应的协议 TCP/UDP 网络请求过程 发送端：在层与层之间传输数据时，每一层都会被打上当前层所属的头部信息。 接收端：在层与层之间传输数据时，每一层都会取消当前层所属的头部信息。 TCP 头部信息 TCP 可靠性保证 三次握手和四次挥手机制 在建立连接的时候，通过3 次握手机制。在结束连接的时候通过4次挥手。这样保证连接的可靠性。 校验和 发送的数据包的二进制相加然后取反，目的是检测数据在传输过程中的任何变化。如果收到段的检验和有差错，TCP将丢弃这个报文段和不确认收到此报文段。 确认应答+序列号 TCP给发送的每一个包进行编号，接收方对数据包进行排序，把有序数据传送给应用层。 超时重传 当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段。那么发送方发送完毕后等待的时间是多少呢？如果这个等待的时间过长，那么会影响TCP传输的整体效率，如果等待时间过短，又会导致频繁的发送重复的包。如何权衡？ 由于TCP传输时保证能够在任何环境下都有一个高性能的通信，因此这个最大超时时间（也就是等待的时间）是动态计算的。 超时以500ms（0.5秒）为一个单位进行控制，每次判定超时重发的超时时间都是500ms的整数倍。重发一次后，仍未响应，那么等待2500ms的时间后，再次重传。等待4500ms的时间继续重传。以一个指数的形式增长。累计到一定的重传次数，TCP就认为网络或者对端出现异常，强制关闭连接。 流量控制 TCP连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP使用的流量控制协议是可变大小的滑动窗口协议。接收方有即时窗口（滑动窗口），随ACK报文发送。 拥塞控制 而且 TCP 引入了慢启动的机制，在开始发送数据时，先发送少量的数据探路。探清当前的网络状态如何，再决定多大的速度进行传输。这时候就引入一个叫做拥塞窗口的概念。发送刚开始定义拥塞窗口为 1，每次收到ACK应答，拥塞窗口加1。在发送数据之前，首先将拥塞窗口与接收端反馈的窗口大小比对，取较小的值作为实际发送的窗口。发送过程中当网络拥塞时，减少数据的发送。发送方有拥塞窗口，发送数据前比对接收方发过来的及时窗口。 TCP 握手和挥手机制 三、 浏览器中输入一个域名地址 发生了什么？ $ nslookup www.baidu.com Server: 192.168.1.1 Address: 192.168.1.1#53 Non-authoritative answer: www.baidu.com canonical name = www.a.shifen.com. Name: www.a.shifen.com Address: 36.152.44.95 Name: www.a.shifen.com Address: 36.152.44.96 四、 WebSocket 和 HTTP, Socket 区别WebSocket 同 HTTP 一样也是应用层的协议,但是它是一种双向通信协议,是建立在TCP之上的。 连接过程 浏览器、服务器通过三次握手建立TCP连接 。这是通信的基础,若失败后续都不执行。 TCP连接成功后,浏览器通过HTTP协议向服务器传送WebSocket 支持的版本号等信息。(开始前的HTTP握手） 服务器收到客户端的握手请求后,同样采用HTTP协议回馈数据。 当收到了连接成功的消息后,通过TCP通道进行传输通信。 WebSocket与HTTP的相同点和不同点 相同点 都是一样基于TCP的,都是可靠性传输协议。 都是应用层协议 不同点 WebSocket是双向通信协议,模拟Socket协议,可以双向发送或接受信息,HTTP是单向的。 WebSocket是需要握手进行建立连接的 ，WebSocket在建立握手时,数据是通过HTTP传输的。但是建立之后,在真正传输时候是不需要HTTP协议的。 WebSocket 与 Socket的关系Socket 其实并不是一个协议,而是为了方便使用 TCP 或 UDP 而抽象出来的层,是位于应用层和传输控制层之间的—组接口。 Socket 是应用层与 TCP/IP 协议族通信的中间软件抽象层,它是一组接口。在设计模式中, Socket其实就是一个门面模式,它把复杂的 TCP/IP 协议族隐藏在 Socket 接口后面,对用户来说,一组简单的接口就是全部,让 Socket去组织数据,以符合指定的协议。 当两台主机通信时,必须通过 Socket 连接, Socket则利用 TCP/IP 协议建立 TCP 连接。TCP连接则更依靠于底层的 IP 协议, IP 协议的连接则依赖于链路层等更低层次。 WebSocket 则是一个典型的应用层协议。 区别 Socket是传输控制层协议, WebSocket是应用层协议。 参考图解 http","link":"/2020/06/06/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E6%A2%B3%E7%90%86/"},{"title":"设计模式","text":"设计模式概述：设计模式划分设计模式可分为创建型(Creational)，结构型(Structural)和行为型(Behavioral)三种，其中创建型模式主要用于描述如何创建对象，结构型模式主要用于描述如何实现类或对象的组合，行为型模式主要用于描述类或对象怎样交互以及怎样分配职责 七大原则 单一职责 职责单一，相对独立，不细说。 开放封闭原则 对扩展开放，修改封闭，这个也不细说了。 依赖倒转 抽象不应该依赖于细节，细节应当依赖于抽象。换言之，要针对接口编程，而不是针对实现编程,依赖倒转原则要求我们在程序代码中传递参数时或在关联关系中，尽量引用层次高的抽象层类，即使用接口和抽象类进行变量类型声明、参数类型声明、方法返回类型声明，以及数据类型的转换等，而不要用具体类来做这些事情。实现依赖倒转原则时，我们需要针对抽象层编程，而将具体类的对象通过依赖注入(DependencyInjection, DI)的方式注入到其他对象中，依赖注入是指当一个对象要与其他对象发生依赖关系时，通过抽象来注入所依赖的对象。常用的注入方式有三种，分别是：构造注入，设值注入（Setter注入）和接口注入 里氏替换原则 所有引用基类（父类）的地方必须能透明地使用其子类的对象。在软件中将一个基类对象替换成它的子类对象，程序将不会产生任何错误和异常，反过来则不成立，如果一个软件实体使用的是一个子类对象的话，那么它不一定能够使用基类对象。 组合复用原则 尽量使用对象组合，而不是继承来达到复用的目的。如果两个类之间是“Has-A”的关系应使用组合或聚合，如果是“Is-A”关系可使用继承。”Is-A”是严格的分类学意义上的定义，意思是一个类是另一个类的”一种”；而”Has-A”则不同，它表示某一个角色具有某一项责任。 接口隔离原则 使用多个特定的接口，而不使用单一的总接口，即客户端不应该依赖那些它不需要的接口。 在使用接口隔离原则时，我们需要注意控制接口的粒度，接口不能太小，如果太小会导致系统中接口泛滥，不利于维护；接口也不能太大，太大的接口将违背接口隔离原则，灵活性较差，使用起来很不方便。 接口隔离原则=使用单一职责原则的思想+考虑客户端具体的功能需求+适当切分定义接口文件+编程语言支持可实现多接口的语法特性 7. 迪米特原则 一个软件实体应当尽可能少地与其他实体发生相互作用。应该尽量减少对象之间的交互，如果两个对象之间不必彼此直接通信，那么这两个对象就不应当发生任何直接的相互作用，如果其中的一个对象需要调用另一个对象的某一个方法的话，可以通过第三者转发这个调用。简言之，就是通过引入一个合理的第三者来降低现有对象之间的耦合度。 迪米特法则目的是降低系统的耦合度，使类与类之间保持松散的耦合关系 在代码的重构中，很多时候以上原则是相铺相成互相结合。 开闭原则是目标，里氏代换原则是基础，依赖倒转原则是手段。相互补充，目标一致，只是分析问题时所站角度不同而已。 23 种设计模式划分创建型 简单工厂 工厂方法 抽象工厂 单例模式 原型模式 建造者模式 结构型 适配器模式 桥接模式 组合模式 装饰模式 外观模式 享元模式 代理模式 行为型模式 职责链模式 命令模式 解释器模式 迭代器模式 中介者模式 备忘录模式 观察者模式 状态模式 策略模式 模版方法模式 访问者模式","link":"/2019/09/05/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"title":"零成本 hexo 博客系统搭建","text":"如何借助 github 搭建一个博客系统，并且解决图床问题。分享一个笔者的解决方案。 1. 图床功能1.1 github 存储图片首先，我们需要建立一个公有的 Github 仓库，用来存放我们上传的图片 创建完成后，点击右上角的个人头像出现下拉菜单，然后点击 Setting， 然后点左侧菜单的最后一个 Developer setting， 然后再点左侧菜单的最后一个 Personal access tokens，然后在右侧有一个 Generate new token 的按钮，点击它，然后选择 repo， 然后直接拉到最下面点击绿色的按钮 Generate token。之后会生成一个 token码，需要注意的是这个token只显示一次！所以可以先把他复制保存的其他地方。 特别要注意这里面的权限。 1.2 PicGo 上传工具下载最新版不同平台的程序，可以去 这里下载。 PicGo Github 安装之后 打开程序配置 仓库名：按照“账户名/仓库名”的格式填写； 分支名：统一填写为「master」； Tocken：将上一步的 Github 的Token 粘贴在这里； 存储路径：若设置为 img/，则会在对应的 repository 下创建一个 img 文件夹； 自定义域名：在上传图片后成功后，PicGo会将“自定义域名+上传的图片名”生成的访问链接，放到剪切板。这里约定遵循如下的格式：域名/用户名/仓库名/分支名。 1.3 图片 cdn由于众所周知的原因，github 默认的链接地址访问受限，所以自定义域名 设置一个cdn 地址，推荐使用 https://cdn.jsdelivr.net/gh/{github account}/{repo name} 2. 有道笔记图片批量下载这个问题，来源于笔者日常使用的记录工具的有道云笔记。有道云笔记本身具有图床功能，比较方便，所以大部分的笔记文档都存储在里面。 但一个问题是，有道云笔记里面的图片链接，不能作为外链访问。这就导致，有些文档想在外部平台分享的时候（比如：博客），图片显示就是个问题，所以在发布到外部平台的文档内的图片就需要做点事情：下载、上传到github图床、文档替换。 2.1 python 图片下载有道云笔记 vipMD 文档内的 图片下载和文档图片链接替换 源码仓库 Github 1. 安装所需要的环境pip install -r requment.txt 2. 创建md目录，并拷贝待处理文档到当前 md 目录下3. 复制 有道云笔记 Vip 请求 Cookie。通过浏览器开发工具 可以查看具体的cookie 4. 执行命令python3 MDDownImage.py 5. md 路径替换以上命令执行完，即可发现路径已经被替换完成。替换的方式，两种方式：一种是文件覆盖；一种是生成新文件。 3. 博客发布博客系统 笔者采用的是 hexo 。 hexo 相关文档可以查看这里:https://hexo.io/zh-cn/docs/index.html 3.3 github io 仓库设置首先 创建一个 以 {account name}.github.io 命名的仓库。 然后设置 GitHubPages 设置之后显示大概就是这个样子，如果需要设置自定义 域名，则需要 Custom domain 中填入购买的的域名。 3.4 域名指定此域名需要在域名提供商购买，需要在对应管理后台对域名做解析。笔者用的是 腾讯云服务。也可以任意选择中意的域名提供商。解析方式大概这样子： 这时候针对 github 的设置已经基本完成。 4. 博客发布脚本4.1 脚本博客发布笔者写了个shell 脚本，文章写完，直接执行 sh deploy.sh 等待发布完成即可。 #!/usr/bin/ssh hexo clean hexo generate touch ./public/.nojekyll #echo &quot;blog.42011024.xyz&quot;&gt;./public/CNAME echo &quot;www.yusuzi.cn&quot;&gt;./public/CNAME hexo deploy git add . git commit -a -m 'deploy blog' git push -u origin master 4.2 解决 GitHub Page 404# GitHub - 解决 GitHub Page 404 #带有下划线的文件报 404 # 解决：在仓库文件夹根目录添加.nojekyll文件 touch ./public/.nojekyll 添加自定义域名，需要 添加个 CNAME 文件，并写入 「自定义的域名」 # echo &quot;www.yusuzi.cn&quot;&gt;./public/CNAME 4.3 hexo deployhexo deploy 命令，是hexo 内置的发布命令需要在 在 博客的 config.yaml文件种配置大概是这样 4.4 上传博客原文件备份git add . git commit -a -m 'deploy blog' git push -u origin master 至此 博客系统已经完成了。","link":"/2020/01/01/%E9%9B%B6%E6%88%90%E6%9C%AC-hexo-%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA/"},{"title":"责任链模式","text":"概念职责链模式(Chain of Responsibility Pattern)：避免请求发送者与接收者耦合在一起，让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。职责链模式是一种对象行为型模式。 结构图责任链模式实现方式，有两种：集合形式、链表形式 Handler（抽象处理者）： 它定义了一个处理请求的接口，一般设计为抽象类，由于不同的具体处理者处理请求的方式不同，因此在其中定义了抽象请求处理方法。因为每一个处理者的下家还是一个处理者，因此在抽象处理者中定义了一个抽象处理者类型的对象（如结构图中的 nextHandler），作为其对下家的引用。通过该引用，处理者可以连成一条链。 ConcreteHandler（具体处理者）：它是抽象处理者的子类，可以处理用户请求，在具体处理者类中实现了抽象处理者中定义的抽象请求处理方法，在处理请求之前需要进行判断，看是否有相应的处理权限，如果可以处理请求就处理它，否则将请求转发给后继者；在具体处理者中可以访问链中下一个对象，以便请求的转发。 示例代码 abstract class Handler { //维持对下家的引用 protected Handler nextHandler; public void setNextHandler(Handler successor) { this.nextHandler=successor; } } 具体处理者是抽象处理者的子类，它具有两大作用：第一是处理请求，不同的具体处理者以不同的形式实现抽象请求处理方法handleRequest()；第二是转发请求，如果该请求超出了当前处理者类的权限，可以将该请求转发给下家。 class ConcreteHandler extends Handler { public void handleRequest(String request) { if (请求满足条件) { //处理请求 } else { this.successor.handleRequest(request); //转发请求 } } class HandlerProcessor{ protect List&lt;Handler> handlerList; public void addHandler(Handler handler){ handlerList.add(handler); } public void processHandler(){ for(Handler handler:handlerList){ handler.handleRequest() } } } 职责链模式并不创建职责链，职责链的创建工作必须由系统的其他部分来完成，一般是在使用该职责链的客户端中创建职责链。职责链模式降低了请求的发送端和接收端之间的耦合，使多个对象都有机会处理这个请求。 链表形式执行： public static void main(String[] args){ Handler a,b,c; a = new ConcreteHandlerA(); b = new ConcreteHandlerA(); c = new ConcreteHandlerA(); a.setNextHandler(b); b.setNextHandler(c); a.handleRequest(); } 集合形式执行： public static void main(String[] args){ Handler a,b,c; a = new ConcreteHandlerA(); b = new ConcreteHandlerA(); c = new ConcreteHandlerA(); HandlerProcessor handleProcessor=new HandlerProcessor(); handleProcessor.add(a); handleProcessor.add(b); handleProcessor.add(c); handleProcessor.processHandler(); } 小结职责链模式的主要优点如下：(1) 职责链模式使得一个对象无须知道是其他哪一个对象处理其请求，对象仅需知道该请求会被处理即可，接收者和发送者都没有对方的明确信息，且链中的对象不需要知道链的结构，由客户端负责链的创建，降低了系统的耦合度。 (2) 请求处理对象仅需维持一个指向其后继者的引用，而不需要维持它对所有的候选处理者的引用，可简化对象的相互连接。 (3) 在给对象分派职责时，职责链可以给我们更多的灵活性，可以通过在运行时对该链进行动态的增加或修改来增加或改变处理一个请求的职责。 (4) 在系统中增加一个新的具体请求处理者时无须修改原有系统的代码，只需要在客户端重新建链即可，从这一点来看是符合“开闭原则”的。 职责链模式的主要缺点如下：(1) 由于一个请求没有明确的接收者，那么就不能保证它一定会被处理，该请求可能一直到链的末端都得不到处理；一个请求也可能因职责链没有被正确配置而得不到处理。 (2) 对于比较长的职责链，请求的处理可能涉及到多个处理对象，系统性能将受到一定影响，而且在进行代码调试时不太方便。 (3) 如果建链不当，可能会造成循环调用，将导致系统陷入死循环。 适用场景在以下情况下可以考虑使用职责链模式： (1) 有多个对象可以处理同一个请求，具体哪个对象处理该请求待运行时刻再确定，客户端只需将请求提交到链上，而无须关心请求的处理对象是谁以及它是如何处理的。 (2) 在不明确指定接收者的情况下，向多个对象中的一个提交一个请求。 (3) 可动态指定一组对象处理请求，客户端可以动态创建职责链来处理请求，还可以改变链中处理者之间的先后次序。","link":"/2019/09/23/%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"}],"tags":[{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"性能优化","slug":"性能优化","link":"/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"微服务","slug":"微服务","link":"/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"锁","slug":"锁","link":"/tags/%E9%94%81/"},{"name":"反射","slug":"反射","link":"/tags/%E5%8F%8D%E5%B0%84/"},{"name":"注解","slug":"注解","link":"/tags/%E6%B3%A8%E8%A7%A3/"},{"name":"缓存","slug":"缓存","link":"/tags/%E7%BC%93%E5%AD%98/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"MQ","slug":"MQ","link":"/tags/MQ/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"爬虫","slug":"爬虫","link":"/tags/%E7%88%AC%E8%99%AB/"},{"name":"注册中心","slug":"注册中心","link":"/tags/%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"架构","slug":"架构","link":"/tags/%E6%9E%B6%E6%9E%84/"},{"name":"优化","slug":"优化","link":"/tags/%E4%BC%98%E5%8C%96/"},{"name":"思维模型","slug":"思维模型","link":"/tags/%E6%80%9D%E7%BB%B4%E6%A8%A1%E5%9E%8B/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"正则","slug":"正则","link":"/tags/%E6%AD%A3%E5%88%99/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"}],"categories":[{"name":"技术","slug":"技术","link":"/categories/%E6%8A%80%E6%9C%AF/"},{"name":"调优","slug":"技术/调优","link":"/categories/%E6%8A%80%E6%9C%AF/%E8%B0%83%E4%BC%98/"},{"name":"基础","slug":"技术/基础","link":"/categories/%E6%8A%80%E6%9C%AF/%E5%9F%BA%E7%A1%80/"},{"name":"虚拟机","slug":"技术/虚拟机","link":"/categories/%E6%8A%80%E6%9C%AF/%E8%99%9A%E6%8B%9F%E6%9C%BA/"},{"name":"微服务","slug":"技术/微服务","link":"/categories/%E6%8A%80%E6%9C%AF/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"并发编程","slug":"技术/并发编程","link":"/categories/%E6%8A%80%E6%9C%AF/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"分布式","slug":"技术/分布式","link":"/categories/%E6%8A%80%E6%9C%AF/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"数据库","slug":"技术/数据库","link":"/categories/%E6%8A%80%E6%9C%AF/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"工具","slug":"技术/工具","link":"/categories/%E6%8A%80%E6%9C%AF/%E5%B7%A5%E5%85%B7/"},{"name":"架构","slug":"技术/架构","link":"/categories/%E6%8A%80%E6%9C%AF/%E6%9E%B6%E6%9E%84/"},{"name":"人文","slug":"人文","link":"/categories/%E4%BA%BA%E6%96%87/"},{"name":"知识库","slug":"知识库","link":"/categories/%E7%9F%A5%E8%AF%86%E5%BA%93/"},{"name":"网络","slug":"网络","link":"/categories/%E7%BD%91%E7%BB%9C/"},{"name":"经验","slug":"经验","link":"/categories/%E7%BB%8F%E9%AA%8C/"},{"name":"设计模式","slug":"人文/设计模式","link":"/categories/%E4%BA%BA%E6%96%87/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]}